{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset, TensorDataset, DataLoader\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, out_size: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Linear(ntoken, d_model) # Embedding layer converted into linear layer\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, out_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "#         src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "    \n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.decoder(output)\n",
    "#         print(output.size())\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change working directory to labels\n",
    "work_dir = \"C:/file_lists_with_labels_ff_estimator\"\n",
    "os.chdir(work_dir)\n",
    "\n",
    "# Reads labels into dict of (filename: ff_value)\n",
    "training_data_labels = {}\n",
    "with open(\"training.txt\") as f:\n",
    "    for line in f:\n",
    "        key, val = line.split()\n",
    "        training_data_labels[key] = float(val)\n",
    "        \n",
    "test_data_labels = {}\n",
    "with open(\"test.txt\") as f:\n",
    "    for line in f:\n",
    "        key, val = line.split()\n",
    "        test_data_labels[key] = float(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2041/2041 [00:47<00:00, 42.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 2041/2041 [00:48<00:00, 41.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Change directory to data\n",
    "os.chdir(\"C:/rf_without_tgc\")\n",
    "\n",
    "# Iterate thru data to create lists of tensors\n",
    "training_data = []\n",
    "training_labels = []\n",
    "for file in tqdm(os.listdir()):\n",
    "    if file not in training_data_labels:\n",
    "        continue\n",
    "        \n",
    "    file_data = pd.read_csv(file, header=None).T\n",
    "    \n",
    "    x_tensor = file_data.to_numpy().astype(np.float32)\n",
    "    y_tensor = float(training_data_labels[file])\n",
    "    \n",
    "    for x in x_tensor:\n",
    "        training_data.append(x)\n",
    "        training_labels.append(y_tensor)\n",
    "    \n",
    "test_data = []\n",
    "test_labels = []\n",
    "for file in tqdm(os.listdir()):\n",
    "    if file not in test_data_labels:\n",
    "        continue\n",
    "        \n",
    "    file_data = pd.read_csv(file, header=None).T\n",
    "    \n",
    "    x_tensor = file_data.to_numpy().astype(np.float32)\n",
    "    y_tensor = float(test_data_labels[file])\n",
    "\n",
    "    for x in x_tensor:\n",
    "        test_data.append(x)\n",
    "        test_labels.append(y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(261120, 1024)\n",
      "(261120, 1024)\n"
     ]
    }
   ],
   "source": [
    "# # Create class for DataLoader compatability\n",
    "# class Data():\n",
    "#     def __init__(self, x, y):\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.x)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         X = self.x[idx]\n",
    "#         y =  self.y[idx]\n",
    "\n",
    "#         return X, y\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "training_labels = np.array(training_labels)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)\n",
    "    \n",
    "# training_data = np.swapaxes(training_data, 1, 2)\n",
    "# test_data = np.swapaxes(test_data, 1, 2)\n",
    "\n",
    "# Create data tensors\n",
    "training_data = torch.Tensor(training_data)\n",
    "training_labels = torch.Tensor(training_labels)\n",
    "\n",
    "test_data = torch.Tensor(test_data)\n",
    "test_labels = torch.Tensor(test_labels)\n",
    "\n",
    "# training_data = (training_data - torch.mean(training_data)) / torch.std(training_data)\n",
    "# test_data = (test_data - torch.mean(test_data)) / torch.std(test_data)\n",
    "\n",
    "train_dataset = TensorDataset(training_data, training_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# # Load tensors into class for torch DataLoaders\n",
    "# train_data = Data(training_data, training_labels)\n",
    "# test_data = Data(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader Parameters\n",
    "train_loader_params = {\n",
    "    \"batch_size\":  32, \n",
    "    \"shuffle\":     True,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_loader = DataLoader(train_dataset, **train_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose device for torch computing\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 1024  # For ff estimation\n",
    "emsize = 1024#200  # embedding dimension\n",
    "d_hid = 100  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.1  # dropout probability\n",
    "model = TransformerModel(ntokens, 1, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "lr = 3  # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 5000\n",
    "    start_time = time.time()\n",
    "#     src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = 261120 // 20 # Total training signals // batch_size\n",
    "    for batch, (signal, target) in enumerate(train_loader):\n",
    "        signal, target = signal.to(device), target.to(device)\n",
    "#         signal = torch.unsqueeze(signal, dim=1)\n",
    "        batch_size = signal.size(0)\n",
    "#         print(signal.size())\n",
    "#         print(target.size())\n",
    "        output = model(signal)\n",
    "        output = torch.squeeze(output)\n",
    "        loss = criterion(output, target)\n",
    "#         print(output.size(), \"\\n\")\n",
    "#         print(output.size(), target.size())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "#             ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2683,  1.1886, -1.3740,  ...,  2.0660, -0.3293, -1.4028],\n",
      "        [ 0.0208,  1.0228,  0.2272,  ...,  1.0667, -0.5378, -0.7382],\n",
      "        [-2.7698, -0.1296,  2.3986,  ..., -0.4175, -0.5109, -0.4602],\n",
      "        ...,\n",
      "        [ 0.1341, -3.6466, -3.8215,  ...,  0.2865,  0.0856,  0.8273],\n",
      "        [ 0.9595,  4.6524, -5.2989,  ...,  0.6947,  0.1482, -0.1551],\n",
      "        [-1.8111, -0.6752, -1.2288,  ...,  0.5500,  0.3867, -0.2375]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-7.2525,  0.8447,  0.8540,  ...,  7.2731,  7.3079, -7.2145],\n",
      "        [-7.2820,  0.8473,  0.8571,  ...,  7.2697,  7.2976, -7.2314],\n",
      "        [-7.2879,  0.8496,  0.8597,  ...,  7.2656,  7.2886, -7.2372],\n",
      "        ...,\n",
      "        [-7.2623,  0.8608,  0.8697,  ...,  3.0070,  7.2677, -7.2170],\n",
      "        [-7.2234,  0.8543,  2.9797,  ...,  7.2595,  7.2885, -7.1785],\n",
      "        [-7.3048,  0.8764,  3.0152,  ...,  7.2119,  7.2357, -7.2587]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-2.6713,  5.5932,  5.4005,  ...,  2.4169,  2.0434, -1.6084],\n",
      "        [-2.6113,  5.5994,  5.3997,  ...,  2.4232,  2.0298, -1.5480],\n",
      "        [-2.6154,  5.5961,  5.4001,  ...,  2.4546,  2.0639, -1.5662],\n",
      "        ...,\n",
      "        [-2.6217,  5.5998,  5.3997,  ...,  2.4514,  2.0598, -1.5488],\n",
      "        [-2.9898,  5.5630,  5.4039,  ...,  2.8727,  2.6459, -2.3432],\n",
      "        [-2.6713,  5.3763,  5.4004,  ...,  2.4101,  2.0118, -1.6011]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.9431e+00,  3.6470e+00,  3.3942e+00,  ...,  1.0064e+00,\n",
      "          8.7952e-03,  3.2578e-01],\n",
      "        [-1.9165e+00,  3.6719e+00,  3.4263e+00,  ...,  9.5557e-01,\n",
      "         -1.8343e-02,  3.2930e-01],\n",
      "        [-1.8955e+00,  3.6471e+00,  3.3928e+00,  ...,  1.0034e+00,\n",
      "          5.8243e-03,  3.3155e-01],\n",
      "        ...,\n",
      "        [-1.8882e+00,  3.6499e+00,  3.3933e+00,  ...,  9.5629e-01,\n",
      "         -1.6648e-02,  3.3528e-01],\n",
      "        [-1.9018e+00,  3.6488e+00,  4.8056e+00,  ...,  9.8300e-01,\n",
      "          3.9079e-04,  3.2920e-01],\n",
      "        [-6.6250e-01,  3.6581e+00,  3.4033e+00,  ...,  9.4681e-01,\n",
      "         -2.6043e-02,  3.3912e-01]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-3.0096, -0.4014, -0.8996,  ...,  0.4520, -1.6931,  1.8745],\n",
      "        [-2.9448, -0.4644, -0.9615,  ...,  0.4719, -1.7324,  1.8683],\n",
      "        [-2.9795, -0.3723,  3.2027,  ...,  0.4635, -1.3281,  1.6855],\n",
      "        ...,\n",
      "        [-2.9838, -0.4199, -0.9051,  ...,  0.4682, -1.7236,  1.8856],\n",
      "        [-2.9861, -0.4131, -0.9053,  ...,  0.4541, -1.7080,  1.8805],\n",
      "        [-2.9648, -0.4129,  3.2035,  ...,  0.4621, -1.7060,  1.8658]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.8356, -1.0462, -0.6781,  ..., -0.4934, -1.9885,  1.7673],\n",
      "        [-1.7697, -1.1413, -0.7797,  ..., -0.4930, -1.9661,  1.6339],\n",
      "        [-1.8062, -1.0399, -0.6726,  ..., -0.4929, -1.9968,  1.5147],\n",
      "        ...,\n",
      "        [-0.7408, -1.1087, -0.7389,  ..., -0.4912, -2.0263,  1.7223],\n",
      "        [-1.7915, -1.0782, -0.7134,  ..., -0.4932, -1.9684,  1.6825],\n",
      "        [-1.7792, -1.1148, -0.7502,  ..., -0.4920, -2.0041,  1.6902]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.8298,  0.0458,  1.7033,  ..., -1.6065, -0.9280,  0.3107],\n",
      "        [ 0.8282,  0.0054,  1.6703,  ..., -1.6094, -0.9285,  0.2878],\n",
      "        [ 0.8230, -0.0509,  1.6302,  ..., -1.6134, -0.9172,  0.1969],\n",
      "        ...,\n",
      "        [ 0.6193, -0.0726,  1.6131,  ..., -1.1124, -0.9141,  0.2379],\n",
      "        [ 0.6178, -0.0367,  1.6416,  ..., -1.6151, -0.9313,  0.2025],\n",
      "        [ 0.8238, -0.0249,  1.6516,  ..., -1.6123, -0.9124,  0.1913]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.8258,  0.7281,  1.4334,  ..., -2.5928, -0.9239,  0.3725],\n",
      "        [ 1.8136,  0.7433,  1.4498,  ..., -2.5845, -0.9206,  0.2862],\n",
      "        [ 1.8449,  0.8404,  1.5373,  ..., -2.5764, -0.0752,  0.5223],\n",
      "        ...,\n",
      "        [ 1.8203,  0.7936,  1.4819,  ..., -2.6013, -0.9853,  0.4541],\n",
      "        [ 1.8192,  0.7510,  1.4485,  ..., -2.6060, -0.9387,  0.3672],\n",
      "        [ 1.8268,  0.7909,  1.4867,  ..., -2.5834, -0.9494,  0.4677]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.4688, -0.0612,  2.0447,  ..., -1.6276, -0.1957, -0.4159],\n",
      "        [ 2.4649, -0.1071,  2.0096,  ..., -1.6378, -0.2026, -0.3525],\n",
      "        [ 2.4771,  0.0067,  2.0893,  ..., -1.6237, -0.1977, -0.3410],\n",
      "        ...,\n",
      "        [ 1.5272,  0.0061,  2.0902,  ..., -1.6204, -0.1937, -0.2989],\n",
      "        [ 2.4528, -0.0322,  2.0695,  ..., -1.6185, -0.1775, -0.4462],\n",
      "        [ 1.4934, -0.1561,  1.9820,  ..., -1.6384, -0.1997, -0.5405]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.0594,  3.6905,  2.2213,  ..., -1.6755,  0.3714, -0.5626],\n",
      "        [ 3.5488,  0.9347,  2.2100,  ..., -1.6832,  0.3699, -0.5770],\n",
      "        [ 3.5526,  0.9312,  2.2041,  ..., -1.6857,  0.3625, -0.5607],\n",
      "        ...,\n",
      "        [ 3.5426,  1.0021,  2.2499,  ..., -1.6767,  0.3580, -0.4575],\n",
      "        [ 3.5154,  0.9063,  2.1795,  ..., -1.6938,  0.3488, -0.6215],\n",
      "        [ 3.5357,  0.9876,  4.4799,  ..., -1.6738,  0.3501, -0.4954]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 3.3438,  0.0359,  2.1675,  ..., -0.7472,  0.6126, -0.8986],\n",
      "        [ 3.3237,  0.0428,  2.1698,  ..., -0.7445,  0.6299, -0.8372],\n",
      "        [ 3.3158,  0.0663,  2.1793,  ..., -0.7471,  0.6022, -0.8892],\n",
      "        ...,\n",
      "        [ 3.3692,  0.0909,  2.1954,  ..., -0.7452,  0.6137, -0.7210],\n",
      "        [ 3.3336,  0.0366,  2.1477,  ..., -0.7497,  0.5784, -0.8128],\n",
      "        [ 3.3496,  0.0692,  2.1765,  ..., -0.7480,  1.2874, -0.7577]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.5431, -2.2858,  1.8720,  ...,  0.7387,  0.4301, -1.0726],\n",
      "        [ 2.1957,  2.4768,  1.8559,  ...,  0.7305,  0.4822, -0.8508],\n",
      "        [ 2.2029, -2.3497,  1.8416,  ...,  0.1401,  0.4282, -1.0638],\n",
      "        ...,\n",
      "        [ 2.1976, -2.3213,  1.8667,  ...,  0.7304,  0.4639, -1.1299],\n",
      "        [ 2.1966, -2.2816,  1.8697,  ...,  0.7396,  0.4295, -1.0469],\n",
      "        [ 2.1978, -2.2968,  1.8666,  ...,  0.1387,  0.4378, -1.0356]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.7778, -2.5759,  1.7453,  ...,  0.6862,  0.6741, -0.9921],\n",
      "        [ 1.7632,  2.2534,  1.6806,  ...,  1.9599,  0.6275, -1.3883],\n",
      "        [ 1.7636, -2.6038,  1.7377,  ...,  1.9191,  0.6469, -1.3561],\n",
      "        ...,\n",
      "        [ 1.7722,  2.3599,  1.7558,  ...,  1.8995,  0.6823, -1.3127],\n",
      "        [ 1.7681, -2.5802,  1.7635,  ...,  1.9041,  0.6740, -1.3187],\n",
      "        [ 1.7548, -2.7287,  4.1364,  ...,  1.9535,  0.6191, -1.5111]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.8559, -1.4475,  1.9839,  ...,  3.5325,  2.0185, -1.4117],\n",
      "        [ 1.4661, -1.4999,  1.9726,  ...,  3.5412,  1.3793, -2.0150],\n",
      "        [ 1.8635,  2.7235,  2.0083,  ...,  3.5254,  1.3542, -1.7122],\n",
      "        ...,\n",
      "        [ 1.8699, -1.3628,  2.0325,  ...,  3.5023,  1.3657, -1.6632],\n",
      "        [ 1.8732, -1.3692,  4.3611,  ...,  3.5418,  1.3367, -1.5411],\n",
      "        [ 1.4702, -1.4240,  1.9879,  ...,  3.5478,  2.0336, -1.6363]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.2590, -1.2255,  1.5176,  ...,  2.9134,  1.0386, -1.2616],\n",
      "        [ 1.2591, -1.2582,  1.5029,  ...,  2.9255,  1.0452, -1.3630],\n",
      "        [ 1.2582, -1.2820,  1.4719,  ...,  2.9666,  1.0340, -1.3600],\n",
      "        ...,\n",
      "        [ 1.2573, -1.2949,  4.0909,  ...,  2.9498,  1.0455, -1.3874],\n",
      "        [ 1.2577, -1.2903,  1.4702,  ...,  2.9379,  1.0363, -1.4038],\n",
      "        [ 1.2590, -1.2988,  1.4552,  ...,  2.9511,  0.9968, -1.1981]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.0677, -1.7795,  0.1267,  ...,  0.8840, -0.0458, -0.2519],\n",
      "        [ 0.0530, -1.7509,  3.4993,  ...,  0.8848,  1.2904, -0.1495],\n",
      "        [ 0.0560, -1.7037,  0.1862,  ...,  0.8731, -0.0204, -0.3672],\n",
      "        ...,\n",
      "        [ 0.0557, -1.8139,  0.0835,  ...,  0.9025, -0.1281, -0.1851],\n",
      "        [ 0.5926,  2.5341,  0.1551,  ...,  0.8903, -0.1009, -0.4264],\n",
      "        [ 0.0596, -1.7763,  0.1242,  ...,  0.8874, -0.0749, -0.1523]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.1896, -1.4955, -0.2530,  ..., -0.5553, -1.0337,  0.3540],\n",
      "        [-0.7632,  2.5884, -0.2836,  ..., -0.5529, -0.9743,  0.0882],\n",
      "        [-0.7916, -1.4636, -0.2207,  ..., -0.5493, -0.9564,  0.3701],\n",
      "        ...,\n",
      "        [-0.7734, -1.5356, -0.2660,  ..., -0.4019, -0.9822,  0.2106],\n",
      "        [-0.8076,  2.6509, -0.2604,  ..., -0.5527, -1.0129,  0.3700],\n",
      "        [-0.7647, -1.5789, -0.3232,  ..., -0.5547, -1.0162, -0.1851]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.3309, -1.4557, -1.2554,  ..., -1.2885, -1.0362,  1.0830],\n",
      "        [-1.2791, -1.5247, -1.3340,  ..., -1.2956, -1.0593,  0.9325],\n",
      "        [-1.2841, -1.5164, -1.3318,  ..., -1.2975, -1.0675,  0.8695],\n",
      "        ...,\n",
      "        [-1.3123, -1.4996, -1.2603,  ..., -1.2828, -0.9580,  0.9322],\n",
      "        [-1.3035, -1.4852, -1.2583,  ..., -1.2872, -0.9706,  0.8090],\n",
      "        [-1.3106, -1.5239, -1.3162,  ..., -1.3059, -1.0339,  0.9604]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-2.1121, -1.4710, -1.8146,  ..., -2.9212, -1.8759,  1.3621],\n",
      "        [-2.1429,  2.6157, -1.7700,  ..., -2.9304, -1.9203,  1.5173],\n",
      "        [-2.1237, -1.4408, -1.8102,  ..., -2.9382, -1.9021,  1.3661],\n",
      "        ...,\n",
      "        [-2.1494, -1.4405, -1.8176,  ..., -1.4815, -1.9154,  1.5147],\n",
      "        [-2.1676, -1.4447, -1.8134,  ..., -2.9340, -1.9068,  1.5336],\n",
      "        [-2.1288, -1.4424, -1.8104,  ..., -2.9328, -1.9003,  1.4734]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-2.1342, -0.9846, -2.2144,  ..., -2.7649,  0.6722,  0.7494],\n",
      "        [-2.1008, -1.0681, -2.2895,  ..., -2.7852, -1.3069,  1.4521],\n",
      "        [-2.0944, -1.1198, -2.3834,  ..., -1.3665,  0.5859,  0.6040],\n",
      "        ...,\n",
      "        [-2.1155, -1.0176, -2.2582,  ..., -2.7778, -1.3410,  1.5249],\n",
      "        [-2.1099, -1.0799, -2.3243,  ..., -2.8103, -1.3550,  1.4576],\n",
      "        [-2.1345, -1.0057, -2.2557,  ..., -2.7868, -1.3513,  1.5350]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.3644,  3.0975, -2.7660,  ..., -1.3392, -0.0595,  1.1620],\n",
      "        [-1.3421, -0.2562, -2.7212,  ..., -1.3266, -0.0284,  1.0603],\n",
      "        [-1.3542, -0.2297, -2.7039,  ..., -1.3301, -0.0599,  1.2047],\n",
      "        ...,\n",
      "        [-1.3592, -0.3155,  2.2142,  ..., -1.3332, -0.0041,  1.0691],\n",
      "        [-1.3351, -0.2859, -2.7505,  ..., -0.5840,  1.3844,  1.0776],\n",
      "        [-1.3099, -0.3141,  2.1034,  ..., -1.3480, -0.0855,  0.9772]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.8950,  0.0922,  2.1907,  ..., -0.4792,  0.9896,  0.8255],\n",
      "        [-0.8680,  0.1081, -2.8133,  ..., -0.4833,  1.0210,  0.7072],\n",
      "        [-0.8779,  0.0955, -2.8571,  ..., -0.4796,  0.9802,  0.7630],\n",
      "        ...,\n",
      "        [-0.9154,  3.3093, -2.6968,  ..., -0.4677,  1.0103,  0.9915],\n",
      "        [-0.8815,  0.0900, -2.8331,  ..., -0.4773,  0.9980,  0.7255],\n",
      "        [-0.9095,  0.1138, -2.8197,  ..., -0.4765,  1.0117,  0.8383]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.5548,  3.3521, -2.6953,  ...,  0.2557,  2.4230,  0.0308],\n",
      "        [-0.5524,  0.3007, -2.7164,  ...,  0.2553,  2.4221,  0.4731],\n",
      "        [ 0.4740,  0.2539, -2.8044,  ...,  0.2550,  2.4167,  0.3943],\n",
      "        ...,\n",
      "        [-0.5594,  0.3163, -2.7044,  ...,  0.2891,  2.4069,  0.0093],\n",
      "        [-0.5786,  0.3404, -2.6503,  ...,  0.2553,  2.4277,  0.0753],\n",
      "        [-0.5339,  0.2420, -2.8039,  ...,  0.2542,  2.4115,  0.3705]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.1167,  0.3402, -2.2358,  ...,  0.7436,  2.4557,  0.1146],\n",
      "        [-0.1034,  0.2773, -2.3301,  ...,  0.7489,  2.4483, -0.0035],\n",
      "        [-0.1150,  0.2874, -2.3334,  ...,  0.7492,  2.4436, -0.3531],\n",
      "        ...,\n",
      "        [ 0.7271,  3.2987, -2.3336,  ...,  0.7478,  2.4324, -0.0312],\n",
      "        [-0.1334,  0.3426, -2.2656,  ...,  0.7438,  2.4480,  0.1707],\n",
      "        [-0.0979,  0.2664, -2.3644,  ...,  0.7492,  2.4301,  0.0115]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.3853,  0.0592,  2.7623,  ...,  0.8975,  1.4520, -0.5374],\n",
      "        [ 0.3732,  0.1592, -1.2014,  ...,  0.8937,  1.4685, -0.7298],\n",
      "        [ 0.3655,  0.0767,  2.7967,  ...,  0.9020,  1.4327, -0.4104],\n",
      "        ...,\n",
      "        [ 0.3799,  0.0921, -1.2868,  ...,  0.8957,  1.4624, -0.5331],\n",
      "        [ 0.3609,  0.1127,  2.8164,  ...,  0.8965,  1.4148, -0.4098],\n",
      "        [ 0.3820,  0.0617, -1.3405,  ...,  0.8988,  1.4306, -0.4989]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.7641,  0.0407,  2.8832,  ...,  1.3348,  1.3593, -0.8315],\n",
      "        [ 0.7670,  0.0884, -0.9663,  ...,  1.3321,  1.3280, -0.7850],\n",
      "        [ 0.7627,  0.0809, -0.9839,  ...,  1.3376,  1.3249, -0.9717],\n",
      "        ...,\n",
      "        [ 0.7629,  0.1309, -0.9144,  ...,  1.3274,  1.3304, -0.7144],\n",
      "        [ 1.2132,  3.2016, -1.0370,  ...,  1.3373,  1.3326, -0.8335],\n",
      "        [ 0.7637,  0.1170, -0.9478,  ...,  0.9188,  1.3286, -0.7243]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.4431,  0.4193, -1.2307,  ...,  2.2351,  1.8930, -0.6465],\n",
      "        [ 1.1832,  3.3495, -1.2341,  ...,  2.2165,  1.8927, -0.6167],\n",
      "        [ 1.1745,  0.4313, -1.2135,  ...,  2.2282,  1.8741, -0.5302],\n",
      "        ...,\n",
      "        [ 1.1805,  0.3824, -1.2494,  ...,  2.2128,  1.9184, -0.7238],\n",
      "        [ 1.1886,  0.3777, -1.2596,  ...,  1.4159,  1.8884, -0.7067],\n",
      "        [ 1.1823,  0.4605, -1.1641,  ...,  2.2074,  1.8927, -0.8803]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.1815,  0.2555, -0.8976,  ...,  2.1218,  1.6164, -0.6107],\n",
      "        [ 1.1799,  0.2712, -0.8333,  ...,  2.1076,  1.6498, -0.6614],\n",
      "        [ 1.1717,  0.2729,  2.8989,  ...,  2.1334,  1.5824, -0.4893],\n",
      "        ...,\n",
      "        [ 1.1745,  0.2528, -0.8543,  ...,  2.1137,  1.6354, -0.5781],\n",
      "        [ 1.1793,  0.2951, -0.8262,  ...,  2.1127,  1.6039, -0.8590],\n",
      "        [ 1.1782,  0.2786,  2.9131,  ...,  2.1160,  1.6553, -0.6393]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.6993, -0.3403,  0.0435,  ...,  1.1919,  0.6652, -0.7947],\n",
      "        [ 0.7240, -0.3973,  3.2020,  ...,  1.2042,  1.5306, -0.6582],\n",
      "        [ 0.7037, -0.3585,  0.0058,  ...,  0.9222,  0.6165, -0.5306],\n",
      "        ...,\n",
      "        [ 0.7073, -0.3848,  0.0039,  ...,  1.1960,  0.6580, -0.5545],\n",
      "        [ 0.6954, -0.3365,  0.0539,  ...,  1.1896,  0.7022, -0.7687],\n",
      "        [ 0.7058, -0.4153, -0.0330,  ...,  1.2060,  0.6515, -0.6618]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.1791, -0.5145,  0.1528,  ...,  0.8409,  0.2043, -0.4026],\n",
      "        [ 0.5243, -0.4390,  0.2291,  ...,  0.8378,  0.2453, -0.2914],\n",
      "        [ 0.5252, -0.4945,  0.1693,  ...,  0.8416,  0.1872, -0.3399],\n",
      "        ...,\n",
      "        [ 0.5338, -0.4443,  0.2314,  ...,  0.8379,  0.2843, -0.3868],\n",
      "        [ 0.5441, -0.5872,  0.0818,  ...,  0.8430,  0.1760, -0.4608],\n",
      "        [ 0.5255, -0.4768,  0.1800,  ...,  0.8394,  0.2130, -0.3189]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.6723, -0.2405, -0.3734,  ...,  0.8611,  0.1241, -0.2558],\n",
      "        [ 0.6759, -0.2606, -0.3857,  ...,  0.8616,  0.1224, -0.2910],\n",
      "        [ 0.6848, -0.2649,  3.0699,  ...,  0.8616,  1.1547, -0.3555],\n",
      "        ...,\n",
      "        [ 0.6706, -0.2219, -0.3447,  ...,  0.8604,  0.1430, -0.2513],\n",
      "        [ 0.6761, -0.2450, -0.3659,  ...,  0.8593,  1.1958, -0.2502],\n",
      "        [ 0.6707, -0.2573, -0.3918,  ...,  0.8629,  0.0669, -0.2468]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.2673, -0.2287, -0.2821,  ...,  0.3337, -0.1019,  0.1091],\n",
      "        [ 0.2771,  3.1147, -0.3262,  ...,  0.3322, -0.1290,  0.0834],\n",
      "        [ 0.2592, -0.1795,  3.1769,  ...,  0.3371, -0.0975,  0.1619],\n",
      "        ...,\n",
      "        [ 0.2808, -0.3019, -0.3127,  ...,  0.3343, -0.0438, -0.0325],\n",
      "        [ 1.0808, -0.2609, -0.3053,  ...,  0.3319,  0.9690,  0.0224],\n",
      "        [ 1.0709, -0.2790, -0.3614,  ...,  0.3313, -0.1119,  0.1012]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.5709, -0.1466,  0.1914,  ..., -0.5358, -0.1609,  0.8210],\n",
      "        [-0.5751, -0.2152,  0.1333,  ..., -0.5380, -0.1282,  0.2949],\n",
      "        [ 0.6410,  3.1923,  3.3646,  ..., -0.5368, -0.1820,  0.8938],\n",
      "        ...,\n",
      "        [-0.5709, -0.1762,  0.1554,  ..., -0.5370, -0.1584,  0.8173],\n",
      "        [-0.5997, -0.1734,  0.1601,  ..., -0.5310, -0.1045,  0.8370],\n",
      "        [-0.5921, -0.2238,  0.0986,  ..., -0.5536, -0.2373,  0.8691]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.3093, -0.7905,  0.4025,  ..., -1.3649, -0.5464,  1.0094],\n",
      "        [-1.2318,  2.9140,  0.3273,  ..., -1.3993, -0.5964,  0.8691],\n",
      "        [-1.2549, -0.8305,  0.3315,  ..., -1.3984, -0.6581,  0.9472],\n",
      "        ...,\n",
      "        [-1.2503, -0.7928,  0.3923,  ..., -0.3337, -0.5550,  0.8776],\n",
      "        [-1.3002, -0.7442,  0.3961,  ..., -1.3909, -0.6157,  1.0289],\n",
      "        [-1.2769, -0.7828,  0.3815,  ..., -1.3962, -0.5790,  0.8962]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-2.3456, -2.4330,  0.6848,  ..., -1.0141, -1.6358,  0.8658],\n",
      "        [-2.3445, -2.4426,  0.7089,  ..., -0.9905, -1.4829,  0.8187],\n",
      "        [-2.3748,  2.4172,  0.7054,  ..., -2.6955, -1.6089,  0.9349],\n",
      "        ...,\n",
      "        [-2.3389, -2.4247,  0.6905,  ..., -2.6712, -1.5532,  0.8581],\n",
      "        [-2.3365, -2.3681,  3.5781,  ..., -2.6797, -1.6205,  0.9421],\n",
      "        [-2.3497, -2.4347,  0.6820,  ..., -2.7016, -1.6272,  0.9400]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-2.3642, -2.6575,  0.6257,  ..., -2.8244, -1.8592,  0.5673],\n",
      "        [-2.3528,  2.2941,  0.6057,  ..., -1.0946, -0.2773,  0.5367],\n",
      "        [-2.3948, -2.5668,  0.6808,  ..., -2.7922, -1.8892,  0.7035],\n",
      "        ...,\n",
      "        [-2.3199, -2.6923,  3.5381,  ..., -2.8137, -1.7937,  0.5694],\n",
      "        [-2.3325, -2.7800,  0.6039,  ..., -2.8115, -1.7070,  0.3986],\n",
      "        [-2.3569, -2.6085,  0.6502,  ..., -2.8064, -1.8243,  0.6184]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.3913, -1.8285,  3.3548,  ..., -1.9567, -1.4008, -0.0224],\n",
      "        [-1.3679, -1.8512,  3.3269,  ..., -1.9785, -1.4229, -0.0636],\n",
      "        [-1.4028, -1.7767,  0.1067,  ..., -1.9608, -1.4655,  0.0593],\n",
      "        ...,\n",
      "        [-1.3924, -1.7682,  0.1309,  ..., -1.9556, -1.4201,  0.0071],\n",
      "        [ 0.2388, -1.8512,  0.1294,  ..., -1.8904, -1.1865, -0.1396],\n",
      "        [-1.3519, -1.7550,  0.1263,  ..., -1.9628, -1.4357, -0.0287]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.3253, -0.3257, -0.9166,  ..., -0.3762, -0.5729, -1.2472],\n",
      "        [ 0.3207, -0.3507, -0.9253,  ..., -0.3709, -0.5361, -1.0551],\n",
      "        [ 0.3176,  3.1220, -0.9122,  ..., -0.3648,  0.5197, -1.0826],\n",
      "        ...,\n",
      "        [ 0.3107, -0.3969, -1.0145,  ..., -0.3843,  0.4892, -1.0349],\n",
      "        [ 0.2914, -0.2696, -0.8605,  ..., -0.3731, -0.6261, -0.9546],\n",
      "        [ 0.3160, -0.3679, -0.9017,  ..., -0.3572, -0.4101, -1.1983]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.6911,  1.1053, -1.3968,  ...,  0.8530,  0.0614, -1.4045],\n",
      "        [ 1.4686,  3.6454, -1.3979,  ...,  0.8522,  0.0827, -1.4167],\n",
      "        [ 1.4704,  1.1171, -1.3276,  ...,  0.8514,  0.1261, -1.5403],\n",
      "        ...,\n",
      "        [ 1.4756,  1.1118, -1.3750,  ...,  0.8520,  0.0545, -1.5729],\n",
      "        [ 1.4685,  1.1574, -1.3073,  ...,  0.8513,  0.1078, -1.4167],\n",
      "        [ 1.4733,  1.1301,  2.6888,  ...,  0.8529,  0.0688, -1.4394]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.6031,  3.2228, -1.4692,  ...,  2.1936,  0.6829, -1.3934],\n",
      "        [ 2.6105,  3.2322, -1.4124,  ...,  2.1658,  0.7555, -1.3575],\n",
      "        [ 2.5993,  4.4246, -1.4381,  ...,  2.1906,  0.6770, -1.3947],\n",
      "        ...,\n",
      "        [ 2.5986,  3.2226, -1.4330,  ...,  2.1722,  0.7437, -1.3969],\n",
      "        [ 2.5960,  3.2208,  2.6948,  ...,  2.1767,  0.7488, -1.5013],\n",
      "        [ 2.6011,  3.2192, -1.4820,  ...,  2.1907,  0.6957, -1.4182]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.9400,  3.5438, -1.5915,  ...,  2.6439,  0.9998, -1.4992],\n",
      "        [ 2.9613,  3.5296, -1.6711,  ...,  2.7116,  0.8901, -1.4470],\n",
      "        [ 2.9458,  3.5390, -1.6005,  ...,  2.6510,  1.0040, -1.5061],\n",
      "        ...,\n",
      "        [ 2.9418,  3.5231, -1.6911,  ...,  2.6781,  0.9603, -1.5057],\n",
      "        [ 2.9509,  3.5305,  2.6300,  ...,  2.6550,  1.0048, -1.4838],\n",
      "        [ 2.9518,  3.5313,  2.6370,  ...,  2.6543,  0.9995, -1.4814]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.4860,  2.2835, -1.7821,  ...,  2.2582,  1.3902, -1.5805],\n",
      "        [ 2.4849,  2.3004,  2.5796,  ...,  2.2639,  0.8868, -1.5729],\n",
      "        [ 2.4848,  2.2574, -1.8758,  ...,  2.2740,  0.9139, -1.6234],\n",
      "        ...,\n",
      "        [ 2.4788,  4.0069, -1.7489,  ...,  2.2646,  0.8882, -1.6268],\n",
      "        [ 2.4855,  2.3045,  2.5845,  ...,  2.2651,  0.8886, -1.5766],\n",
      "        [ 2.4848,  2.2734, -1.8538,  ...,  2.2881,  0.8662, -1.6282]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.6397,  2.3432, -1.9621,  ...,  2.6840,  1.1082, -1.5299],\n",
      "        [ 2.6529,  2.3794, -1.8286,  ...,  2.6084,  1.1870, -1.5005],\n",
      "        [ 2.6385,  2.3528, -1.9067,  ...,  2.6416,  1.1601, -1.5376],\n",
      "        ...,\n",
      "        [ 2.6498,  2.3710, -1.9086,  ...,  1.6654,  1.5532, -1.4983],\n",
      "        [ 2.6385,  2.3655, -1.8983,  ...,  2.6460,  1.1277, -1.5151],\n",
      "        [ 2.6446,  2.4030, -1.7760,  ...,  2.6093,  1.1763, -1.5000]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.0876,  1.1431, -1.9768,  ...,  2.1954,  0.9363, -1.5199],\n",
      "        [ 2.0825,  1.1775, -1.9105,  ...,  2.1761,  0.9214, -1.5188],\n",
      "        [ 2.0828,  1.1726, -1.9096,  ...,  2.1855,  0.9464, -1.5202],\n",
      "        ...,\n",
      "        [ 2.0834,  1.1820, -1.9028,  ...,  1.3814,  0.9234, -1.5101],\n",
      "        [ 2.0846,  1.1868, -1.8736,  ...,  2.1529,  1.3625, -1.5193],\n",
      "        [ 2.0832,  1.1695, -1.9203,  ...,  1.3788,  0.9338, -1.5357]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.0273,  1.1560, -1.8890,  ...,  2.3732,  1.1991, -1.3484],\n",
      "        [ 2.0283,  1.2083, -1.8232,  ...,  2.3757,  1.1755, -1.3303],\n",
      "        [ 2.0323,  1.1925, -1.8453,  ...,  2.3789,  1.1664, -1.3205],\n",
      "        ...,\n",
      "        [ 1.8858,  1.2017, -1.8316,  ...,  2.3887,  1.1484, -1.3258],\n",
      "        [ 2.0281,  1.1560, -1.9060,  ...,  2.3850,  1.1905, -1.3510],\n",
      "        [ 2.0294,  1.1510, -1.9296,  ...,  2.3847,  1.1989, -1.3486]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.4575,  0.2565, -1.6451,  ...,  1.8034,  0.9320, -1.2618],\n",
      "        [ 1.4573,  0.2069, -1.7106,  ...,  1.8223,  0.9027, -1.2650],\n",
      "        [ 1.5661,  0.2017, -1.7411,  ...,  1.8391,  0.8786, -1.2614],\n",
      "        ...,\n",
      "        [ 1.4575,  0.2082, -1.7084,  ...,  1.8167,  0.8976, -1.2598],\n",
      "        [ 1.4555,  0.1863, -1.7232,  ...,  1.1298,  0.9011, -1.2633],\n",
      "        [ 1.4569,  0.1952, -1.7274,  ...,  1.8366,  0.8958, -1.2706]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.2597,  0.2102, -1.5134,  ...,  1.8593,  1.0585, -1.0060],\n",
      "        [ 1.2547,  0.1768, -1.5692,  ...,  1.8670,  1.0412, -0.9981],\n",
      "        [ 1.2541,  0.2080, -1.4797,  ...,  1.8363,  1.0947, -1.0019],\n",
      "        ...,\n",
      "        [ 1.2560,  0.1420, -1.5983,  ...,  1.8644,  1.0545, -1.0081],\n",
      "        [ 1.2561,  0.1785, -1.5497,  ...,  1.8535,  1.0730, -1.0068],\n",
      "        [ 1.2512,  0.1607, -1.5614,  ...,  1.8451,  1.1006, -1.0057]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.4006,  0.7330, -1.5546,  ...,  2.4517,  1.9268, -0.6795],\n",
      "        [ 1.4027,  0.7248, -1.5685,  ...,  1.4825,  1.9185, -0.6756],\n",
      "        [ 1.4016,  0.7172, -1.5938,  ...,  2.4738,  1.7205, -0.6825],\n",
      "        ...,\n",
      "        [ 1.4015,  0.7195, -1.5603,  ...,  2.4596,  1.7243, -0.6771],\n",
      "        [ 1.4022,  0.7127, -1.5703,  ...,  2.4696,  1.7207, -0.6779],\n",
      "        [ 1.3987,  0.7622, -1.5159,  ...,  2.4514,  1.7260, -0.6765]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.9984,  0.7948,  2.7911,  ...,  2.1663,  1.7203, -0.2423],\n",
      "        [ 1.0017,  0.8066, -1.0812,  ...,  2.1648,  1.8803, -0.2399],\n",
      "        [ 1.0057,  0.7812, -1.1073,  ...,  2.1706,  1.7161, -0.2450],\n",
      "        ...,\n",
      "        [ 0.9891,  0.8059, -1.1114,  ...,  2.1846,  1.7077, -0.2424],\n",
      "        [ 0.9894,  0.8060, -1.1139,  ...,  2.1698,  1.7135, -0.2364],\n",
      "        [ 0.9959,  0.8113, -1.0882,  ...,  2.1660,  1.7178, -0.2427]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.0471,  0.5329, -0.2979,  ...,  1.1127,  1.3638,  0.4184],\n",
      "        [ 0.0480,  0.6020, -0.2156,  ...,  1.1068,  1.1768,  0.4217],\n",
      "        [ 0.0646,  0.5587, -0.2720,  ...,  1.1231,  1.3564,  0.4180],\n",
      "        ...,\n",
      "        [ 0.0580,  0.5762, -0.2602,  ...,  1.1113,  1.1630,  0.4212],\n",
      "        [ 0.7984,  0.5172, -0.3358,  ...,  1.1291,  1.3504,  0.4210],\n",
      "        [ 0.0435,  0.5881, -0.2483,  ...,  1.1080,  1.1755,  0.4220]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.2570,  0.5497,  1.1475,  ..., -0.0992,  0.2121,  1.4434],\n",
      "        [-1.2377,  0.5180,  1.1145,  ..., -0.1032,  0.5001,  1.4415],\n",
      "        [-1.2198,  0.4668,  1.0720,  ..., -0.3686,  0.2115,  1.4393],\n",
      "        ...,\n",
      "        [-1.2328,  0.4383,  1.0566,  ..., -0.3702,  0.2134,  1.4384],\n",
      "        [ 0.1036,  0.5119,  1.1213,  ..., -0.3657,  0.2065,  1.4407],\n",
      "        [-1.2790,  0.5250,  1.1169,  ..., -0.3646,  0.1964,  1.4442]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-2.3133, -0.0997,  2.0085,  ..., -1.7486, -0.6292,  1.8434],\n",
      "        [-2.2735, -0.1486,  2.0040,  ..., -1.7370, -0.6391,  1.8426],\n",
      "        [-2.2747, -0.1502,  1.9994,  ..., -1.7435, -0.6339,  1.8425],\n",
      "        ...,\n",
      "        [-0.4170, -0.1615,  1.9855,  ..., -0.9111, -0.6691,  1.8426],\n",
      "        [-2.2997, -0.1429,  1.9962,  ..., -1.7575, -0.6827,  1.8433],\n",
      "        [-2.2454, -0.1892,  1.9682,  ..., -1.7631, -0.2927,  1.8426]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-3.4274, -1.5302,  2.8588,  ..., -3.4612, -1.6644,  1.8509],\n",
      "        [-3.5081, -1.5563,  2.8469,  ..., -3.5059, -1.1617,  1.8500],\n",
      "        [-3.4519, -1.4529,  2.8800,  ..., -3.4260, -1.6453,  1.8500],\n",
      "        ...,\n",
      "        [-3.4314, -1.4685,  2.8728,  ..., -3.4506, -1.7090,  1.8497],\n",
      "        [-3.4469, -1.5407,  2.8545,  ..., -3.4702, -1.6709,  1.8501],\n",
      "        [-1.0184,  2.3773,  2.8321,  ..., -3.5005, -1.7175,  1.8507]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-3.7442, -2.0059,  4.3420,  ..., -4.0846, -2.1397,  1.8971],\n",
      "        [-3.7996, -1.9078,  3.0946,  ..., -4.0942, -2.1992,  1.8929],\n",
      "        [-3.8254, -1.9682,  3.0784,  ..., -4.0740, -2.1760,  1.8948],\n",
      "        ...,\n",
      "        [-1.1960, -1.9893,  3.0629,  ..., -4.1180, -2.1548,  1.8953],\n",
      "        [-3.8253, -1.9552,  3.0853,  ..., -4.0723, -2.1549,  1.8939],\n",
      "        [-3.8336, -1.9563,  3.0897,  ..., -4.0032, -2.0579,  1.8957]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-3.2262, -1.5050,  2.6437,  ..., -2.0968, -1.9390,  1.9132],\n",
      "        [-3.2601,  2.4053,  2.6607,  ..., -3.6965, -2.0070,  1.9100],\n",
      "        [-3.3039, -1.4970,  4.1453,  ..., -2.0813, -1.9891,  1.9079],\n",
      "        ...,\n",
      "        [-0.9829, -1.4419,  2.6634,  ..., -3.6610, -2.0221,  1.9038],\n",
      "        [-0.9807, -1.4576,  4.1525,  ..., -3.6503, -2.0028,  1.9061],\n",
      "        [-1.0426, -1.3881,  2.6894,  ..., -3.6252, -1.9969,  1.8995]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-2.2773, -0.2358,  3.7246,  ..., -2.4881, -1.0907,  1.9620],\n",
      "        [-2.3849, -0.1356,  1.7656,  ..., -2.4336, -1.3490,  1.9543],\n",
      "        [-2.2878, -0.0850,  1.7763,  ..., -2.4481, -1.3762,  1.9558],\n",
      "        ...,\n",
      "        [-2.2743, -0.2230,  1.6980,  ..., -2.4740, -1.3606,  1.9640],\n",
      "        [-2.2985, -0.1409,  1.7482,  ..., -2.4512, -1.3662,  1.9585],\n",
      "        [-2.2820, -0.1482,  1.7405,  ..., -2.4523, -1.3564,  1.9616]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.6266,  0.1752,  1.0412,  ..., -2.0328, -1.3076,  1.7080],\n",
      "        [-1.6260,  0.2618,  1.1069,  ..., -1.9847, -1.2801,  1.7037],\n",
      "        [-1.6140,  3.0460,  1.0417,  ..., -1.9949, -1.2687,  1.7110],\n",
      "        ...,\n",
      "        [-0.0399,  0.2073,  3.4476,  ..., -1.9970, -1.2865,  1.7084],\n",
      "        [-1.6261,  0.1971,  1.0610,  ..., -1.9862, -1.2630,  1.7087],\n",
      "        [-1.6744,  0.2318,  1.0765,  ..., -2.0007, -1.2871,  1.6992]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.2586, -0.0626,  0.5108,  ..., -2.0167, -1.6579,  1.1397],\n",
      "        [-1.0804, -0.0693,  0.5140,  ..., -2.0350, -1.6780,  1.1402],\n",
      "        [-1.1002, -0.1021,  0.4990,  ..., -2.0188, -1.3828,  1.1359],\n",
      "        ...,\n",
      "        [-1.0420, -0.1405,  0.4689,  ..., -1.2022, -1.4078,  1.1486],\n",
      "        [-1.0642, -0.0428,  0.5415,  ..., -2.0336, -1.6354,  1.1380],\n",
      "        [-1.0940, -0.0425,  0.5418,  ..., -2.0231, -1.6550,  1.1322]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.4955,  0.0820, -0.0540,  ..., -1.5497, -1.4707,  0.8021],\n",
      "        [-0.4909,  0.0544, -0.0448,  ..., -1.5295, -1.4118,  0.8063],\n",
      "        [-0.4582,  0.0580, -0.0590,  ..., -1.5398, -1.4583,  0.8024],\n",
      "        ...,\n",
      "        [-0.4875,  0.0707, -0.0583,  ..., -1.5144, -1.4120,  0.7960],\n",
      "        [-0.5263,  0.0680, -0.0654,  ..., -1.5676, -1.4961,  0.7905],\n",
      "        [-0.4766,  0.0425, -0.0841,  ..., -1.5672, -1.4673,  0.8009]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.0348,  0.4711, -0.5275,  ..., -0.4049, -0.7969,  0.6537],\n",
      "        [ 0.0454,  3.1760, -0.5372,  ..., -0.7271, -0.6786,  0.6608],\n",
      "        [ 0.0491,  0.4960, -0.5247,  ..., -0.7499, -0.8113,  0.6565],\n",
      "        ...,\n",
      "        [ 0.0713,  0.4821, -0.5406,  ..., -0.7389, -0.6962,  0.6616],\n",
      "        [ 0.0524,  3.1617, -0.5235,  ..., -0.4105, -0.8095,  0.6544],\n",
      "        [ 0.0442,  0.5061, -0.4931,  ..., -0.7267, -0.8034,  0.6533]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.7081,  0.6046,  2.4977,  ..., -0.1423, -0.5709,  0.2590],\n",
      "        [ 0.7326,  0.6076, -1.2428,  ..., -0.1385, -0.5575,  0.2736],\n",
      "        [ 0.7059,  0.6007, -1.2504,  ..., -0.1455, -0.5849,  0.2608],\n",
      "        ...,\n",
      "        [ 0.7109,  0.6801, -1.1461,  ..., -0.1404, -0.5747,  0.2520],\n",
      "        [ 0.7181,  0.6459, -1.1867,  ..., -0.0453, -0.5795,  0.2577],\n",
      "        [ 0.7175,  0.6140, -1.2306,  ..., -0.1456, -0.5826,  0.2622]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.9528,  0.8024, -1.2671,  ...,  0.3265, -0.0856,  0.1730],\n",
      "        [ 0.9574,  0.7539, -1.3370,  ...,  0.3278, -0.0882,  0.1803],\n",
      "        [ 0.9619,  3.2839,  2.4451,  ...,  0.3272, -0.0570,  0.1812],\n",
      "        ...,\n",
      "        [ 0.9647,  0.7791, -1.2898,  ...,  0.3255, -0.0809,  0.1894],\n",
      "        [ 0.9558,  0.7627, -1.3116,  ...,  0.3266, -0.0918,  0.1753],\n",
      "        [ 0.9567,  0.7931, -1.2862,  ...,  0.3280, -0.0908,  0.1778]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.4761,  0.9223,  2.1809,  ...,  1.0095,  0.2133, -0.2144],\n",
      "        [ 1.6999,  0.8686, -2.0956,  ...,  1.0183,  0.2119, -0.2037],\n",
      "        [ 1.6926,  0.9076, -2.0439,  ...,  1.0219,  0.2127, -0.2173],\n",
      "        ...,\n",
      "        [ 1.4817,  0.8839, -2.1074,  ...,  1.0046,  0.2146, -0.1977],\n",
      "        [ 1.4733,  0.9098, -2.0458,  ...,  1.0109,  0.2132, -0.2210],\n",
      "        [ 1.4761,  0.9152, -2.0454,  ...,  1.0110,  0.2121, -0.2223]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.9425,  3.4003, -1.9745,  ...,  1.5484,  0.4764, -0.4756],\n",
      "        [ 1.8601,  0.8725, -2.0557,  ...,  1.5501,  0.4755, -0.4513],\n",
      "        [ 1.9409,  0.8600, -2.0800,  ...,  0.9958,  0.4790, -0.4721],\n",
      "        ...,\n",
      "        [ 1.9408,  0.8552, -2.0930,  ...,  1.5673,  0.4669, -0.4632],\n",
      "        [ 1.9369,  0.8686, -2.0216,  ...,  1.5158,  0.4695, -0.4357],\n",
      "        [ 1.9399,  0.8733,  2.0563,  ...,  1.5614,  0.4781, -0.4709]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.8123,  0.3053, -1.3981,  ...,  1.4301,  0.6104, -0.4281],\n",
      "        [ 1.8130,  0.2927, -1.3822,  ...,  1.4017,  0.6033, -0.4255],\n",
      "        [ 1.8129,  0.2976, -1.3738,  ...,  1.4051,  0.6018, -0.4187],\n",
      "        ...,\n",
      "        [ 1.6965,  0.2837, -1.4109,  ...,  1.4109,  0.6046, -0.4200],\n",
      "        [ 1.6984,  0.3487,  2.1934,  ...,  1.4070,  0.6069, -0.4313],\n",
      "        [ 1.8104,  0.2806, -1.3891,  ...,  1.3958,  0.5992, -0.4107]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.7879,  0.2297, -0.8051,  ...,  1.8532,  0.8417, -0.4988],\n",
      "        [ 1.7879,  0.2663, -0.7634,  ...,  1.7955,  0.8384, -0.5051],\n",
      "        [ 1.7920,  3.2026, -0.7600,  ...,  1.7817,  0.8237, -0.5110],\n",
      "        ...,\n",
      "        [ 1.7893,  3.1673,  2.2985,  ...,  1.8148,  0.8443, -0.5088],\n",
      "        [ 1.7962,  0.3043, -0.7352,  ...,  1.8120,  0.8501, -0.5294],\n",
      "        [ 1.7916,  0.2831,  2.3388,  ...,  1.7638,  0.8250, -0.5146]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.8420,  0.8562,  0.1676,  ...,  2.7208,  1.2346, -0.6986],\n",
      "        [ 1.8328,  3.4207,  0.1569,  ...,  2.7440,  1.2461, -0.6798],\n",
      "        [ 1.8298,  0.8387,  0.1590,  ...,  1.5647,  1.2448, -0.6722],\n",
      "        ...,\n",
      "        [ 1.8303,  0.8969,  0.2201,  ...,  2.7073,  1.2402, -0.6864],\n",
      "        [ 1.8354,  0.8713,  0.1817,  ...,  2.7435,  1.2510, -0.6943],\n",
      "        [ 1.8326,  0.8362,  0.1487,  ...,  2.8010,  1.2372, -0.6769]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.6112,  0.5914,  0.4224,  ...,  2.8701,  1.3152, -0.7143],\n",
      "        [ 1.6168,  0.6507,  0.4607,  ...,  2.8764,  1.3082, -0.7290],\n",
      "        [ 1.6163,  0.6332,  0.4490,  ...,  2.8513,  1.3256, -0.7473],\n",
      "        ...,\n",
      "        [ 1.6152,  0.6217,  0.4356,  ...,  2.8889,  1.2996, -0.7349],\n",
      "        [ 1.6215,  3.4258,  2.6453,  ...,  2.8411,  1.3109, -0.7478],\n",
      "        [ 1.6096,  3.3802,  0.4776,  ...,  2.8638,  1.2999, -0.7226]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.0559e+00, -3.4822e-01,  3.4138e-03,  ...,  2.2463e+00,\n",
      "          1.0769e+00, -6.7880e-01],\n",
      "        [ 1.0548e+00, -3.3718e-01, -2.2495e-03,  ...,  2.2596e+00,\n",
      "          1.0737e+00, -6.6061e-01],\n",
      "        [ 1.1603e+00, -3.7059e-01, -1.9764e-02,  ...,  1.1142e+00,\n",
      "          1.1225e+00, -6.8218e-01],\n",
      "        ...,\n",
      "        [ 1.1586e+00, -3.4709e-01, -8.0205e-03,  ...,  2.2637e+00,\n",
      "          1.0752e+00, -6.6795e-01],\n",
      "        [ 1.1598e+00, -3.2224e-01,  2.1567e-02,  ...,  2.2466e+00,\n",
      "          1.0772e+00, -6.8726e-01],\n",
      "        [ 1.1600e+00, -3.2090e-01,  1.9031e-02,  ...,  2.2691e+00,\n",
      "          1.0932e+00, -6.9041e-01]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.6603, -2.1830, -1.2936,  ...,  1.1390,  0.5498, -0.6337],\n",
      "        [ 0.6603, -2.1877, -1.3146,  ...,  1.1308,  0.5664, -0.6446],\n",
      "        [ 0.6602,  2.4367,  1.8437,  ...,  0.1956,  0.5872, -0.6612],\n",
      "        ...,\n",
      "        [ 0.6591, -2.1611, -1.2990,  ...,  1.1076,  0.5577, -0.6630],\n",
      "        [ 0.6611, -2.2222, -1.3371,  ...,  1.0730,  0.5234, -0.6196],\n",
      "        [ 0.6630, -2.2093, -1.3147,  ...,  1.0866,  0.5358, -0.5987]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.0532, -2.9244, -1.4649,  ...,  0.4310,  0.2846, -0.3883],\n",
      "        [-0.0625, -2.8997, -1.4605,  ...,  0.4272,  0.2841, -0.4053],\n",
      "        [-0.0557, -2.8328, -1.4231,  ...,  0.3851,  0.2572, -0.3909],\n",
      "        ...,\n",
      "        [-0.0607,  2.2332,  1.7406,  ...,  0.4171,  0.2952, -0.4160],\n",
      "        [-0.0555, -2.8172, -1.4284,  ...,  0.4427,  0.3034, -0.4062],\n",
      "        [-0.0447, -2.8776, -1.4348,  ...,  0.4058,  0.2606, -0.3715]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.2261, -2.8837, -0.7358,  ..., -0.7389,  0.1560,  0.1476],\n",
      "        [-1.2298,  2.2364, -0.7289,  ..., -0.0304,  0.1824,  0.1392],\n",
      "        [-1.2098, -2.8877, -0.7399,  ..., -0.0293,  0.2164,  0.1677],\n",
      "        ...,\n",
      "        [-1.2254, -2.8321, -0.7163,  ...,  0.0193,  0.2358,  0.1275],\n",
      "        [-1.1926, -2.9094, -0.7457,  ..., -0.7078,  0.1768,  0.1770],\n",
      "        [-1.2037, -2.9169, -0.7536,  ..., -0.7174,  0.1842,  0.1496]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.5178e+00, -2.6225e+00, -4.8736e-01,  ..., -4.6162e-01,\n",
      "         -2.1632e-02,  2.6952e-01],\n",
      "        [-1.5050e+00, -2.7304e+00, -5.3624e-01,  ..., -4.3131e-01,\n",
      "         -1.3955e-02,  3.0011e-01],\n",
      "        [-1.5010e+00,  2.3071e+00, -5.1024e-01,  ..., -1.0799e+00,\n",
      "         -3.5027e-02,  2.9367e-01],\n",
      "        ...,\n",
      "        [-1.5107e+00,  2.2881e+00, -5.0766e-01,  ..., -4.2994e-01,\n",
      "          5.4414e-04,  2.8592e-01],\n",
      "        [-1.5266e+00, -2.7038e+00, -5.1822e-01,  ..., -1.0764e+00,\n",
      "         -1.4826e-03,  2.8099e-01],\n",
      "        [-1.5015e+00, -2.6266e+00, -4.8887e-01,  ..., -4.5214e-01,\n",
      "         -2.7360e-02,  2.9238e-01]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.0551, -2.0569, -0.4817,  ..., -0.6431, -0.2084,  0.0886],\n",
      "        [-1.0531, -2.0862, -0.4569,  ..., -0.7075, -0.2499,  0.0960],\n",
      "        [-1.0411, -2.1528, -0.5072,  ..., -0.7007, -0.2646,  0.1237],\n",
      "        ...,\n",
      "        [-1.0572, -2.0941, -0.4627,  ..., -1.2885, -0.2525,  0.0969],\n",
      "        [-1.0672, -2.1668, -0.5039,  ..., -0.6777, -0.2415,  0.0880],\n",
      "        [-1.0736,  2.5416, -0.4429,  ..., -0.6698, -0.2434,  0.0728]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.1777, -1.8884, -0.5215,  ..., -1.1154, -0.4567,  0.2225],\n",
      "        [-1.1683, -1.8950, -0.5327,  ..., -1.1213, -0.4681,  0.2430],\n",
      "        [-1.1834, -1.9303, -0.5247,  ..., -1.0571, -0.3895,  0.2276],\n",
      "        ...,\n",
      "        [-1.1738, -1.9094, -0.5205,  ..., -1.1218, -0.4731,  0.2431],\n",
      "        [-1.1754, -1.9438, -0.5516,  ..., -1.0916, -0.4445,  0.2375],\n",
      "        [-1.1647, -1.9383, -0.5317,  ..., -1.1183, -0.4646,  0.2546]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.7711, -2.1617, -0.8598,  ..., -1.8428, -0.6623,  0.6188],\n",
      "        [-1.7703, -2.1861,  1.6719,  ..., -2.2482, -0.6513,  0.6154],\n",
      "        [-1.7653,  2.4232, -0.9489,  ..., -1.8022, -0.6697,  0.6359],\n",
      "        ...,\n",
      "        [-1.7329, -2.2747, -0.9089,  ..., -1.8550, -0.7584,  0.6669],\n",
      "        [-1.7207, -2.2790, -0.9399,  ..., -1.8329, -0.7090,  0.6664],\n",
      "        [-1.7649,  2.4569, -0.9051,  ..., -1.8239, -0.6935,  0.6364]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.6628,  2.6970, -0.6493,  ..., -1.9761, -0.7776,  0.7259],\n",
      "        [-1.6639, -1.7156, -0.7097,  ..., -1.9528, -0.6955,  0.7377],\n",
      "        [-1.6436, -1.7422, -0.7031,  ..., -1.9617, -0.7657,  0.7537],\n",
      "        ...,\n",
      "        [-1.6869, -1.6755, -0.6490,  ..., -1.9220, -0.7404,  0.7174],\n",
      "        [-1.6395, -1.7294, -0.7157,  ..., -1.9785, -0.7951,  0.7658],\n",
      "        [-1.6487, -1.7033, -0.6799,  ..., -1.9796, -0.7836,  0.7520]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.0544, -0.1442,  0.2895,  ..., -1.9710, -0.5729,  0.6508],\n",
      "        [-1.0532, -0.1994,  0.2479,  ..., -1.9607, -0.5805,  0.6736],\n",
      "        [-1.0528, -0.1625,  2.0446,  ..., -1.5198, -0.6133,  0.6703],\n",
      "        ...,\n",
      "        [-1.0563, -0.2178,  0.2386,  ..., -1.5258, -0.6342,  0.6816],\n",
      "        [-1.0587, -0.1588,  0.2570,  ..., -1.5089, -0.6018,  0.6613],\n",
      "        [-1.0607, -0.1287,  0.2851,  ..., -1.9910, -0.6250,  0.6543]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.3785,  2.5314,  2.1847,  ..., -0.6046, -0.1719,  0.6167],\n",
      "        [-0.3796,  2.5012,  2.1765,  ..., -1.2008, -0.2280,  0.6210],\n",
      "        [-0.3777,  4.1502,  2.1704,  ..., -0.5491, -0.2270,  0.6388],\n",
      "        ...,\n",
      "        [-0.3787,  2.5541,  2.1938,  ..., -0.5869, -0.2151,  0.6001],\n",
      "        [-0.3784,  2.5345,  2.1855,  ..., -0.5425, -0.1983,  0.6113],\n",
      "        [-0.3774,  2.4964,  2.7265,  ..., -0.6154, -0.2621,  0.6522]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[0.8006, 3.9864, 2.9288,  ..., 0.1583, 0.0683, 0.1674],\n",
      "        [0.7902, 3.9956, 2.8932,  ..., 0.1112, 0.0985, 0.1830],\n",
      "        [0.3920, 4.6849, 2.8940,  ..., 0.0922, 0.0284, 0.1542],\n",
      "        ...,\n",
      "        [0.7925, 3.9941, 2.8933,  ..., 0.1839, 0.1147, 0.1645],\n",
      "        [0.7910, 3.9847, 2.8930,  ..., 0.1722, 0.0801, 0.1888],\n",
      "        [0.7858, 4.0024, 2.8940,  ..., 0.1156, 0.1040, 0.1956]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.8029,  5.0582,  2.9385,  ...,  0.8848,  0.3264, -0.7353],\n",
      "        [ 1.4998,  5.1051,  2.9368,  ...,  0.8798,  0.3415, -0.7434],\n",
      "        [ 1.4684,  5.0587,  2.9395,  ...,  0.9345,  0.3319, -0.7133],\n",
      "        ...,\n",
      "        [ 2.8010,  5.1053,  2.9376,  ..., -0.0261,  0.3579, -0.7720],\n",
      "        [ 2.7753,  5.1068,  2.9388,  ...,  0.9452,  0.3661, -0.7347],\n",
      "        [ 2.7861,  5.1064,  2.9382,  ...,  0.8670,  0.3138, -0.7292]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 3.3056,  5.4463,  2.7533,  ...,  1.3535,  0.5359, -1.0663],\n",
      "        [ 3.3233,  5.4537,  2.7473,  ...,  1.4082,  0.5457, -1.0446],\n",
      "        [ 3.3158,  5.4539,  2.7473,  ...,  1.3362,  0.5065, -1.0468],\n",
      "        ...,\n",
      "        [ 3.3312,  5.4524,  2.7474,  ...,  1.3666,  0.4948, -1.0405],\n",
      "        [ 3.3116,  5.4530,  2.7474,  ...,  1.3385,  0.5041, -1.0347],\n",
      "        [ 3.3231,  5.4530,  2.7473,  ...,  0.3551,  0.5397, -1.0363]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.6692,  4.9033,  2.1171,  ...,  1.4196,  0.5394, -0.8325],\n",
      "        [ 2.6888,  4.9037,  2.1124,  ...,  1.4393,  0.5437, -0.8404],\n",
      "        [ 2.6516,  4.9035,  2.1151,  ...,  1.4515,  0.5583, -0.8219],\n",
      "        ...,\n",
      "        [ 1.4623,  4.9035,  2.1143,  ...,  1.4291,  0.5422, -0.8417],\n",
      "        [ 2.6524,  4.9047,  2.1024,  ...,  1.5256,  0.6105, -0.8096],\n",
      "        [ 2.6591,  4.9041,  2.1077,  ...,  1.5205,  0.5500, -0.8261]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.2108,  3.6435,  2.0436,  ...,  0.2269,  0.4558, -0.2155],\n",
      "        [ 1.1965,  3.6452,  1.0902,  ...,  1.1692,  0.4645, -0.2027],\n",
      "        [ 1.2014,  3.6325,  1.0719,  ...,  1.1117,  0.4244, -0.1940],\n",
      "        ...,\n",
      "        [ 1.1905,  4.3065,  1.0695,  ...,  1.1803,  0.4620, -0.2054],\n",
      "        [ 1.2148,  3.6453,  1.0867,  ...,  1.1865,  0.4842, -0.2266],\n",
      "        [ 1.1994,  3.6377,  2.0413,  ...,  1.1357,  0.4403, -0.2018]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[0.4879, 2.8313, 0.3209,  ..., 1.2585, 0.5281, 0.0281],\n",
      "        [0.4918, 2.8146, 0.2985,  ..., 1.2568, 0.4885, 0.0314],\n",
      "        [0.1748, 2.8120, 0.2742,  ..., 1.2740, 0.4950, 0.0345],\n",
      "        ...,\n",
      "        [0.4879, 2.8433, 0.3317,  ..., 1.1973, 0.4595, 0.0285],\n",
      "        [0.4867, 2.8261, 0.3125,  ..., 1.2232, 0.4719, 0.0298],\n",
      "        [0.4939, 2.8099, 1.7157,  ..., 1.2089, 0.5164, 0.0401]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 1.8674e-01,  2.2801e+00, -3.8575e-01,  ...,  1.7376e+00,\n",
      "          6.3160e-01,  2.4869e-02],\n",
      "        [ 1.8565e-01,  2.3157e+00, -3.3800e-01,  ...,  1.6840e+00,\n",
      "          6.1062e-01,  1.2608e-02],\n",
      "        [ 1.8468e-01,  2.2822e+00,  1.4136e+00,  ...,  1.7686e+00,\n",
      "          6.4187e-01,  2.2351e-02],\n",
      "        ...,\n",
      "        [-3.5920e-04,  2.3052e+00, -3.6462e-01,  ...,  1.7389e+00,\n",
      "          6.2633e-01,  1.9058e-02],\n",
      "        [ 1.7841e-01,  2.2863e+00, -3.9584e-01,  ...,  1.7484e+00,\n",
      "          6.3314e-01,  3.2554e-02],\n",
      "        [ 1.8294e-01,  2.2858e+00, -3.5995e-01,  ...,  1.7566e+00,\n",
      "          6.3438e-01,  3.4071e-02]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.1978,  1.4309, -0.9807,  ...,  1.6606,  0.5949,  0.0763],\n",
      "        [-0.1984,  3.3318, -0.9638,  ...,  1.5702,  0.5451,  0.0784],\n",
      "        [-0.2337,  1.4076, -0.9679,  ...,  1.6746,  0.5924,  0.1151],\n",
      "        ...,\n",
      "        [-0.1973,  3.3194, -0.9556,  ...,  1.6791,  0.5994,  0.0818],\n",
      "        [-0.1969,  1.4749, -0.9113,  ...,  1.7080,  0.6076,  0.0679],\n",
      "        [-0.1979,  1.4327, -0.9521,  ...,  1.7110,  0.6173,  0.0819]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.4400,  0.2983, -1.3952,  ...,  0.3351,  0.4526,  0.1318],\n",
      "        [-0.4402,  0.3334, -1.3718,  ...,  0.3114,  0.3750,  0.1266],\n",
      "        [-0.4407,  0.2632, -1.4187,  ...,  1.1234,  0.3769,  0.1316],\n",
      "        ...,\n",
      "        [-0.4421,  0.3064, -1.4022,  ...,  1.0586,  0.3504,  0.1269],\n",
      "        [-0.4420,  0.3361, -1.3494,  ...,  1.1099,  0.3905,  0.1193],\n",
      "        [-0.4428,  2.8389, -1.3638,  ...,  1.1300,  0.3959,  0.1175]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.9702, -0.6046, -1.7749,  ...,  1.0576,  0.3429,  0.2928],\n",
      "        [-0.9635, -0.6008, -1.7384,  ...,  1.0171,  0.3454,  0.2993],\n",
      "        [-0.9737, -0.5977, -1.7553,  ...,  1.0918,  0.3688,  0.2887],\n",
      "        ...,\n",
      "        [-0.9657, -0.6661, -1.7970,  ...,  1.0152,  0.3811,  0.2955],\n",
      "        [-0.9579, -0.6175, -1.7484,  ...,  0.9547,  0.2994,  0.3132],\n",
      "        [-0.9674, -0.6666, -1.8017,  ...,  1.0259,  0.3386,  0.3032]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-2.0424, -1.6356, -2.3331,  ...,  1.2723,  0.3802,  0.7613],\n",
      "        [-1.3883, -1.6088, -2.3141,  ...,  1.2041,  0.4069,  0.7558],\n",
      "        [-2.0297, -1.6427, -2.3406,  ...,  1.2622,  0.3812,  0.7544],\n",
      "        ...,\n",
      "        [-2.0529, -1.6052, -2.3632,  ...,  1.2232,  0.3540,  0.7525],\n",
      "        [-2.0306, -1.5600,  0.6046,  ...,  1.2284,  0.3695,  0.7486],\n",
      "        [-2.0226, -1.5992, -2.3279,  ...,  1.2311,  0.3614,  0.7719]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-2.0719, -2.0978, -2.0930,  ...,  1.0909,  0.2828,  0.7503],\n",
      "        [-2.0828, -2.1083, -2.0937,  ...,  0.3446,  0.2826,  0.7475],\n",
      "        [-2.0971, -2.1020,  0.6845,  ...,  1.1102,  0.2877,  0.7362],\n",
      "        ...,\n",
      "        [-2.0943, -2.1050, -2.0690,  ...,  1.1255,  0.3075,  0.7465],\n",
      "        [-1.3924, -2.2084, -2.1589,  ...,  1.0599,  0.3557,  0.7428],\n",
      "        [-2.0774, -2.1261, -2.1234,  ...,  1.0846,  0.2817,  0.7479]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.1837, -2.0939, -1.0605,  ...,  0.6550,  0.1631,  0.2829],\n",
      "        [-1.1791, -2.0338, -1.0585,  ...,  0.6498,  0.1073,  0.2742],\n",
      "        [-0.8300, -2.1026, -1.0621,  ...,  0.6423,  0.0988,  0.2905],\n",
      "        ...,\n",
      "        [-0.8324, -2.1136, -1.0576,  ...,  0.6489,  0.1027,  0.2853],\n",
      "        [-1.1778, -2.1440, -1.0980,  ...,  0.6776,  0.1150,  0.2889],\n",
      "        [-0.8342, -2.1552, -1.0850,  ...,  0.6433,  0.0988,  0.2847]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.1085, -2.3559, -0.7111,  ...,  0.4512,  0.0200,  0.2507],\n",
      "        [-1.1128, -2.2935, -0.6723,  ...,  0.3986, -0.0136,  0.2434],\n",
      "        [-1.1063, -2.3378, -0.6839,  ...,  0.3908, -0.0155,  0.2408],\n",
      "        ...,\n",
      "        [-1.1067, -2.3823,  1.1112,  ...,  0.4382,  0.0042,  0.2518],\n",
      "        [-1.1117, -2.3086,  1.1305,  ...,  0.4073, -0.0042,  0.2456],\n",
      "        [-1.1056, -2.3676, -0.7225,  ...,  0.4729,  0.0198,  0.2521]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.2133,  1.3746, -0.8049,  ...,  0.3295, -0.0491,  0.5577],\n",
      "        [-1.7615, -2.7870, -0.7739,  ...,  0.2839, -0.0817,  0.5539],\n",
      "        [-1.7531, -2.7875, -0.8204,  ...,  0.2986, -0.0900,  0.5642],\n",
      "        ...,\n",
      "        [-1.7503, -2.7980, -0.7951,  ...,  0.2805, -0.0956,  0.5433],\n",
      "        [-1.2198, -2.7983, -0.8103,  ...,  0.3304, -0.0538,  0.5542],\n",
      "        [-1.7586, -2.7763, -0.8184,  ...,  0.3270, -0.0549,  0.5519]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.4866, -2.5272, -0.4296,  ...,  0.1697, -0.1039,  0.5229],\n",
      "        [-1.5121,  1.5240, -0.4105,  ...,  0.1542, -0.1180,  0.5119],\n",
      "        [-1.5133, -2.3896, -0.3753,  ...,  0.1372, -0.1229,  0.5094],\n",
      "        ...,\n",
      "        [-1.4961,  1.4791, -0.4158,  ...,  0.1689, -0.1149,  0.5211],\n",
      "        [-1.0733, -2.3838, -0.3772,  ...,  0.1339, -0.1217,  0.5084],\n",
      "        [-1.5258, -2.4395, -0.4177,  ...,  0.1853, -0.0828,  0.5012]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.6079, -1.3896,  0.2473,  ..., -0.0131, -0.1441,  0.2797],\n",
      "        [-0.6053, -1.5372,  0.1986,  ...,  0.0148, -0.1373,  0.2872],\n",
      "        [-0.6033, -1.4184,  0.2498,  ...,  0.0450, -0.1077,  0.2861],\n",
      "        ...,\n",
      "        [-0.6048, -1.4508,  0.2308,  ...,  0.0350, -0.1084,  0.2848],\n",
      "        [-0.6043, -1.4783,  0.2149,  ...,  0.0434, -0.1049,  0.2880],\n",
      "        [-0.6028, -1.5265,  0.1977,  ..., -0.4533, -0.1505,  0.2878]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.0317, -0.8097,  0.8516,  ..., -0.6129, -0.1765,  0.0122],\n",
      "        [ 0.0341, -0.8601,  0.8437,  ..., -0.1498, -0.1600,  0.0113],\n",
      "        [ 0.0350, -0.8438,  0.8478,  ..., -0.1672, -0.1548,  0.0094],\n",
      "        ...,\n",
      "        [ 0.0335, -0.8617,  1.5634,  ..., -0.1588, -0.1544,  0.0127],\n",
      "        [ 0.0340, -0.8267,  0.8482,  ..., -0.2321, -0.2101,  0.0080],\n",
      "        [ 0.0353,  2.1086,  0.8513,  ..., -0.1634, -0.1387,  0.0068]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.7369, -0.4538,  1.8643,  ..., -0.4994, -0.1616, -0.4816],\n",
      "        [ 0.7563, -0.4125,  1.8643,  ..., -0.5147, -0.2621, -0.4882],\n",
      "        [ 0.7397, -0.4366,  1.8642,  ..., -0.5492, -0.1729, -0.4811],\n",
      "        ...,\n",
      "        [ 0.7517, -0.4416,  1.8644,  ..., -0.4652, -0.2325, -0.4857],\n",
      "        [ 0.4085, -0.4101,  1.8644,  ..., -0.5081, -0.2513, -0.4887],\n",
      "        [ 0.7558, -0.4010,  1.8648,  ..., -0.4985, -0.2329, -0.4885]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.8887, -0.0266,  1.3640,  ..., -0.5099, -0.2177, -0.4806],\n",
      "        [ 0.8674, -0.0323,  1.7224,  ..., -0.5419, -0.2345, -0.4771],\n",
      "        [ 0.8744,  2.3928,  1.3632,  ..., -0.5168, -0.2294, -0.4791],\n",
      "        ...,\n",
      "        [ 0.8871, -0.0382,  1.3631,  ..., -0.5121, -0.2186, -0.4804],\n",
      "        [ 0.8868, -0.0371,  1.3655,  ..., -0.5121, -0.2248, -0.4793],\n",
      "        [ 0.8909,  2.4221,  1.3697,  ..., -0.5255, -0.2297, -0.4812]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.3821,  0.3509,  1.1055,  ..., -0.1844, -0.0488, -0.0178],\n",
      "        [ 0.3781,  0.3597, -0.4517,  ..., -0.1903, -0.0784, -0.0134],\n",
      "        [ 0.3822,  2.5335, -0.4248,  ..., -0.1010, -0.0123, -0.0146],\n",
      "        ...,\n",
      "        [ 0.3714,  0.3337, -0.4598,  ..., -0.1781, -0.0514, -0.0105],\n",
      "        [ 0.3842,  0.3762, -0.4378,  ..., -0.1859, -0.0646, -0.0158],\n",
      "        [ 0.3787,  0.3586, -0.4400,  ..., -0.1537, -0.0316, -0.0138]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.6756e-01,  6.2957e-01, -1.8184e+00,  ..., -6.8064e-02,\n",
      "          4.9653e-02,  2.3534e-01],\n",
      "        [ 2.5529e-01,  5.5841e-01, -1.9169e+00,  ..., -2.4037e-02,\n",
      "          6.6387e-02,  2.4320e-01],\n",
      "        [ 5.9928e-02,  2.6260e+00,  6.1316e-01,  ..., -3.5220e-02,\n",
      "          6.9704e-02,  2.4090e-01],\n",
      "        ...,\n",
      "        [ 2.5711e-01,  5.9508e-01, -1.8283e+00,  ..., -5.4310e-04,\n",
      "          8.5496e-02,  2.4211e-01],\n",
      "        [ 5.7791e-02,  6.0197e-01, -1.8863e+00,  ..., -3.4105e-02,\n",
      "          8.4307e-02,  2.4110e-01],\n",
      "        [ 2.5890e-01,  5.9350e-01, -1.8852e+00,  ...,  3.3742e-03,\n",
      "          8.7542e-02,  2.3979e-01]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 3.9079e-01,  7.2768e-01, -3.8765e+00,  ..., -2.4586e-02,\n",
      "          1.6625e-01,  4.9279e-01],\n",
      "        [ 4.0162e-01,  7.3302e-01, -3.8610e+00,  ...,  2.5251e-02,\n",
      "          2.0964e-01,  4.9359e-01],\n",
      "        [ 3.9219e-01,  7.2604e-01, -3.8134e+00,  ..., -4.1203e-01,\n",
      "          2.6594e-01,  4.9802e-01],\n",
      "        ...,\n",
      "        [ 3.8850e-01,  7.1532e-01, -3.8772e+00,  ..., -3.4245e-04,\n",
      "          1.9225e-01,  5.0011e-01],\n",
      "        [ 3.9490e-01,  7.3670e-01, -3.8177e+00,  ..., -1.5646e-02,\n",
      "          1.7634e-01,  4.9677e-01],\n",
      "        [ 3.9209e-01,  7.4215e-01, -3.7479e+00,  ..., -6.1835e-02,\n",
      "          1.5435e-01,  4.9685e-01]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.3536,  0.7502, -0.1259,  ...,  0.0496,  0.2244,  0.5338],\n",
      "        [ 0.3481,  0.7885, -3.9730,  ...,  0.0687,  0.2606,  0.5327],\n",
      "        [ 0.3464,  0.7811, -3.9784,  ...,  0.0442,  0.2242,  0.5372],\n",
      "        ...,\n",
      "        [ 0.3400,  0.7700, -4.0451,  ...,  0.0542,  0.2322,  0.5375],\n",
      "        [ 0.3452,  0.7858, -3.9954,  ...,  0.0390,  0.2261,  0.5359],\n",
      "        [ 0.1221,  0.7400, -4.0886,  ...,  0.0449,  0.2971,  0.5342]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.2039,  0.7641, -2.5835,  ...,  0.1056,  0.2113,  0.3093],\n",
      "        [ 0.2028,  0.7522, -2.5908,  ...,  0.1062,  0.2110,  0.3094],\n",
      "        [ 0.2016,  0.7750, -2.5909,  ...,  0.0902,  0.2726,  0.3070],\n",
      "        ...,\n",
      "        [ 0.2121,  0.7391, -2.6183,  ...,  0.1226,  0.2111,  0.3082],\n",
      "        [ 0.2091,  2.6734, -2.6214,  ...,  0.1487,  0.1975,  0.3101],\n",
      "        [ 0.2043,  0.7719, -2.5918,  ..., -0.3342,  0.1712,  0.3106]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.0566,  2.6684, -0.2342,  ...,  0.1320,  0.1056, -0.1834],\n",
      "        [ 0.0593,  0.7552, -0.2374,  ...,  0.1485,  0.1139, -0.1836],\n",
      "        [ 0.0654,  0.8008, -0.2107,  ..., -0.3250,  0.1079, -0.1844],\n",
      "        ...,\n",
      "        [ 0.0561,  0.8005, -0.2055,  ...,  0.1410,  0.1246, -0.1835],\n",
      "        [ 0.0616,  0.7960, -0.2231,  ...,  0.1517,  0.1124, -0.1842],\n",
      "        [-0.0776,  0.8279, -0.2042,  ...,  0.1054,  0.0766, -0.1837]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-1.5188e-01,  5.6246e-01,  1.8605e+00,  ...,  1.4566e-01,\n",
      "          4.7465e-03, -4.4467e-01],\n",
      "        [-1.5351e-01,  5.6697e-01,  1.8605e+00,  ...,  1.9973e-01,\n",
      "          3.7723e-02, -4.4498e-01],\n",
      "        [-1.5623e-01,  5.6022e-01,  1.8606e+00,  ...,  1.4095e-01,\n",
      "          2.5548e-03, -4.4567e-01],\n",
      "        ...,\n",
      "        [-1.5793e-01,  5.4289e-01,  1.8616e+00,  ...,  1.7052e-01,\n",
      "          1.2021e-01, -4.4604e-01],\n",
      "        [-1.5696e-01,  2.5749e+00,  1.8602e+00,  ...,  1.5367e-01,\n",
      "          1.2841e-02, -4.4563e-01],\n",
      "        [-1.5423e-01,  5.6028e-01,  1.8611e+00,  ...,  1.8834e-01,\n",
      "          2.5482e-02, -4.4496e-01]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-5.0623e-01,  4.5999e-02,  4.4098e+00,  ...,  2.7724e-01,\n",
      "         -3.6123e-02, -5.8425e-01],\n",
      "        [-5.0495e-01, -2.9403e-02,  4.4585e+00,  ...,  3.2221e-01,\n",
      "          4.2124e-03, -5.8675e-01],\n",
      "        [-5.0594e-01,  2.0622e-02,  4.4355e+00,  ...,  2.9556e-01,\n",
      "         -2.7462e-02, -5.8589e-01],\n",
      "        ...,\n",
      "        [-5.0579e-01, -1.8394e-02,  2.6504e+00,  ...,  2.9659e-01,\n",
      "         -2.5678e-02, -5.8644e-01],\n",
      "        [-5.0572e-01, -1.1048e-03,  4.4299e+00,  ...,  3.3584e-01,\n",
      "         -7.9153e-03, -5.8648e-01],\n",
      "        [-5.0543e-01, -3.3150e-02,  4.4511e+00,  ...,  3.4512e-01,\n",
      "          2.3175e-02, -5.8688e-01]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.5349, -0.1581,  5.2269,  ...,  0.3134, -0.0651, -0.6917],\n",
      "        [-0.6053, -0.2259,  5.2763,  ...,  0.3230, -0.0776, -0.6949],\n",
      "        [-0.6056, -0.1746,  5.2301,  ...,  0.3415, -0.0360, -0.6929],\n",
      "        ...,\n",
      "        [-0.6062, -0.1389,  2.9123,  ...,  0.3154, -0.0681, -0.6914],\n",
      "        [-0.6052, -0.1380,  5.1899,  ...,  0.2803, -0.0866, -0.6931],\n",
      "        [-0.6093, -0.1959,  2.8967,  ...,  0.3196, -0.0723, -0.6913]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-4.5279e-01,  3.3794e-02,  4.4531e+00,  ...,  2.1292e-01,\n",
      "         -1.2019e-01, -7.3811e-01],\n",
      "        [-4.5369e-01,  1.4153e-02,  4.4548e+00,  ...,  2.2991e-01,\n",
      "         -1.1177e-01, -7.3244e-01],\n",
      "        [-4.5288e-01, -2.2501e-02,  4.5037e+00,  ..., -1.7355e-01,\n",
      "         -7.0997e-02, -7.3669e-01],\n",
      "        ...,\n",
      "        [-4.5312e-01,  3.1227e-02,  4.4499e+00,  ...,  2.6998e-01,\n",
      "         -6.6970e-02, -7.3473e-01],\n",
      "        [-4.5347e-01,  6.7342e-04,  2.6345e+00,  ...,  2.5010e-01,\n",
      "         -9.8306e-02, -7.3504e-01],\n",
      "        [-4.5358e-01, -1.9568e-02,  4.4875e+00,  ...,  2.7142e-01,\n",
      "         -7.7526e-02, -7.3330e-01]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.0662,  0.5009,  2.5379,  ...,  0.1135, -0.0809, -0.7373],\n",
      "        [-0.0651,  0.5312,  2.5283,  ...,  0.1299, -0.0108, -0.7353],\n",
      "        [-0.0683,  0.5627,  2.5159,  ...,  0.0809, -0.0996, -0.7370],\n",
      "        ...,\n",
      "        [-0.0686,  2.5213,  2.5313,  ...,  0.0604, -0.1328, -0.7356],\n",
      "        [-0.1750,  0.5254,  2.5258,  ...,  0.1063, -0.0072, -0.7367],\n",
      "        [-0.0712,  0.5106,  2.5302,  ..., -0.3299, -0.1118, -0.7402]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.1004,  0.5642,  1.3656,  ...,  0.0124, -0.1408, -0.6789],\n",
      "        [ 0.0913,  0.5587,  1.3642,  ...,  0.0160, -0.1476, -0.6821],\n",
      "        [-0.0740,  0.5800,  1.3658,  ...,  0.0390, -0.1022, -0.6809],\n",
      "        ...,\n",
      "        [ 0.0971,  0.5821,  1.4626,  ..., -0.4034, -0.1339, -0.6785],\n",
      "        [ 0.0922,  0.5843,  1.3657,  ...,  0.0314, -0.1067, -0.6807],\n",
      "        [ 0.0930,  0.5456,  1.3631,  ...,  0.0525, -0.1059, -0.6821]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.0833,  2.3732,  0.5327,  ...,  0.0210, -0.1790, -0.5484],\n",
      "        [ 0.0907,  0.1589,  0.5253,  ...,  0.0587, -0.1314, -0.5435],\n",
      "        [ 0.0881,  0.1784,  0.5338,  ...,  0.0216, -0.1669, -0.5454],\n",
      "        ...,\n",
      "        [ 0.0837,  0.1981,  1.1452,  ...,  0.0062, -0.1875, -0.5468],\n",
      "        [ 0.0938,  0.1937,  0.5339,  ...,  0.0477, -0.1462, -0.5410],\n",
      "        [-0.0840,  0.1947,  0.5389,  ..., -0.3954, -0.1865, -0.5429]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.0555, -0.5282, -0.1669,  ..., -0.3465, -0.1396, -0.3844],\n",
      "        [-0.0560, -0.6001, -0.1947,  ...,  0.1219, -0.2134, -0.3859],\n",
      "        [-0.0541, -0.5316, -0.1611,  ...,  0.0798, -0.2439, -0.3834],\n",
      "        ...,\n",
      "        [-0.1905, -0.5344, -0.1628,  ...,  0.0978, -0.2192, -0.3864],\n",
      "        [-0.0537, -0.5325, -0.1644,  ..., -0.3598, -0.2412, -0.3842],\n",
      "        [-0.0556, -0.5545, -0.1646,  ...,  0.0985, -0.1366, -0.3874]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.0948, -0.8874, -0.7148,  ..., -0.3576, -0.2656, -0.1606],\n",
      "        [-0.0943,  1.9656, -0.7161,  ..., -0.3465, -0.2422, -0.1604],\n",
      "        [-0.0985, -0.9711, -0.7550,  ...,  0.1138, -0.2471, -0.1627],\n",
      "        ...,\n",
      "        [-0.0970, -0.9159, -0.7229,  ...,  0.0974, -0.2532, -0.1642],\n",
      "        [-0.0916, -0.8688, -0.6987,  ...,  0.1209, -0.2174, -0.1580],\n",
      "        [-0.1041, -0.9301, -0.7211,  ...,  0.1086, -0.2363, -0.1671]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.0877, -1.1114, -1.0683,  ..., -0.3470, -0.1770,  0.1844],\n",
      "        [-0.2272, -1.1532, -1.0819,  ...,  0.1023, -0.1692,  0.1837],\n",
      "        [-0.0866, -1.1131, -1.0540,  ...,  0.0835, -0.1904,  0.1863],\n",
      "        ...,\n",
      "        [-0.0854, -1.0777, -1.0410,  ...,  0.0698, -0.1903,  0.1856],\n",
      "        [-0.0874, -1.0966, -1.0671,  ...,  0.1205, -0.1531,  0.1858],\n",
      "        [-0.0873, -1.1144, -1.0758,  ...,  0.0867, -0.1774,  0.1852]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.0885, -1.3565, -1.1784,  ..., -0.3727, -0.0685,  0.7056],\n",
      "        [-0.0840, -1.2847, -1.1379,  ...,  0.0662, -0.0417,  0.7107],\n",
      "        [-0.0818,  1.7922, -1.1601,  ...,  0.0665, -0.0496,  0.7105],\n",
      "        ...,\n",
      "        [-0.0864, -1.3129, -1.1292,  ..., -0.3916, -0.0904,  0.7098],\n",
      "        [-0.0865,  1.7665, -1.1894,  ...,  0.0834, -0.0435,  0.7084],\n",
      "        [-0.0829, -1.2928, -1.1483,  ...,  0.0555, -0.0712,  0.7106]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-3.2871e-03, -1.2267e+00, -1.3737e+00,  ...,  2.2230e-02,\n",
      "          1.1451e-02,  9.1922e-01],\n",
      "        [-9.4180e-03, -1.2226e+00, -1.3465e+00,  ...,  5.5732e-02,\n",
      "          5.5917e-02,  9.1592e-01],\n",
      "        [ 4.7581e-03, -1.2491e+00, -1.3460e+00,  ...,  4.4508e-02,\n",
      "          1.9725e-02,  9.2079e-01],\n",
      "        ...,\n",
      "        [-1.3413e-03, -1.1541e+00, -1.3228e+00,  ...,  1.1366e-02,\n",
      "          1.7095e-02,  9.2074e-01],\n",
      "        [-7.3024e-04, -1.1749e+00, -1.3012e+00,  ...,  6.1324e-02,\n",
      "          7.7997e-02,  9.2072e-01],\n",
      "        [-2.2246e-03, -1.2330e+00, -1.3681e+00,  ...,  5.0729e-02,\n",
      "          4.0240e-02,  9.1840e-01]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 2.0593e-01, -5.8118e-01, -1.5881e+00,  ..., -3.2802e-02,\n",
      "          8.9852e-02,  9.0269e-01],\n",
      "        [ 2.0308e-01, -5.8320e-01, -1.5708e+00,  ..., -1.5195e-02,\n",
      "          1.2066e-01,  9.0157e-01],\n",
      "        [ 2.1128e-01, -6.0818e-01, -1.5890e+00,  ..., -2.6023e-02,\n",
      "          9.8025e-02,  9.0161e-01],\n",
      "        ...,\n",
      "        [ 2.0007e-01, -5.7813e-01, -1.5817e+00,  ..., -1.5816e-02,\n",
      "          1.2156e-01,  9.0069e-01],\n",
      "        [ 2.0929e-01, -5.8295e-01, -1.5841e+00,  ..., -1.6119e-02,\n",
      "          1.1756e-01,  9.0285e-01],\n",
      "        [-6.6374e-02, -6.3368e-01, -1.6088e+00,  ..., -5.1443e-04,\n",
      "          1.3150e-01,  8.9992e-01]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.5787,  2.4693, -2.1395,  ..., -0.0566,  0.3300,  0.8499],\n",
      "        [ 0.5612,  0.5540,  0.0987,  ..., -0.5073,  0.2258,  0.8465],\n",
      "        [ 0.5786,  0.5792, -2.1197,  ..., -0.0772,  0.2423,  0.8497],\n",
      "        ...,\n",
      "        [ 0.5843,  0.5638, -2.1166,  ..., -0.1232,  0.1972,  0.8497],\n",
      "        [ 0.5660,  0.5639, -2.1230,  ..., -0.0777,  0.2475,  0.8465],\n",
      "        [ 0.5789,  0.5455, -2.1401,  ..., -0.0889,  0.2323,  0.8476]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.2618,  1.2290, -2.0788,  ..., -0.5467,  0.2892,  0.6055],\n",
      "        [ 0.7225,  1.2351, -2.0756,  ..., -0.1207,  0.2926,  0.6055],\n",
      "        [ 0.7417,  1.2122, -2.1213,  ..., -0.1069,  0.2958,  0.6071],\n",
      "        ...,\n",
      "        [ 0.7371,  1.2454, -2.0800,  ..., -0.1283,  0.2797,  0.6063],\n",
      "        [ 0.7237,  1.2390, -2.1019,  ..., -0.1000,  0.2930,  0.6046],\n",
      "        [ 0.7396,  1.2194, -2.1141,  ..., -0.1157,  0.2884,  0.6057]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.7047,  1.5102, -1.5092,  ..., -0.5439,  0.3492,  0.0884],\n",
      "        [ 0.7169,  1.4743, -1.5314,  ..., -0.1071,  0.2533,  0.0873],\n",
      "        [ 0.7078,  1.5046, -1.4938,  ..., -0.1071,  0.2743,  0.0883],\n",
      "        ...,\n",
      "        [ 0.6918,  1.4718,  0.3543,  ..., -0.1255,  0.2226,  0.0832],\n",
      "        [ 0.2319,  1.4746, -1.5311,  ..., -0.0486,  0.2877,  0.0861],\n",
      "        [ 0.7206,  1.4976, -1.5024,  ..., -0.1007,  0.2656,  0.0886]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.5288,  1.5346, -0.4730,  ..., -0.0553,  0.1576, -0.7657],\n",
      "        [ 0.5253,  1.5467, -0.4793,  ..., -0.0588,  0.1647, -0.7652],\n",
      "        [ 0.5171,  1.5641, -0.4525,  ..., -0.0628,  0.1560, -0.7663],\n",
      "        ...,\n",
      "        [ 0.5234,  1.5119, -0.5080,  ..., -0.0233,  0.2808, -0.7664],\n",
      "        [ 0.5071,  1.5092, -0.4998,  ..., -0.0553,  0.1381, -0.7694],\n",
      "        [ 0.1271,  1.5332, -0.4956,  ..., -0.0325,  0.1817, -0.7651]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 4.0261e-01,  1.5292e+00,  1.0012e+00,  ..., -1.8218e-02,\n",
      "          1.3868e-01, -1.1200e+00],\n",
      "        [ 4.1684e-01,  1.5320e+00,  9.8875e-02,  ..., -2.7379e-03,\n",
      "          1.3652e-01, -1.1197e+00],\n",
      "        [ 4.2174e-01,  2.7933e+00,  1.0023e-01,  ...,  3.2559e-03,\n",
      "          1.5509e-01, -1.1187e+00],\n",
      "        ...,\n",
      "        [ 5.9919e-02,  1.5190e+00,  9.1046e-02,  ...,  2.7416e-03,\n",
      "          1.5212e-01, -1.1190e+00],\n",
      "        [ 4.0493e-01,  1.5432e+00,  1.0264e-01,  ...,  1.1692e-02,\n",
      "          1.6421e-01, -1.1197e+00],\n",
      "        [ 4.1676e-01,  1.5493e+00,  1.0153e+00,  ..., -4.9591e-01,\n",
      "          1.2498e-01, -1.1189e+00]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 3.1736e-01,  1.3886e+00,  3.6050e-01,  ..., -5.2136e-02,\n",
      "          1.0156e-01, -1.0519e+00],\n",
      "        [ 3.1652e-01,  1.3479e+00,  3.3373e-01,  ...,  6.7901e-03,\n",
      "          1.4461e-01, -1.0522e+00],\n",
      "        [ 1.2423e-03,  1.3719e+00,  3.5137e-01,  ...,  1.1426e-02,\n",
      "          1.5015e-01, -1.0513e+00],\n",
      "        ...,\n",
      "        [ 3.2225e-01,  1.3777e+00,  3.4446e-01,  ...,  8.7582e-03,\n",
      "          1.5858e-01, -1.0507e+00],\n",
      "        [ 3.1309e-01,  1.3675e+00,  3.4047e-01,  ..., -1.2334e-02,\n",
      "          1.3452e-01, -1.0519e+00],\n",
      "        [ 2.5291e-03,  1.3648e+00,  3.4813e-01,  ..., -4.8492e-01,\n",
      "          1.0951e-01, -1.0525e+00]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.2218,  1.0194,  0.3756,  ..., -0.0113,  0.1856, -0.6306],\n",
      "        [ 0.2197,  1.0110,  1.1349,  ..., -0.0288,  0.1674, -0.6313],\n",
      "        [ 0.2167,  1.0376,  0.3852,  ..., -0.0391,  0.1611, -0.6307],\n",
      "        ...,\n",
      "        [ 0.2291,  1.0196,  0.3752,  ..., -0.0095,  0.2573, -0.6293],\n",
      "        [ 0.2220,  1.0586,  0.3909,  ..., -0.0438,  0.1650, -0.6305],\n",
      "        [ 0.2197,  1.0263,  0.3781,  ..., -0.0593,  0.1282, -0.6319]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[ 0.1013,  2.4689,  0.3415,  ..., -0.0801,  0.2565, -0.0074],\n",
      "        [ 0.1045,  0.6643,  1.1298,  ..., -0.0864,  0.2585, -0.0078],\n",
      "        [ 0.1105,  0.6515,  0.3478,  ..., -0.0864,  0.2516, -0.0059],\n",
      "        ...,\n",
      "        [ 0.1065,  0.6614,  0.3439,  ..., -0.0910,  0.2659, -0.0048],\n",
      "        [ 0.1015,  0.6454,  1.1261,  ..., -0.0928,  0.2475, -0.0070],\n",
      "        [ 0.1051,  0.6201,  0.3302,  ..., -0.1006,  0.2360, -0.0072]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.0066,  0.1136,  0.1889,  ..., -0.1010,  0.2490,  0.5885],\n",
      "        [-0.0056,  0.0792,  0.1837,  ..., -0.0974,  0.2597,  0.5881],\n",
      "        [-0.0053,  0.0937,  0.1835,  ..., -0.1057,  0.2512,  0.5890],\n",
      "        ...,\n",
      "        [-0.0076,  0.1078,  0.1887,  ..., -0.1111,  0.2502,  0.5866],\n",
      "        [-0.1854,  0.0754,  0.1817,  ..., -0.1141,  0.2248,  0.5877],\n",
      "        [-0.0089,  0.1062,  1.0706,  ..., -0.1003,  0.2549,  0.5877]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.1278, -0.6229, -0.1443,  ..., -0.0970,  0.1947,  1.3483],\n",
      "        [-0.1316, -0.6666,  0.9305,  ..., -0.0750,  0.2883,  1.3435],\n",
      "        [-0.1304, -0.6543, -0.1539,  ..., -0.1044,  0.1760,  1.3435],\n",
      "        ...,\n",
      "        [-0.2543, -0.5985, -0.1386,  ..., -0.1277,  0.1523,  1.3456],\n",
      "        [-0.1311, -0.6829, -0.1691,  ..., -0.0760,  0.2008,  1.3438],\n",
      "        [-0.2576, -0.7131,  0.9428,  ..., -0.0802,  0.1804,  1.3442]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.2097, -0.9926, -0.2248,  ..., -0.6046,  0.1585,  1.6493],\n",
      "        [-0.3028, -1.0516, -0.2528,  ..., -0.0848,  0.1758,  1.6472],\n",
      "        [-0.2077, -1.0718,  0.9167,  ..., -0.0704,  0.2640,  1.6503],\n",
      "        ...,\n",
      "        [-0.2081, -1.0181, -0.2399,  ..., -0.1066,  0.1520,  1.6489],\n",
      "        [-0.2079, -1.0879, -0.2640,  ..., -0.0653,  0.1946,  1.6496],\n",
      "        [-0.2071, -1.0458, -0.2552,  ..., -0.0897,  0.1734,  1.6522]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.2502,  1.9079, -0.0666,  ..., -0.0872,  0.2080,  1.5080],\n",
      "        [-0.2513, -1.0528, -0.0616,  ..., -0.0935,  0.2015,  1.5073],\n",
      "        [-0.2480, -1.1014, -0.0717,  ..., -0.1179,  0.1606,  1.5077],\n",
      "        ...,\n",
      "        [-0.3246, -1.0403, -0.0574,  ..., -0.1091,  0.1933,  1.5126],\n",
      "        [-0.2466, -1.0870,  1.0223,  ..., -0.6330,  0.1778,  1.5110],\n",
      "        [-0.2495, -1.0763, -0.0661,  ..., -0.1136,  0.1673,  1.5063]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.2642,  1.9855,  0.4024,  ..., -0.0959,  0.2846,  0.9794],\n",
      "        [-0.2640, -0.7947,  0.4147,  ..., -0.1044,  0.2702,  0.9788],\n",
      "        [-0.2631, -0.8339,  0.4072,  ..., -0.0924,  0.2744,  0.9790],\n",
      "        ...,\n",
      "        [-0.2639, -0.8455,  0.4033,  ..., -0.0816,  0.2847,  0.9783],\n",
      "        [-0.2634, -0.7888,  0.4232,  ..., -0.0964,  0.2845,  0.9801],\n",
      "        [-0.3343, -0.8260,  0.4140,  ..., -0.1015,  0.2705,  0.9798]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.2799, -0.7708,  0.5098,  ..., -0.1104,  0.2773,  0.8471],\n",
      "        [-0.2790, -0.8516,  0.4838,  ..., -0.0869,  0.3519,  0.8455],\n",
      "        [-0.2792, -0.7942,  0.4942,  ..., -0.0796,  0.2985,  0.8477],\n",
      "        ...,\n",
      "        [-0.2776, -0.8312,  0.4871,  ..., -0.1116,  0.2472,  0.8471],\n",
      "        [-0.3462, -0.8147,  0.4879,  ..., -0.0889,  0.3682,  0.8461],\n",
      "        [-0.2821, -0.7560,  0.4975,  ..., -0.1162,  0.2849,  0.8453]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.2980, -1.0953,  0.1795,  ..., -0.0462,  0.2597,  1.0490],\n",
      "        [-0.2949, -1.1431,  0.1683,  ..., -0.0739,  0.2240,  1.0511],\n",
      "        [-0.3554, -1.1487,  0.1737,  ..., -0.0671,  0.2322,  1.0481],\n",
      "        ...,\n",
      "        [-0.2975, -1.1261,  0.1811,  ..., -0.0516,  0.2526,  1.0484],\n",
      "        [-0.2962, -1.1552,  1.1324,  ..., -0.0712,  0.2267,  1.0496],\n",
      "        [-0.2961, -1.1128,  0.1852,  ..., -0.0779,  0.2882,  1.0486]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.3244,  1.7428,  0.8839,  ..., -0.0459,  0.1236,  1.5846],\n",
      "        [-0.3235, -1.7160, -0.4546,  ..., -0.0494,  0.1193,  1.5867],\n",
      "        [-0.3236, -1.7108, -0.4624,  ..., -0.0636,  0.0998,  1.5857],\n",
      "        ...,\n",
      "        [-0.3264, -1.7158, -0.4635,  ..., -0.6125,  0.0897,  1.5783],\n",
      "        [-0.3256, -1.7461, -0.4690,  ..., -0.0400,  0.1174,  1.5814],\n",
      "        [-0.3258, -1.6800, -0.4508,  ..., -0.0748,  0.1067,  1.5821]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-460adf066ba5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m#     val_loss = evaluate(model, val_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#     val_ppl = math.exp(val_loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-93a2c7cd62c1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_last_lr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 50\n",
    "best_model = None\n",
    "\n",
    "begin_time = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "#     val_loss = evaluate(model, val_data)\n",
    "#     val_ppl = math.exp(val_loss)\n",
    "#     elapsed = time.time() - epoch_start_time\n",
    "#     print('-' * 89)\n",
    "#     print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "#           f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "#     print('-' * 89)\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(time.time() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "261120it [07:03, 616.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.98628712 7.98628902 7.98628807 ... 7.98629284 7.98628521 7.98629093]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is 0.17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1717398998418664"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# DataLoader Parameters\n",
    "loader_params = {\n",
    "    \"batch_size\":  1, \n",
    "    \"shuffle\":     False,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "\n",
    "# Create test DataLoader\n",
    "test_loader = DataLoader(test_dataset, **loader_params)\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialise arrays and dict\n",
    "    predictions = np.array([])\n",
    "    labels = np.array([])\n",
    "    averaged_dict = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (signal, label) in tqdm(enumerate(test_loader)):\n",
    "            # Send input to device\n",
    "            signal = torch.Tensor(signal).to(device)\n",
    "            signal = torch.unsqueeze(signal, 0)\n",
    "             \n",
    "            # Get output of net, append to lists\n",
    "            output = model(signal).cpu().detach().numpy()\n",
    "            output = output[0][0]\n",
    "            predictions = np.append(predictions, output)            \n",
    "            labels = np.append(labels, label)\n",
    "#             print(output, \"\\n\")\n",
    "#             print(output, label)\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] not in averaged_dict:\n",
    "                averaged_dict[labels[i]] = [predictions[i]]\n",
    "            else:\n",
    "                averaged_dict[labels[i]].append(predictions[i])\n",
    "            \n",
    "    for i in averaged_dict:\n",
    "        averaged_dict[i] = np.mean(averaged_dict[i])\n",
    "    \n",
    "    averaged_predictions = []\n",
    "    ordered_labels = []\n",
    "        \n",
    "    for i in averaged_dict:\n",
    "        ordered_labels.append(i)\n",
    "        averaged_predictions.append(averaged_dict[i])\n",
    "    \n",
    "    print(predictions)\n",
    "    print(labels)\n",
    "    \n",
    "#     r = scipy.stats.pearsonr(predictions, labels)\n",
    "    \n",
    "    r = stats.pearsonr(averaged_predictions, ordered_labels)\n",
    "    \n",
    "    print('Pearson r of the model is %.2f' % r[0])\n",
    "    return r[0]\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch  3.42 | loss 92.58 | ppl 16100028925287311301350875111210690281472.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch  3.24 | loss 85.33 | ppl 11455713630718390626473127553364131840.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch  3.26 | loss 82.97 | ppl 1077043320158796336044229770602348544.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch  3.24 | loss 81.36 | ppl 216755044073410641543040811992612864.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch  3.25 | loss 81.82 | ppl 340941116277746081024917681024794624.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch  3.23 | loss 81.53 | ppl 255088760811615562379166751957975040.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch  3.25 | loss 81.75 | ppl 319270115387276113419745448691761152.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch  3.24 | loss 81.03 | ppl 154841679147969441814124579803627520.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch  3.26 | loss 81.35 | ppl 212861861180462809998693135351283712.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch  3.22 | loss 80.63 | ppl 103782486209703131615633588283768832.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch  3.23 | loss 80.78 | ppl 121223679116630367977328952691654656.00\n",
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch  3.25 | loss 81.22 | ppl 188101469882215013980681520136847360.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch  3.27 | loss 81.06 | ppl 159393838991365518511636059059650560.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch  3.25 | loss 80.67 | ppl 108134372065187084769776710392479744.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch  3.25 | loss 80.95 | ppl 143602821159950428129989764208132096.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch  3.22 | loss 80.66 | ppl 107010230980549814259901124304699392.00\n",
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch  3.27 | loss 80.67 | ppl 108030162797723870410359303833649152.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch  3.25 | loss 80.69 | ppl 110788747144680901841180683568939008.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch  3.26 | loss 80.69 | ppl 110759981153975089102812815091040256.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch  3.30 | loss 80.65 | ppl 105992900167645759886283179531698176.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch  3.27 | loss 80.29 | ppl 74137772631856784972200873977446400.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch  3.23 | loss 80.98 | ppl 147563844260426365676361866564075520.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch  3.25 | loss 80.56 | ppl 97445070054619800573794853027053568.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch  3.24 | loss 80.87 | ppl 132188319363622327095806748367257600.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch  3.28 | loss 80.40 | ppl 82517413654785271715702317751730176.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch  3.24 | loss 80.76 | ppl 118157681207219645619613999048949760.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch  3.21 | loss 80.28 | ppl 73622789583022555698629279351832576.00\n",
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch  3.26 | loss 80.37 | ppl 80467153800493004396878287480029184.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch  3.24 | loss 79.99 | ppl 54958069826546716616170334165401600.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch  3.23 | loss 80.56 | ppl 97200262806000172282441452578930688.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch  3.28 | loss 80.32 | ppl 76067169673052372229831531364876288.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch  3.25 | loss 80.52 | ppl 92915616276892531727038103110549504.00\n",
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch  3.24 | loss 81.23 | ppl 189684994256260883255182341055709184.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch  3.26 | loss 79.63 | ppl 38224468401091010822742148552589312.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch  3.24 | loss 80.87 | ppl 132013459111472745004534460577742848.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch  3.23 | loss 79.97 | ppl 53607791321979771514267517145055232.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch  3.28 | loss 79.73 | ppl 42140851217539204377314557098459136.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch  3.22 | loss 80.09 | ppl 60514037840531144323078653122445312.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch  3.25 | loss 80.38 | ppl 80826854304447809064448375157424128.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch  3.27 | loss 80.07 | ppl 59712627429829207736159072608583680.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch  3.25 | loss 80.67 | ppl 107777646766654323404085854665179136.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch  3.28 | loss 80.24 | ppl 70143979192705709764077901462896640.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch  3.27 | loss 80.55 | ppl 96508238114405882667064872849637376.00\n",
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch  3.24 | loss 79.94 | ppl 52271463985733043212226405655904256.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch  3.25 | loss 80.29 | ppl 73723453214953508296235981348012032.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch  3.26 | loss 80.27 | ppl 72760840976046770992834842644709376.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch  3.24 | loss 80.17 | ppl 65882380898466095261453505634762752.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch  3.27 | loss 79.91 | ppl 50788713597085815791079441580425216.00\n",
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch  3.26 | loss 80.65 | ppl 106053145432064590673311621514788864.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch  3.31 | loss 80.09 | ppl 60543091477122202849948123283849216.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch  3.25 | loss 80.62 | ppl 102776088138213426765720043906924544.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch  3.28 | loss 80.04 | ppl 57868489527470566389069356046745600.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch  3.30 | loss 79.81 | ppl 45760726158234540547273778540838912.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch  3.23 | loss 80.49 | ppl 90867968984984728612532436393263104.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch  3.26 | loss 80.46 | ppl 87351458292515632114013675458658304.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch  3.24 | loss 79.97 | ppl 53518806700654342577764676317216768.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch  3.24 | loss 80.76 | ppl 118334032882622100430087727874048000.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch  3.24 | loss 79.76 | ppl 43764299770618332989406195176439808.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch  3.25 | loss 80.25 | ppl 70824121636264084989225250756493312.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch  3.27 | loss 79.96 | ppl 53342354093592758977764817264181248.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch  3.23 | loss 80.00 | ppl 55501557216097412390413813019574272.00\n",
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch  3.29 | loss 80.18 | ppl 66185454142692086902521032859451392.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch  3.23 | loss 80.23 | ppl 70001352214846847874765582545977344.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch  3.23 | loss 80.22 | ppl 68939535002007974282229289756655616.00\n",
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch  3.25 | loss 80.12 | ppl 62165277800766019851160216389287936.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch  3.28 | loss 79.80 | ppl 45494061548927529170258587585347584.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch  3.25 | loss 80.25 | ppl 70844866579541590467286161697013760.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch  3.30 | loss 80.11 | ppl 62029059747923145144806166751084544.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch  3.22 | loss 79.72 | ppl 41965427938510870886571819973738496.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch  3.24 | loss 79.92 | ppl 51109179225013272587111609212600320.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch  3.27 | loss 79.89 | ppl 49737454510078287593684151318872064.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch  3.22 | loss 80.04 | ppl 57521908676649670357711489112997888.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch  3.21 | loss 80.52 | ppl 93344909671356342258398770313035776.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch  3.27 | loss 79.71 | ppl 41460643275870174933355277343260672.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch  3.24 | loss 79.94 | ppl 52404821219023020860353409739390976.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch  3.25 | loss 80.20 | ppl 67851335120070213201786785029423104.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch  3.30 | loss 79.49 | ppl 33228151714756309313696399605366784.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch  3.27 | loss 80.14 | ppl 64028990257113939193576147128418304.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch  3.23 | loss 79.90 | ppl 50301439974539137757235109926273024.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch  3.25 | loss 80.18 | ppl 66079491171851594518950767774138368.00\n",
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch  3.27 | loss 79.96 | ppl 53277853053190937629184432866852864.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch  3.25 | loss 80.21 | ppl 68119029749628180062592543085297664.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch  3.24 | loss 79.47 | ppl 32658948683328080995785308349923328.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch  3.24 | loss 80.10 | ppl 61258528328098998565681513680076800.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch  3.26 | loss 80.26 | ppl 71656629751359969171583964364668928.00\n",
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch  3.23 | loss 80.21 | ppl 68442439214381164592550161533632512.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch  3.25 | loss 79.72 | ppl 41921856612781220966480466430394368.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch  3.19 | loss 80.27 | ppl 72520177390078741072757918399463424.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch  3.29 | loss 79.43 | ppl 31394664227542920208568581554700288.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch  3.24 | loss 80.36 | ppl 79259248953262771361696148988362752.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch  3.21 | loss 80.37 | ppl 80108475512960141844172946513330176.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch  3.24 | loss 79.47 | ppl 32598666659660978611287706013007872.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch  3.22 | loss 80.02 | ppl 56737882565496572753828851742670848.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch  3.22 | loss 79.86 | ppl 48074426625122987404545473176928256.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch  3.30 | loss 79.87 | ppl 48676204550054504834818564541120512.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch  3.26 | loss 79.62 | ppl 37711371269585837018702090778705920.00\n",
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch  3.22 | loss 79.93 | ppl 51463416107690008658778016823902208.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch  3.25 | loss 79.54 | ppl 34943914991950130800723674578026496.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch  3.23 | loss 80.48 | ppl 89927310312462602320755423442894848.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch  3.23 | loss 79.53 | ppl 34656859609085897487959468328091648.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch  3.25 | loss 79.06 | ppl 21708701386123117101165049085427712.00\n",
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch  3.21 | loss 80.60 | ppl 101373057872646496518857744376135680.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch  3.26 | loss 79.66 | ppl 39399978546678271841526234145095680.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch  3.24 | loss 79.99 | ppl 54802928973314808104682673215111168.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch  3.26 | loss 79.97 | ppl 53655414551279895726722088811102208.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch  3.25 | loss 79.58 | ppl 36376135468804073690639912730099712.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch  3.31 | loss 80.12 | ppl 62566389166864969755521940113588224.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch  3.25 | loss 79.53 | ppl 34512495861210426749374261949890560.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch  3.27 | loss 80.11 | ppl 61661896003289897560817948919070720.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch  3.25 | loss 79.43 | ppl 31337018669708861722592101194006528.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch  3.22 | loss 79.44 | ppl 31540810179439750103816060526395392.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch  3.25 | loss 80.03 | ppl 56850769766054065054800417117962240.00\n",
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch  3.22 | loss 79.96 | ppl 53013776995708708287942003892682752.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch  3.21 | loss 79.58 | ppl 36342877890404638148628543692603392.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch  3.24 | loss 79.79 | ppl 45026387124886214859502843942928384.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch  3.24 | loss 80.05 | ppl 58505862422158405691343774253318144.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch  3.24 | loss 79.79 | ppl 45125895438216921111452078361804800.00\n",
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch  3.27 | loss 79.86 | ppl 48355177163285988215128505760874496.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch  3.24 | loss 79.76 | ppl 43526189930534866880228660999094272.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch  3.26 | loss 79.86 | ppl 48237808677377654248353568680574976.00\n",
      "[11.7020483 11.7020483 11.7020483 ... 11.7020483 11.7020483 11.7020483]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is nan\n",
      "Model 0:   emsize: 1024   d_hid: 50   nlayers: 1   nhead: 1   r:nan\n",
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch  3.30 | loss 92.98 | ppl 23907513151756564127166031256428803522560.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch  3.26 | loss 84.76 | ppl 6476603030901768043304642783756681216.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch  3.32 | loss 82.40 | ppl 611314058541132302774236366015299584.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch  3.33 | loss 81.69 | ppl 301477746924623258468733901982400512.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch  3.29 | loss 80.99 | ppl 149411800935268139411999291711946752.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch  3.27 | loss 81.80 | ppl 335816810465160523128821206310977536.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch  3.33 | loss 81.37 | ppl 217441722065836776241783857410473984.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch  3.26 | loss 81.18 | ppl 181208084232260726025589832430911488.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch  3.31 | loss 81.38 | ppl 219223505244310113930699040589086720.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch  3.35 | loss 80.57 | ppl 97954744550266734928454032929325056.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch  3.27 | loss 80.99 | ppl 148875183122891104556987425721679872.00\n",
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch  3.28 | loss 80.60 | ppl 100809828737322656052101981602316288.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch  3.25 | loss 80.76 | ppl 118429604980481372494635786374742016.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch  3.31 | loss 81.11 | ppl 167672591850788372332204060921823232.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch  3.27 | loss 80.82 | ppl 125536560644681039411311802704723968.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch  3.32 | loss 80.68 | ppl 109670159274766027118600459382685696.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch  3.29 | loss 80.53 | ppl 94305303637320140817483904751501312.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch  3.31 | loss 80.99 | ppl 148824862998500992786865649795203072.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch  3.26 | loss 81.02 | ppl 153457038127688465430741183864242176.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch  3.31 | loss 80.45 | ppl 86863782331745235171212561353801728.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch  3.31 | loss 80.57 | ppl 98242362227085501108352044098387968.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch  3.25 | loss 80.60 | ppl 101349945026809568412368357350703104.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch  3.29 | loss 80.15 | ppl 64397342772006266139349853784768512.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch  3.32 | loss 81.00 | ppl 151142295768526043165236100417454080.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch  3.32 | loss 80.51 | ppl 92339374791183229713548785341169664.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch  3.28 | loss 80.52 | ppl 93401818858699005446874080074530816.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch  3.27 | loss 80.42 | ppl 84258658810938304912165701010063360.00\n",
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch  3.31 | loss 80.60 | ppl 101391657001008572923839838557306880.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch  3.29 | loss 80.73 | ppl 114598336080019418835174879884476416.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch  3.27 | loss 80.16 | ppl 65216586217136851687427467624054784.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch  3.33 | loss 80.78 | ppl 121053376611085915536925992933130240.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch  3.29 | loss 80.29 | ppl 74046868937011479079302382756036608.00\n",
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch  3.30 | loss 80.09 | ppl 60543051268119706516311373279920128.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch  3.32 | loss 79.77 | ppl 44220511329741115548451313805164544.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch  3.32 | loss 80.06 | ppl 58573999607906645544711675577368576.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch  3.31 | loss 80.57 | ppl 98148083373120830026354791773372416.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch  3.29 | loss 80.41 | ppl 83530347126458354019902088300986368.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch  3.28 | loss 80.36 | ppl 79327023189140261660590340348837888.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch  3.27 | loss 80.28 | ppl 73325388014198249543619080091598848.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch  3.32 | loss 80.31 | ppl 75393339001281865740673530910998528.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch  3.30 | loss 79.79 | ppl 45081238499038571210470963202228224.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch  3.27 | loss 80.60 | ppl 101183446984768084232114104787009536.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch  3.32 | loss 80.63 | ppl 103613016943723813950201856515899392.00\n",
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch  3.31 | loss 79.95 | ppl 52906637161968435043349872561356800.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch  3.29 | loss 80.42 | ppl 83917209289491409357202904498307072.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch  3.28 | loss 79.96 | ppl 53284111836924227671648942858174464.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch  3.33 | loss 79.68 | ppl 40150158459326025074496322848948224.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch  3.25 | loss 80.32 | ppl 75977233925883839721141902721941504.00\n",
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch  3.29 | loss 80.23 | ppl 69740950062539230471809883898380288.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch  3.34 | loss 79.93 | ppl 51500630163321758869829415872757760.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch  3.35 | loss 80.53 | ppl 93719311133372825315298719555387392.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch  3.31 | loss 79.79 | ppl 44774988087685305569373000923348992.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch  3.30 | loss 80.10 | ppl 61126803942736508047778728625831936.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch  3.30 | loss 80.49 | ppl 90299007625339097191390866386714624.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch  3.31 | loss 80.06 | ppl 59017934435544205076156361287401472.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch  3.30 | loss 80.32 | ppl 75961541505086188653419793752260608.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch  3.29 | loss 80.01 | ppl 55715905164782644428938928501817344.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch  3.27 | loss 80.10 | ppl 61092297435702866966092113172758528.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch  3.30 | loss 80.02 | ppl 56545953218797064895656773428445184.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch  3.27 | loss 80.25 | ppl 70912621228631183711010435895394304.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch  3.29 | loss 80.29 | ppl 73911379295290286163095718239141888.00\n",
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch  3.27 | loss 79.97 | ppl 53943830232654451124546335777226752.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch  3.31 | loss 80.08 | ppl 59857483672246024857249452444876800.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch  3.24 | loss 79.93 | ppl 51861849338265826855810300991504384.00\n",
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch  3.39 | loss 80.17 | ppl 65705826047549967566189402056032256.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch  3.29 | loss 79.96 | ppl 53241399272036612910110509052723200.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch  3.28 | loss 79.92 | ppl 51212554512216636388983768696750080.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch  3.32 | loss 80.39 | ppl 82059420079838622292630993138352128.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch  3.28 | loss 79.71 | ppl 41271626307793485428350188599115776.00\n",
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch  3.31 | loss 80.14 | ppl 63809604644930497346968686610612224.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch  3.30 | loss 80.20 | ppl 67416337870343114419995701969682432.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch  3.27 | loss 79.89 | ppl 49467723914827981638736749801766912.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch  3.31 | loss 79.98 | ppl 54337542176581945032482823545552896.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch  3.31 | loss 80.29 | ppl 74190179030879168097351327146835968.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch  3.38 | loss 79.95 | ppl 52558264276582596303804158814715904.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch  3.34 | loss 80.08 | ppl 60080375700351918498688933757452288.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch  3.40 | loss 79.58 | ppl 36545846328964174555013621307408384.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch  3.41 | loss 80.18 | ppl 66316716412828676875918357422931968.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch  3.38 | loss 79.96 | ppl 53122114506145490237419240015527936.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch  3.43 | loss 79.76 | ppl 43516311949387099730806535490633728.00\n",
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch  3.30 | loss 79.57 | ppl 35895944541607526646943410659786752.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch  3.37 | loss 80.10 | ppl 61281308030173585770656619688886272.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch  3.31 | loss 80.08 | ppl 60142494826893431172053533464723456.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch  3.33 | loss 79.93 | ppl 51589058009175380613679177403465728.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch  3.28 | loss 80.33 | ppl 77115863486777082701561214732861440.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch  3.29 | loss 79.94 | ppl 52028956971935452545230985546956800.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch  3.31 | loss 79.96 | ppl 53365920858991544981030337358331904.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch  3.35 | loss 80.07 | ppl 59598661048108291190765259195416576.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch  3.24 | loss 80.10 | ppl 61281770404885767692966089805791232.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch  3.24 | loss 80.05 | ppl 58065043365874112992491852940705792.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch  3.36 | loss 79.85 | ppl 47814531948900294887822723548971008.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch  3.33 | loss 79.90 | ppl 50189786613906189497156480935133184.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch  3.25 | loss 80.17 | ppl 65455057789693921785363789105856512.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch  3.28 | loss 79.64 | ppl 38543743754764154359040460940378112.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch  3.25 | loss 79.84 | ppl 47325000938794985373639491450830848.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch  3.35 | loss 79.89 | ppl 49390410876089691620917111794171904.00\n",
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch  3.28 | loss 80.60 | ppl 100756697538745344432856687615934464.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch  3.27 | loss 79.23 | ppl 25680996293229241409330292105150464.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch  3.33 | loss 79.91 | ppl 50513850937247597942623163985690624.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch  3.31 | loss 79.95 | ppl 52751336831141482041469435442626560.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch  3.27 | loss 80.02 | ppl 56511913074085453527654222207647744.00\n",
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch  3.32 | loss 79.97 | ppl 53777767505398463106110741339439104.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch  3.29 | loss 80.00 | ppl 55249184501635563915066656310689792.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch  3.32 | loss 80.14 | ppl 63615171257667174535285028983668736.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch  3.32 | loss 79.83 | ppl 46674990337211836201734374424576000.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch  3.29 | loss 79.95 | ppl 52800747043894884570737723496726528.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch  3.30 | loss 79.50 | ppl 33476330434722268882932747412176896.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch  3.30 | loss 80.35 | ppl 78421070198593258950760758664757248.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch  3.26 | loss 79.59 | ppl 36902903854458508270641680260005888.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch  3.31 | loss 79.73 | ppl 42494304672652691071498331541733376.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch  3.31 | loss 80.06 | ppl 58704995315170070030166096084992000.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch  3.32 | loss 79.46 | ppl 32161948464272170277737346342322176.00\n",
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch  3.30 | loss 79.94 | ppl 52365237395460147682356071818592256.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch  3.30 | loss 79.39 | ppl 30198254330308382395816228067213312.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch  3.27 | loss 79.89 | ppl 49438359116023609514700144767926272.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch  3.31 | loss 79.72 | ppl 41754496628266205638285254681690112.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch  3.31 | loss 79.83 | ppl 46746679598206753804533873304928256.00\n",
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch  3.30 | loss 80.14 | ppl 63761539397118012694368883776159744.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch  3.30 | loss 79.99 | ppl 54949604079642900376314090499342336.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch  3.29 | loss 79.61 | ppl 37646652123890666277243900586360832.00\n",
      "[11.0531435 11.0531435 11.0531435 ... 11.0531435 11.0531435 11.0531435]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is nan\n",
      "Model 1:   emsize: 1024   d_hid: 50   nlayers: 1   nhead: 4   r:nan\n",
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch  9.87 | loss 92.35 | ppl 12803321642704919686033590727892315144192.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch  9.73 | loss 86.05 | ppl 23429390502372005195546029009590550528.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch  9.84 | loss 82.74 | ppl 857333126378943581125339226914684928.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch  9.73 | loss 82.35 | ppl 583253137291554158310792275280527360.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch  9.85 | loss 81.83 | ppl 345324835269938475370949216349192192.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch  9.77 | loss 81.52 | ppl 253091082434366528155571061851684864.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch  9.84 | loss 81.85 | ppl 350831721116827981974669853160636416.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch  9.84 | loss 81.06 | ppl 159644824240380612302402517041414144.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch  9.78 | loss 81.44 | ppl 233926848719915567290608930594488320.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch  9.77 | loss 80.79 | ppl 121753868298947013254370957115523072.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch  9.78 | loss 80.76 | ppl 119024892277360192414447773258088448.00\n",
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch  9.77 | loss 81.10 | ppl 166645172907186713925548386222604288.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch  9.82 | loss 80.26 | ppl 71637031441844135685658230223011840.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch  9.76 | loss 81.48 | ppl 243586559789706421246199110918209536.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch  9.82 | loss 80.36 | ppl 79775207556878997847340957556539392.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch  9.80 | loss 80.76 | ppl 119001314885606827552380875823906816.00\n",
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch  9.79 | loss 80.72 | ppl 113421873480347111416651132581183488.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch  9.90 | loss 80.70 | ppl 111525049461556302788437808506732544.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch  9.86 | loss 80.42 | ppl 84654263558105618409427235728523264.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch  9.81 | loss 80.69 | ppl 110940083620265296326305101725564928.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch  9.80 | loss 80.84 | ppl 128794331651590125119327442739134464.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch  9.78 | loss 80.48 | ppl 89901466614382229198249975305207808.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch  9.84 | loss 80.95 | ppl 143372768856006576054417986441707520.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch  9.82 | loss 80.38 | ppl 80736076548994246850760012139069440.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch  9.74 | loss 80.70 | ppl 111857417660572799285231784158035968.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch  9.79 | loss 80.39 | ppl 81601309550586477143098325886566400.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch  9.80 | loss 80.59 | ppl 99544883669755863204060661595242496.00\n",
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch  9.79 | loss 79.78 | ppl 44565956597498580242772973141884928.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch  9.74 | loss 81.00 | ppl 150875878258637349529920440086036480.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch  9.82 | loss 80.11 | ppl 62133547518206177490758740285063168.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch  9.79 | loss 80.59 | ppl 100395436577878346463343968732053504.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch  9.86 | loss 80.20 | ppl 67840725792322602745665418006888448.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch  9.76 | loss 80.57 | ppl 97789117970670999197704378986266624.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch  9.83 | loss 80.33 | ppl 77010426831228102638990325807316992.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch  9.88 | loss 80.29 | ppl 74085640986384864763788601778503680.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch  9.81 | loss 80.29 | ppl 73697972681670137334579165078224896.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch  9.75 | loss 80.07 | ppl 59413310337781618456254292529512448.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch  9.88 | loss 80.55 | ppl 95738101647260735377680733896179712.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch  9.83 | loss 80.33 | ppl 76819263066699203369547003959705600.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch  9.78 | loss 80.49 | ppl 90406164372151162168778615496376320.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch  9.80 | loss 80.52 | ppl 93107533407859913133421549552926720.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch  9.80 | loss 80.27 | ppl 72492776616774445568113299878313984.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch  9.88 | loss 79.93 | ppl 51831034957372629861324702114381824.00\n",
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch  9.81 | loss 80.72 | ppl 114367957734523436659848265836003328.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch  9.84 | loss 79.97 | ppl 53822819214638727564282512160063488.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch  9.84 | loss 80.36 | ppl 79468301827964699096249893847564288.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch  9.86 | loss 80.29 | ppl 73888194077485535746166683410104320.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch  9.80 | loss 80.22 | ppl 68754462712382117876202153327984640.00\n",
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch  9.79 | loss 80.01 | ppl 55835394657469546481633956628267008.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch  9.81 | loss 80.16 | ppl 65027990816203670291242691183247360.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch  9.82 | loss 80.33 | ppl 77358702774535482847103521371717632.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch  9.78 | loss 80.17 | ppl 65590603222945398513771364015407104.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch  9.76 | loss 80.07 | ppl 59372273874543674845916280272191488.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch  9.82 | loss 80.47 | ppl 88736628281575987758142695378255872.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch  9.81 | loss 80.31 | ppl 75468651784192906902062431723323392.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch  9.80 | loss 80.38 | ppl 81313272049083356638503167554748416.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch  9.79 | loss 80.24 | ppl 70260072783440257791656036948508672.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch  9.74 | loss 79.93 | ppl 51626757664495741519745884786524160.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch  9.82 | loss 79.95 | ppl 52499016751840093699747810625191936.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch  9.76 | loss 80.17 | ppl 65666839697672924565514681298452480.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch  9.79 | loss 80.15 | ppl 64336463600725411250595355170963456.00\n",
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch  9.81 | loss 80.37 | ppl 80369133187171690595259771487518720.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch  9.78 | loss 79.80 | ppl 45444119201751496424770510373519360.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch  9.81 | loss 80.37 | ppl 80270208750796800407797296395190272.00\n",
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch  9.79 | loss 80.52 | ppl 92879330257237884718486281747693568.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch  9.79 | loss 79.71 | ppl 41485526588178194402570714705035264.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch  9.73 | loss 80.23 | ppl 69453074181933768095442086020513792.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch  9.87 | loss 79.78 | ppl 44280497472042273437972481280311296.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch  9.76 | loss 80.63 | ppl 103924960157297260090329638941753344.00\n",
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch  9.86 | loss 79.60 | ppl 37241570181208664418376996148477952.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch  9.78 | loss 79.76 | ppl 43656103121048127141405999048425472.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch  9.74 | loss 79.75 | ppl 43045850646866015174634762930225152.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch  9.86 | loss 79.44 | ppl 31549094122763318827293346596126720.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch  9.80 | loss 80.38 | ppl 81258170512724704186202738701369344.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch  9.77 | loss 79.97 | ppl 53896828424449550743927883198103552.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch  9.74 | loss 80.32 | ppl 75937187088171687376622551251812352.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch  9.84 | loss 80.08 | ppl 60028742926537015959116488672018432.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch  9.83 | loss 80.22 | ppl 68903688792121201885911299158704128.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch  9.79 | loss 79.82 | ppl 46080841672859973799269490784993280.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch  9.76 | loss 80.17 | ppl 65614606367511167163498065932845056.00\n",
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch  9.79 | loss 79.90 | ppl 50261182186203592285500195194535936.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch  9.78 | loss 79.89 | ppl 49831898012791393448511374639497216.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch  9.76 | loss 80.05 | ppl 58332680710560894234660944184606720.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch  9.86 | loss 79.86 | ppl 48226531364474065253715751185940480.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch  9.80 | loss 79.66 | ppl 39409581567771313601895868535930880.00\n",
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch  9.75 | loss 79.90 | ppl 50005183980078405021230199136583680.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch  9.86 | loss 79.61 | ppl 37428950055470003972493256180105216.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch  9.77 | loss 79.99 | ppl 54672139270400791972908379280506880.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch  9.77 | loss 79.88 | ppl 48983940336422513430927115389239296.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch  9.88 | loss 80.03 | ppl 57298150957483032647635768119918592.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch  9.77 | loss 80.23 | ppl 69796026136263080493758880189251584.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch  9.75 | loss 79.55 | ppl 35428022657956645295712933999804416.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch  9.75 | loss 79.95 | ppl 52548486619107102052588582552469504.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch  9.85 | loss 79.56 | ppl 35858319090102986886195713541144576.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch  9.75 | loss 79.70 | ppl 40852515124823488885744149771321344.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch  9.83 | loss 80.13 | ppl 63241611283877226274272624125149184.00\n",
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch  9.79 | loss 79.61 | ppl 37476476923022802506334075099283456.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch  9.77 | loss 79.95 | ppl 52719240212038991487605194958569472.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch  9.83 | loss 79.83 | ppl 46533059729053202056867886584037376.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch  9.79 | loss 80.20 | ppl 67985733192092927375164145942921216.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch  9.75 | loss 80.11 | ppl 61883433328907174267504586416717824.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch  9.78 | loss 79.49 | ppl 33237568498846715203023484538060800.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch  9.84 | loss 79.81 | ppl 45676412112873704323890702245167104.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch  9.70 | loss 80.32 | ppl 76558494479293103158778861048561664.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch  9.81 | loss 79.71 | ppl 41291510022327435253742374704119808.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch  9.84 | loss 79.91 | ppl 50710956309786995943077853578395648.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch  9.82 | loss 79.99 | ppl 54723678082258298266780390458916864.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch  9.80 | loss 79.92 | ppl 51073563506006081361431565892059136.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch  9.81 | loss 79.91 | ppl 50533251923709577411013853027762176.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch  9.80 | loss 80.12 | ppl 62646384133124493250723649377271808.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch  9.76 | loss 80.14 | ppl 63982632973807525255303575248044032.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch  9.81 | loss 80.09 | ppl 60890697946219633802118843204632576.00\n",
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch  9.74 | loss 79.57 | ppl 36107591641707238856758805233926144.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch  9.82 | loss 80.20 | ppl 67677015530362845352810816571703296.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch  9.78 | loss 79.52 | ppl 34449323052468968775937074093096960.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch  9.84 | loss 79.29 | ppl 27127652042412676193381867400986624.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch  9.74 | loss 80.33 | ppl 77135225692475532052464413689511936.00\n",
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch  9.78 | loss 79.55 | ppl 35451157607621690681138597240766464.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch  9.81 | loss 79.54 | ppl 34910698998126210027653573868781568.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch  9.76 | loss 79.55 | ppl 35167654411053257740376383632179200.00\n",
      "[11.40778828 11.40778828 11.40778828 ... 11.40778828 11.40778828\n",
      " 11.40778828]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is nan\n",
      "Model 2:   emsize: 1024   d_hid: 50   nlayers: 4   nhead: 1   r:nan\n",
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch 10.08 | loss 92.00 | ppl 9060938193799021752857265320136355610624.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch 10.03 | loss 85.28 | ppl 10847006110067376952143161611754930176.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch 10.09 | loss 82.47 | ppl 657120062816703908823412999052066816.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch  9.98 | loss 82.13 | ppl 468062232238569128098692434533285888.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch 10.10 | loss 81.88 | ppl 362055355591169466412341965260587008.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch 10.02 | loss 81.55 | ppl 261039837095967671274250623942393856.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch 10.07 | loss 81.77 | ppl 326730472071701685434218636346654720.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch 10.07 | loss 81.00 | ppl 150219473307400051616012695192469504.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch 10.02 | loss 81.42 | ppl 228891520993345244516832527031205888.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch 10.04 | loss 80.66 | ppl 107413593689820835362537008126754816.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch 10.01 | loss 80.91 | ppl 137987717486255793372322337755496448.00\n",
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch 10.02 | loss 81.25 | ppl 193025416141147874943879891872907264.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch 10.01 | loss 80.75 | ppl 117091971334205525582150981262508032.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch 10.02 | loss 80.62 | ppl 102698922494780095091892628452540416.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch 10.02 | loss 81.22 | ppl 186926194891002140095580984108384256.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch 10.05 | loss 80.14 | ppl 63917850767625468324639331623895040.00\n",
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch 10.01 | loss 80.85 | ppl 129987392793035028557305896232812544.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch 10.05 | loss 80.63 | ppl 104014469515748132857615460985733120.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch 10.03 | loss 80.33 | ppl 76947994663834862159569956013867008.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch 10.01 | loss 80.35 | ppl 78740387693981618991910719429017600.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch 10.06 | loss 80.51 | ppl 92416454371158114252103981566263296.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch 10.06 | loss 80.58 | ppl 99271216337157498609952007051542528.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch  9.99 | loss 80.72 | ppl 114092611349834328963294493944053760.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch 10.02 | loss 80.45 | ppl 87204880683503264334973398209789952.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch 10.00 | loss 79.97 | ppl 54009972745752024513852119327965184.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch 10.03 | loss 80.96 | ppl 145092114699974918491165183484887040.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch 10.04 | loss 80.45 | ppl 86928847960087483455270008775507968.00\n",
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch 10.07 | loss 80.45 | ppl 86576981718759138856032706880339968.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch 10.14 | loss 80.67 | ppl 108532779391465703843185720746311680.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch 10.09 | loss 80.15 | ppl 64603096511872464864306239973097472.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch 10.11 | loss 80.46 | ppl 87409408893684585247395772922593280.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch 10.02 | loss 80.44 | ppl 86251291882535884969761716157022208.00\n",
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch 10.11 | loss 80.71 | ppl 112702399887995359163601824264486912.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch 10.10 | loss 80.17 | ppl 65914723561549459287093381968166912.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch 10.04 | loss 79.88 | ppl 49365306685294580573992549366104064.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch 10.11 | loss 80.21 | ppl 68665181238255322409464498493587456.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch 10.05 | loss 80.69 | ppl 109977305945987839547538464466534400.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch 10.08 | loss 80.16 | ppl 64853029998592122703167923511361536.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch 10.04 | loss 80.55 | ppl 96087136387172990493648129397620736.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch 10.07 | loss 80.55 | ppl 96211213420698763627835085968900096.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch 10.05 | loss 80.65 | ppl 106352815093594441446004728468078592.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch 10.04 | loss 80.11 | ppl 61801503983390568034883647662194688.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch 10.06 | loss 80.19 | ppl 66735899641771945109554105986056192.00\n",
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch 10.07 | loss 80.43 | ppl 85116014825713277944874024505966592.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch  9.96 | loss 80.64 | ppl 104569857671665585297202396534931456.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch 10.05 | loss 80.25 | ppl 70919080359148240140126440518582272.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch 10.03 | loss 80.36 | ppl 79036018982088851910083294045143040.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch 10.04 | loss 80.09 | ppl 60889297362923025159464971331960832.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch 10.03 | loss 80.15 | ppl 64563824361887772528669205156855808.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch 10.06 | loss 79.96 | ppl 53282763103978749691021286904430592.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch 10.03 | loss 79.78 | ppl 44471174904492800996111437216612352.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch 10.10 | loss 80.27 | ppl 72452388795122367625672123924611072.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch 10.04 | loss 80.44 | ppl 86061067874080250368395027318571008.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch  9.97 | loss 80.14 | ppl 63984332559476391489249645890109440.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch 10.08 | loss 79.92 | ppl 51093982867510105639894453091041280.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch 10.02 | loss 80.31 | ppl 75212024666260019753818432647200768.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch 10.08 | loss 80.35 | ppl 78439991492262253397330218268491776.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch 10.14 | loss 80.12 | ppl 62475454683738513792991429954371584.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch 10.07 | loss 80.20 | ppl 67563748218610994132652615517863936.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch 10.10 | loss 80.01 | ppl 56079228153636742645903665372594176.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch 10.10 | loss 80.14 | ppl 63944326940565948009125565039640576.00\n",
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch 10.04 | loss 79.97 | ppl 53844325310003991871594725373902848.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch 10.01 | loss 80.08 | ppl 59942640897329499223904938646044672.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch 10.08 | loss 79.97 | ppl 53896562152438539937948791161749504.00\n",
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch 10.07 | loss 79.99 | ppl 54915331282829087679782195491241984.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch 10.02 | loss 80.56 | ppl 96908271292709404935034970564460544.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch 10.09 | loss 80.00 | ppl 55308147802108709486459750776307712.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch 10.02 | loss 80.13 | ppl 62920091423614051407453807263612928.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch 10.08 | loss 79.67 | ppl 39802107789399452693038868341981184.00\n",
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch 10.08 | loss 80.39 | ppl 81963877039403158720962342135267328.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch 10.12 | loss 80.52 | ppl 92884889989073268576711967670009856.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch 10.02 | loss 79.69 | ppl 40658260529674007361989552380051456.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch 10.06 | loss 80.07 | ppl 59651612403013953455391859374292992.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch 10.15 | loss 80.01 | ppl 56017129782106224290442105602441216.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch 10.06 | loss 80.65 | ppl 106365402392887279249304340721041408.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch 10.03 | loss 79.89 | ppl 49615141809755098569803772186329088.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch 10.04 | loss 80.31 | ppl 75395249570651590642958194232524800.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch 10.02 | loss 79.89 | ppl 49644806672729001169117091746283520.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch 10.04 | loss 79.90 | ppl 50373265510357219833430544843014144.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch 10.07 | loss 79.94 | ppl 52197532508242306420135266866954240.00\n",
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch 10.02 | loss 80.39 | ppl 81762237087193892247431102752358400.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch 10.11 | loss 79.77 | ppl 43870038710244966827180892897673216.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch 10.08 | loss 79.71 | ppl 41371261058450037227598029556547584.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch 10.07 | loss 80.02 | ppl 56589659995422282877743866476232704.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch 10.09 | loss 79.81 | ppl 46020290801362612250537032171913216.00\n",
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch 10.06 | loss 79.94 | ppl 52015058929354735008443909959319552.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch 10.06 | loss 80.49 | ppl 90793685862544889767897157683118080.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch 10.10 | loss 79.63 | ppl 38138415692908826855839709741973504.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch 10.00 | loss 80.04 | ppl 57709735036780361434516552763310080.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch 10.05 | loss 80.05 | ppl 57983320574208857757892441645514752.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch 10.07 | loss 79.97 | ppl 53908762424842169390972926162894848.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch 10.00 | loss 79.96 | ppl 53389150829663049728981645981122560.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch 10.04 | loss 79.12 | ppl 23085567092007886897365146409107456.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch 10.03 | loss 80.51 | ppl 92454490736821430019577772760891392.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch 10.06 | loss 79.83 | ppl 46843667802017197560962366262214656.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch 10.03 | loss 79.96 | ppl 53106962647223560587097576786886656.00\n",
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch 10.08 | loss 80.41 | ppl 83395544424605647631361182135222272.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch 10.07 | loss 79.44 | ppl 31521131793750128872828912046964736.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch 10.05 | loss 79.76 | ppl 43783507310464199166326616568627200.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch 10.04 | loss 80.12 | ppl 62594866020886937497074990328053760.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch 10.04 | loss 80.16 | ppl 65032400021422730958569686026944512.00\n",
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch 10.03 | loss 79.50 | ppl 33547781868795115410989178771472384.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch 10.04 | loss 80.13 | ppl 63105769081124880184858595018407936.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch 10.08 | loss 79.71 | ppl 41550396822198257049842517509406720.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch 10.02 | loss 80.10 | ppl 60956182922553085340001610118463488.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch 10.08 | loss 79.61 | ppl 37377189508061319179039041847820288.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch 10.04 | loss 79.86 | ppl 48063898109938221330128974706114560.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch 10.05 | loss 79.51 | ppl 33827843805034843482811284924137472.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch 10.11 | loss 79.93 | ppl 51865097492462203993058204406775808.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch 10.09 | loss 79.50 | ppl 33728737867770300142621776651747328.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch 10.05 | loss 79.62 | ppl 37737356243877208554308984444551168.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch 10.05 | loss 80.04 | ppl 57771576964185147587294532361781248.00\n",
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch 10.01 | loss 79.84 | ppl 46980949199015615742558234298286080.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch 10.07 | loss 79.58 | ppl 36277070498708935827584612695015424.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch 10.07 | loss 79.61 | ppl 37500092164736877116615946972168192.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch 10.07 | loss 79.76 | ppl 43739038850196347604259764497809408.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch 10.07 | loss 79.75 | ppl 43172790009426469791236305429987328.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch 10.10 | loss 79.92 | ppl 51184972073668829639354510680784896.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch 10.04 | loss 79.87 | ppl 48875528534063925655236728739332096.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch 10.07 | loss 79.41 | ppl 30801729179033717992295620419780608.00\n",
      "[12.21600819 12.21600819 12.21600819 ... 12.21600819 12.21600819\n",
      " 12.21600819]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is nan\n",
      "Model 3:   emsize: 1024   d_hid: 50   nlayers: 4   nhead: 4   r:nan\n",
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch  3.32 | loss 94.68 | ppl 131958895131737298318949562272722400051200.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch  3.24 | loss 86.15 | ppl 25857003276835279111533854598482624512.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch  3.30 | loss 82.64 | ppl 776343421750879933145374492136046592.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch  3.30 | loss 82.85 | ppl 954420152231463325444764427060510720.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch  3.36 | loss 82.12 | ppl 459788538976021813043677316727701504.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch  3.35 | loss 81.99 | ppl 404933717221347242246546490109460480.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch  3.32 | loss 81.34 | ppl 211697738273502747971681032622571520.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch  3.34 | loss 81.55 | ppl 260450236405769730727591171806199808.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch  3.30 | loss 80.63 | ppl 104000205960644818620063249822908416.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch  3.29 | loss 81.41 | ppl 225864834996124303210693025928839168.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch  3.30 | loss 80.57 | ppl 97787987547923117841547493596200960.00\n",
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch  3.32 | loss 81.34 | ppl 211552067990562938078245768327593984.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch  3.31 | loss 80.19 | ppl 66903305288099698284752497283170304.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch  3.35 | loss 81.29 | ppl 201188146183880973592060091170816000.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch  3.30 | loss 80.43 | ppl 85046526393409249081515348936622080.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch  3.31 | loss 81.30 | ppl 202899785473137729974618306249752576.00\n",
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch  3.31 | loss 80.83 | ppl 127408889567753586074617951842992128.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch  3.33 | loss 80.41 | ppl 83174948180702956440792478955601920.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch  3.31 | loss 80.43 | ppl 85423206443261888710422488860852224.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch  3.29 | loss 80.60 | ppl 100455062880569069320206298961149952.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch  3.33 | loss 81.05 | ppl 158837059250235189022759418723303424.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch  3.27 | loss 80.21 | ppl 68594772371422498740338678084665344.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch  3.36 | loss 80.25 | ppl 71351746833001613409791701468315648.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch  3.30 | loss 80.49 | ppl 90240085522497033287271391114035200.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch  3.30 | loss 80.31 | ppl 75189906298342388027931259635236864.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch  3.35 | loss 80.80 | ppl 122745384635645437081308364078579712.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch  3.35 | loss 80.53 | ppl 94382422428024185344308458695950336.00\n",
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch  3.32 | loss 80.36 | ppl 79621201823843351859215575145250816.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch  3.30 | loss 80.56 | ppl 96612289612366027443804198806749184.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch  3.27 | loss 80.68 | ppl 109109338700752881602937340978790400.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch  3.32 | loss 80.22 | ppl 69068468767233240852982716686139392.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch  3.33 | loss 80.83 | ppl 127055201027117990811058755292102656.00\n",
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch  3.29 | loss 80.30 | ppl 74488517558036560032764119643848704.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch  3.31 | loss 80.62 | ppl 102780905684221424488170725551636480.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch  3.33 | loss 80.52 | ppl 93082017469048113835007112707571712.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch  3.33 | loss 80.18 | ppl 66284615281285431122026713604685824.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch  3.32 | loss 79.62 | ppl 37812022776033666056375379332956160.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch  3.32 | loss 80.74 | ppl 116191695684965291279738120997699584.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch  3.31 | loss 80.14 | ppl 63662153224277245158619309313884160.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch  3.30 | loss 80.00 | ppl 55679402491286342126600044326944768.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch  3.33 | loss 80.38 | ppl 81168338582572468520329490672386048.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch  3.28 | loss 80.06 | ppl 58737267587492442456378125653639168.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch  3.32 | loss 80.33 | ppl 77002225697440263009775752048541696.00\n",
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch  3.33 | loss 80.05 | ppl 58003792137450303621067869826056192.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch  3.29 | loss 80.34 | ppl 78064151197295017461637278146232320.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch  3.35 | loss 80.14 | ppl 63777217414375478816752795429896192.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch  3.31 | loss 79.89 | ppl 49713034076698097158764478481825792.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch  3.32 | loss 80.70 | ppl 111537430627278067169061003115626496.00\n",
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch  3.29 | loss 79.85 | ppl 47466652420028641837645667871227904.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch  3.35 | loss 80.62 | ppl 103082650117972163865472902677659648.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch  3.35 | loss 79.91 | ppl 50848723815781425241248966486523904.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch  3.30 | loss 79.93 | ppl 51622894526676410136355580407709696.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch  3.29 | loss 80.09 | ppl 60477960746508080134930449926455296.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch  3.28 | loss 80.13 | ppl 63301034512493687447552462883389440.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch  3.31 | loss 80.49 | ppl 90762724297682992384633347478913024.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch  3.29 | loss 80.03 | ppl 56977680397968962496532003634020352.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch  3.30 | loss 79.97 | ppl 53943031341276531864397749877211136.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch  3.29 | loss 80.49 | ppl 90305243282245651111228354846523392.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch  3.27 | loss 80.24 | ppl 70556104163057200168147264814972928.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch  3.32 | loss 80.02 | ppl 56514007324390389489534062576009216.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch  3.34 | loss 80.16 | ppl 64760001006181089226937996192776192.00\n",
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch  3.35 | loss 80.26 | ppl 71520653974583593964822745839566848.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch  3.30 | loss 80.43 | ppl 84919588745252654898841551860924416.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch  3.31 | loss 79.79 | ppl 44754771078249509537526875351416832.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch  3.30 | loss 80.37 | ppl 80562021924614250950784993185497088.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch  3.30 | loss 79.99 | ppl 54995778923313359199798527564709888.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch  3.32 | loss 80.13 | ppl 63079125745370370607696046872068096.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch  3.32 | loss 80.05 | ppl 58285201735149840781324196281057280.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch  3.30 | loss 79.92 | ppl 51299421683148632643408199514324992.00\n",
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch  3.32 | loss 80.25 | ppl 70858761285526134518388905635479552.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch  3.33 | loss 80.04 | ppl 57723550777569278057889525821079552.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch  3.26 | loss 80.11 | ppl 61616159176313697251466767255994368.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch  3.28 | loss 79.80 | ppl 45178975983345472246636998213763072.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch  3.36 | loss 80.15 | ppl 64533373593387913798562931107954688.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch  3.26 | loss 80.35 | ppl 78797198523700200380504346378371072.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch  3.31 | loss 79.76 | ppl 43768757397160512433175675625537536.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch  3.28 | loss 80.14 | ppl 63969256070088081817469437222060032.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch  3.33 | loss 80.04 | ppl 57651398158039514209295200262553600.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch  3.31 | loss 79.21 | ppl 25193171267509966437251106583085056.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch  3.34 | loss 80.26 | ppl 71586387953404276369633134560411648.00\n",
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch  3.32 | loss 80.06 | ppl 59073395110087406043472106222518272.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch  3.29 | loss 80.28 | ppl 73634008263140771954203147315970048.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch  3.31 | loss 80.53 | ppl 94572988563349395978850469144428544.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch  3.31 | loss 79.59 | ppl 36887498337665035592292726778888192.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch  3.35 | loss 80.12 | ppl 62529148140040288835818315942199296.00\n",
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch  3.29 | loss 79.99 | ppl 54922710921529541374030611675611136.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch  3.30 | loss 80.38 | ppl 81207740505306986148471608646303744.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch  3.27 | loss 79.95 | ppl 52880896144109292973955898413154304.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch  3.30 | loss 80.18 | ppl 66202725324578849221803148897681408.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch  3.29 | loss 79.80 | ppl 45497052349339196260277020339994624.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch  3.29 | loss 79.70 | ppl 41024598892708679552629320540225536.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch  3.30 | loss 79.91 | ppl 50684364174618768849297321819635712.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch  3.35 | loss 80.17 | ppl 65569253272021601153807529826844672.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch  3.36 | loss 79.93 | ppl 51634715912770793153912441564823552.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch  3.34 | loss 80.01 | ppl 55723904677226451082038525940465664.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch  3.35 | loss 79.93 | ppl 51755937043595862745720113388847104.00\n",
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch  3.32 | loss 80.41 | ppl 83841049077895685050191158328688640.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch  3.32 | loss 79.59 | ppl 36840427884912357389322076005859328.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch  3.31 | loss 79.79 | ppl 44899756533996188228342481065869312.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch  3.31 | loss 80.01 | ppl 56014802015975979110281708732153856.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch  3.34 | loss 79.46 | ppl 32373117816636358626827560460222464.00\n",
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch  3.30 | loss 80.18 | ppl 66277550969501733417623107533799424.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch  3.27 | loss 80.06 | ppl 58790714359856113163942480987553792.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch  3.29 | loss 79.46 | ppl 32260583150263490559471924174389248.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch  3.33 | loss 80.01 | ppl 56061412377617415171481918075043840.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch  3.29 | loss 79.45 | ppl 31997437859905921717834128864313344.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch  3.30 | loss 79.82 | ppl 46252759303075211210033918394761216.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch  3.31 | loss 79.71 | ppl 41403018910645302357489916049358848.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch  3.36 | loss 79.92 | ppl 51175321676527230213193408477921280.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch  3.29 | loss 79.82 | ppl 46499386058583326551326499773874176.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch  3.30 | loss 79.75 | ppl 43214413542702366595135620385865728.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch  3.34 | loss 79.83 | ppl 46793585589051039747786518061645824.00\n",
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch  3.28 | loss 79.65 | ppl 39097639433906850442662263220338688.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch  3.30 | loss 79.58 | ppl 36322939946569818898826115057451008.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch  3.29 | loss 80.07 | ppl 59218965680593148253476152982634496.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch  3.35 | loss 79.27 | ppl 26786410522066122479962167896440832.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch  3.32 | loss 79.80 | ppl 45334100754974915390336503581245440.00\n",
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch  3.36 | loss 79.91 | ppl 50772253815832325825539474121031680.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch  3.31 | loss 79.88 | ppl 48944492562932471085707701098905600.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch  3.30 | loss 79.69 | ppl 40661144045577141929998421082505216.00\n",
      "[11.18699265 11.18699265 11.18699265 ... 11.18699265 11.18699265\n",
      " 11.18699265]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is nan\n",
      "Model 4:   emsize: 1024   d_hid: 200   nlayers: 1   nhead: 1   r:nan\n",
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch  3.37 | loss 94.09 | ppl 72928217479009014122138562027398570704896.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch  3.35 | loss 86.49 | ppl 36391788174785545771514678148285333504.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch  3.34 | loss 82.55 | ppl 708576169549018840973078861591871488.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch  3.34 | loss 82.67 | ppl 801193153505350376181259500934660096.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch  3.36 | loss 81.64 | ppl 285873900999723466240783977241640960.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch  3.35 | loss 81.85 | ppl 351796040487697475740675316817854464.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch  3.34 | loss 81.61 | ppl 276755562057990848117043501010518016.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch  3.34 | loss 81.34 | ppl 212004299196262975860900288958300160.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch  3.38 | loss 80.82 | ppl 126022603188563698401067734596059136.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch  3.34 | loss 81.06 | ppl 160557799460515853354553268651425792.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch  3.36 | loss 80.22 | ppl 68827986711982234681897116558688256.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch  3.35 | loss 81.45 | ppl 235648167141786993198287993517899776.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch  3.33 | loss 80.51 | ppl 92638102318494461166173392954458112.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch  3.39 | loss 80.99 | ppl 148716034262533469594945369145344000.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch  3.34 | loss 80.70 | ppl 111726195121573854284525498628308992.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch  3.33 | loss 80.67 | ppl 108688745254623352896376477277224960.00\n",
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch  3.35 | loss 80.59 | ppl 100268714569950625854364443408859136.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch  3.37 | loss 80.95 | ppl 143305285770121025308092734329847808.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch  3.39 | loss 80.67 | ppl 107853601576019180130520425264316416.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch  3.33 | loss 80.31 | ppl 75313585137193195435597142622732288.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch  3.35 | loss 80.28 | ppl 73480186119591654744670041259638784.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch  3.33 | loss 80.91 | ppl 137254037946771016053097787660697600.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch  3.36 | loss 80.17 | ppl 65653046686304004865709768641085440.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch  3.36 | loss 80.88 | ppl 133789774381568746122094705907859456.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch  3.46 | loss 80.29 | ppl 74273230329204576936702866712690688.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch  3.40 | loss 80.42 | ppl 84642596499732085109193424905437184.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch  3.38 | loss 80.14 | ppl 63597886579527479559537658502840320.00\n",
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch  3.35 | loss 80.36 | ppl 79728751476426195206254238388715520.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch  3.42 | loss 80.33 | ppl 77261519733075243822180479774228480.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch  3.41 | loss 80.80 | ppl 122806437951864340896845880881053696.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch  3.35 | loss 80.57 | ppl 97929554926737987444921215733465088.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch  3.39 | loss 80.41 | ppl 83827249721862537063231158807953408.00\n",
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch  3.37 | loss 80.28 | ppl 73202069439786817461256703386320896.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch  3.34 | loss 80.26 | ppl 71993533135714102261315453377839104.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch  3.37 | loss 80.50 | ppl 90974803127394500090242289467654144.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch  3.35 | loss 80.80 | ppl 123510138151528145326903131405025280.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch  3.37 | loss 80.80 | ppl 123433867542546474442411534821883904.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch  3.33 | loss 80.08 | ppl 60022128528544139884665893797494784.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch  3.39 | loss 80.43 | ppl 85581510193110918553104162970664960.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch  3.36 | loss 80.38 | ppl 81417987014949372649182318045429760.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch  3.34 | loss 80.32 | ppl 75992970144106247041940474566803456.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch  3.37 | loss 80.37 | ppl 80349319302873384401386677406269440.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch  3.39 | loss 80.07 | ppl 59644170205452282475742641990402048.00\n",
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch  3.36 | loss 80.24 | ppl 70673584066070881814027690190045184.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch  3.34 | loss 80.48 | ppl 89747740223090840828476384528564224.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch  3.34 | loss 80.10 | ppl 61130134010664109688163196878192640.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch  3.36 | loss 80.36 | ppl 79336375586174999725818120132100096.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch  3.36 | loss 80.19 | ppl 66810486501104788937388841368551424.00\n",
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch  3.36 | loss 79.80 | ppl 45334144749881640812647971762995200.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch  3.38 | loss 80.71 | ppl 112864968472947808632754214518390784.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch  3.39 | loss 80.63 | ppl 103946168017494614029810932002062336.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch  3.37 | loss 80.04 | ppl 57677759293261010105693841406296064.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch  3.40 | loss 80.32 | ppl 76064394768107777845320881056776192.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch  3.40 | loss 80.09 | ppl 60439845996934450688188371980779520.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch  3.39 | loss 79.93 | ppl 51509166949344656961324578573910016.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch  3.37 | loss 80.13 | ppl 63087610562332442312292459119378432.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch  3.38 | loss 80.31 | ppl 75720471118375859079993848181555200.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch  3.34 | loss 79.95 | ppl 52749083159392818700502409028829184.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch  3.37 | loss 79.79 | ppl 44968863368385446697272938338451456.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch  3.35 | loss 80.52 | ppl 92882129604432810744261239417864192.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch  3.39 | loss 79.91 | ppl 50793915355111812923479342857584640.00\n",
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch  3.39 | loss 80.49 | ppl 90258040728882289143119685202477056.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch  3.45 | loss 80.63 | ppl 104200895108215004938595637452079104.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch  3.39 | loss 79.44 | ppl 31508794368230854848347290034241536.00\n",
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch  3.35 | loss 79.83 | ppl 46551754332126939200530140067004416.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch  3.37 | loss 80.55 | ppl 96109700071611768053261271853694976.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch  3.34 | loss 80.36 | ppl 79752321541904736443395683844620288.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch  3.37 | loss 79.78 | ppl 44322574379681489525774082295988224.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch  3.40 | loss 79.88 | ppl 49283714983324049134294333780918272.00\n",
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch  3.38 | loss 80.04 | ppl 57503858202253409496278094611218432.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch  3.33 | loss 80.45 | ppl 86687695012500177593764691539132416.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch  3.35 | loss 79.87 | ppl 48444833797886394924480971927453696.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch  3.32 | loss 80.60 | ppl 101458337016267098486849226468753408.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch  3.36 | loss 79.95 | ppl 52702402966948446287284317821337600.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch  3.38 | loss 80.22 | ppl 69271774803087072381861526769762304.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch  3.42 | loss 80.02 | ppl 56677470413144161092948162814935040.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch  3.39 | loss 80.32 | ppl 76628116592461179486292370471780352.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch  3.39 | loss 79.92 | ppl 50944676504765749231372067466641408.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch  3.35 | loss 79.91 | ppl 50566552700378997628031371185225728.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch  3.36 | loss 79.83 | ppl 46755494506512757754258543251292160.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch  3.39 | loss 80.07 | ppl 59575701308091368457264107656052736.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch  3.35 | loss 79.88 | ppl 49054816955678962482832847941926912.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch  3.36 | loss 80.02 | ppl 56509967061367084612417814624468992.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch  3.37 | loss 79.98 | ppl 54564623951003792244605834305208320.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch  3.34 | loss 79.77 | ppl 44218477506941014600108475089420288.00\n",
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch  3.38 | loss 79.59 | ppl 36800330568373742693548618179149824.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch  3.34 | loss 80.00 | ppl 55383972783812229478277667058876416.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch  3.34 | loss 80.22 | ppl 68730575222233848262132697510445056.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch  3.35 | loss 80.02 | ppl 56508449458533935891244210460819456.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch  3.39 | loss 79.91 | ppl 50878607537907838137240637598072832.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch  3.38 | loss 79.69 | ppl 40755536607177278618163819826905088.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch  3.38 | loss 80.46 | ppl 88071884143869313090400977690820608.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch  3.39 | loss 79.85 | ppl 47618637426898219115399144884142080.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch  3.35 | loss 80.10 | ppl 61084171081444498427262763983175680.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch  3.32 | loss 79.80 | ppl 45147675110978862296995881356886016.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch  3.36 | loss 80.00 | ppl 55379093900244754684107097099993088.00\n",
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch  3.34 | loss 79.62 | ppl 37707894466871101268416371332481024.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch  3.37 | loss 79.76 | ppl 43428292490901781217142422533832704.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch  3.34 | loss 80.16 | ppl 65300594218247232918953728478806016.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch  3.38 | loss 79.70 | ppl 40927077525471146529318139995357184.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch  3.35 | loss 79.34 | ppl 28680627643092558856413951360499712.00\n",
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch  3.31 | loss 80.36 | ppl 79568482479144396096202503377387520.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch  3.37 | loss 80.10 | ppl 61302617756366122888582665011724288.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch  3.36 | loss 79.56 | ppl 35659433157285735531569653911388160.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch  3.36 | loss 79.75 | ppl 43193937195909271771083120538812416.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch  3.40 | loss 79.84 | ppl 46996392361509881872965816805228544.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch  3.39 | loss 79.90 | ppl 50278441589111236048604571109425152.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch  3.36 | loss 79.99 | ppl 55057718980414976800721844880539648.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch  3.37 | loss 79.91 | ppl 50600823637925192940268515164684288.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch  3.36 | loss 79.93 | ppl 51584594441249035491046213344559104.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch  3.38 | loss 79.94 | ppl 52183676526378710910415426967044096.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch  3.38 | loss 79.91 | ppl 50444052471144642621094720952074240.00\n",
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch  3.34 | loss 79.99 | ppl 54837207177980568338600520826486784.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch  3.39 | loss 79.72 | ppl 41697317592995780700560687791865856.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch  3.35 | loss 79.57 | ppl 35979715796373131086600381835247616.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch  3.35 | loss 79.76 | ppl 43558609021399684284079647420841984.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch  3.38 | loss 79.93 | ppl 51905540761063832116127630568194048.00\n",
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch  3.35 | loss 80.02 | ppl 56585178531891600862484989843865600.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch  3.38 | loss 79.40 | ppl 30432704742999543546786108319727616.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch  3.40 | loss 80.19 | ppl 67076893019633436981829258467868672.00\n",
      "[12.38103676 12.38103676 12.38103676 ... 12.38103676 12.38103676\n",
      " 12.38103676]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is nan\n",
      "Model 5:   emsize: 1024   d_hid: 200   nlayers: 1   nhead: 4   r:nan\n",
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch 10.18 | loss 96.15 | ppl 570715567668547203085735804156244909883392.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch 10.09 | loss 86.05 | ppl 23441028050657281715640034521551732736.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch 10.16 | loss 82.79 | ppl 898311480021589657292921258855366656.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch 10.15 | loss 82.32 | ppl 563531894128629477322179660258213888.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch 10.12 | loss 81.68 | ppl 297266026562725670341139515280719872.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch 10.08 | loss 81.88 | ppl 362720836275131743308785963698749440.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch 10.18 | loss 81.55 | ppl 261207704719452546126974862575534080.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch 10.14 | loss 81.15 | ppl 175376311533066494475556017045241856.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch 10.13 | loss 81.05 | ppl 158835451098589575795519454850318336.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch 10.08 | loss 81.19 | ppl 181538343367522943269569607837417472.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch 10.12 | loss 81.04 | ppl 156911310641553264604237554169937920.00\n",
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch 10.12 | loss 80.73 | ppl 114635665393623576098131116537413632.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch 10.15 | loss 80.72 | ppl 114012236729022060598631988904067072.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch 10.13 | loss 81.07 | ppl 160728843548293144153942448767238144.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch 10.16 | loss 80.57 | ppl 98426770469576172839457243648753664.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch 10.15 | loss 80.99 | ppl 149799072286530141902986959682273280.00\n",
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch 10.14 | loss 81.17 | ppl 179218738837945943200853092877729792.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch 10.15 | loss 80.24 | ppl 70581285070694945204586412117065728.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch 10.15 | loss 80.42 | ppl 84074470068608764966450897419763712.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch 10.20 | loss 80.65 | ppl 106514020862518960704761171038699520.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch 10.13 | loss 80.50 | ppl 91536606085498186837700822364585984.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch 10.09 | loss 81.12 | ppl 170452446409654473078010975218565120.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch 10.16 | loss 80.80 | ppl 123850348531793797803280631324475392.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch 10.11 | loss 80.62 | ppl 102689488161982706460951049888858112.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch 10.09 | loss 80.65 | ppl 106024214028563239197216224877477888.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch 10.20 | loss 80.49 | ppl 90347018014312336347819221033418752.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch 10.11 | loss 80.69 | ppl 110071838580570298202446794655268864.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch 10.11 | loss 80.47 | ppl 88871206753989416528622349669367808.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch 10.13 | loss 80.75 | ppl 116925063227904198808643172260904960.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch 10.16 | loss 80.11 | ppl 61825812079595634769900445215227904.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch 10.15 | loss 80.71 | ppl 112991463966542047509613830035472384.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch 10.17 | loss 80.40 | ppl 82725121168878867426494616172494848.00\n",
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch 10.12 | loss 80.76 | ppl 118997489204452222177880352927776768.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch 10.12 | loss 80.40 | ppl 82511045985238573493259738300809216.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch 10.14 | loss 80.25 | ppl 71378824582467322497645710399242240.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch 10.09 | loss 80.23 | ppl 69713795689968829061735619850403840.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch 10.13 | loss 80.14 | ppl 63461494292050849287978234964606976.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch 10.09 | loss 80.60 | ppl 101186506071736519856787258538459136.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch 10.17 | loss 80.34 | ppl 77685413093214608949836782034223104.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch 10.12 | loss 80.04 | ppl 57751806964129722889886009072287744.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch 10.15 | loss 80.32 | ppl 76657179365876403648601422502559744.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch 10.14 | loss 80.12 | ppl 62775296101408285498837307221868544.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch 10.10 | loss 80.20 | ppl 67842719010407248202160556616974336.00\n",
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch 10.13 | loss 79.93 | ppl 51873974459833903920791332210081792.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch 10.12 | loss 80.43 | ppl 85476945126559683866454696298283008.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch 10.13 | loss 80.08 | ppl 60120275754535169430948543782191104.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch 10.10 | loss 79.97 | ppl 53724867641474323244208270251917312.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch 10.16 | loss 80.12 | ppl 62317317427927202190931812069408768.00\n",
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch 10.12 | loss 80.76 | ppl 118782448782283268137502228447494144.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch 10.12 | loss 79.73 | ppl 42274129697996410754115885493911552.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch 10.14 | loss 80.62 | ppl 102856104499136462900330116683923456.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch 10.14 | loss 80.20 | ppl 67771153217049139627921897766780928.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch 10.18 | loss 80.08 | ppl 60182412878863916994665765968805888.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch 10.14 | loss 80.36 | ppl 79481098290281717273931130213498880.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch 10.07 | loss 79.96 | ppl 53399075599747103277730382007500800.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch 10.09 | loss 80.53 | ppl 94547148707413085757763024590995456.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch 10.13 | loss 80.30 | ppl 74817637812240109679649088843284480.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch 10.14 | loss 79.87 | ppl 48892466298959684082144707210117120.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch 10.16 | loss 80.07 | ppl 59177964381471757370045512846147584.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch 10.13 | loss 80.32 | ppl 76063393858671374268340603468120064.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch 10.13 | loss 80.59 | ppl 100041425538117852356430584464015360.00\n",
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch 10.10 | loss 79.97 | ppl 53776726543619090643794292132282368.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch 10.16 | loss 80.11 | ppl 61720975955200920315517107894747136.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch 10.10 | loss 80.16 | ppl 65128252894996482667367764091469824.00\n",
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch 10.13 | loss 80.01 | ppl 56025005522216147528315440101064704.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch 10.11 | loss 80.35 | ppl 78408885641039629235950250268557312.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch 10.14 | loss 80.07 | ppl 59596028860837057739105398306111488.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch 10.13 | loss 79.97 | ppl 53749434229500281204182697432317952.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch 10.14 | loss 80.32 | ppl 76668753121321500552920562302713856.00\n",
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch 10.14 | loss 80.13 | ppl 63315666284933151080727376281206784.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch 10.07 | loss 80.62 | ppl 102662267210231445712483503036170240.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch 10.12 | loss 79.51 | ppl 34111089181303452465001629579476992.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch 10.16 | loss 80.12 | ppl 62723383200152627383204804208623616.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch 10.34 | loss 79.66 | ppl 39438448456379709599081368100274176.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch 10.14 | loss 80.13 | ppl 62938996284818278552334542890336256.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch 10.18 | loss 80.12 | ppl 62174604738612019416562355511754752.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch 10.21 | loss 79.68 | ppl 40151638631611228142501649595760640.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch 10.16 | loss 80.11 | ppl 62148551031483418672838026616373248.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch 10.14 | loss 79.86 | ppl 48347989351032407629901571236560896.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch 10.17 | loss 80.22 | ppl 68952518165996864184873709163511808.00\n",
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch 10.13 | loss 80.03 | ppl 57302214806129082937473416038449152.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch 10.14 | loss 79.90 | ppl 50021317137634565386929912290476032.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch 10.14 | loss 80.33 | ppl 77007075412400671188964540420194304.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch 10.17 | loss 79.67 | ppl 39743129414604003807398509959708672.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch 10.13 | loss 80.16 | ppl 64813758958714835189293164905103360.00\n",
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch 10.12 | loss 79.61 | ppl 37473206909124262378937067107254272.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch 10.09 | loss 80.10 | ppl 61057865329700608321046999946231808.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch 10.13 | loss 79.62 | ppl 37717192724165817013419392192479232.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch 10.13 | loss 79.94 | ppl 52421603547838243068695599424471040.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch 10.12 | loss 79.86 | ppl 48303087771839749377038351889072128.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch 10.11 | loss 80.08 | ppl 60162192778055695175865046367469568.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch 10.10 | loss 79.75 | ppl 43333155406242103442987231096078336.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch 10.13 | loss 80.18 | ppl 66295632109907299332834340328439808.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch 10.10 | loss 79.73 | ppl 42274392831664079008369862785368064.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch 10.10 | loss 79.86 | ppl 47950792172916987834515184855023616.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch 10.16 | loss 79.84 | ppl 47162656971808068763594651540127744.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch 10.19 | loss 79.74 | ppl 42641218758577933582325124874371072.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch 10.12 | loss 80.03 | ppl 57138226721442619998197610407002112.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch 10.14 | loss 79.72 | ppl 41887825987546660948870969151193088.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch 10.13 | loss 80.11 | ppl 62007107280868613197320248772001792.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch 10.13 | loss 79.98 | ppl 54379960596751163867919959239688192.00\n",
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch 10.15 | loss 79.81 | ppl 45816861527933888492415433320693760.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch 10.18 | loss 79.80 | ppl 45235089957702282903017040885317632.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch 10.05 | loss 79.89 | ppl 49847968721818895377837434146390016.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch 10.10 | loss 79.94 | ppl 52298195737298554387098096151560192.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch 10.16 | loss 79.47 | ppl 32513491002630753771595789429112832.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch 10.15 | loss 79.73 | ppl 42281560124696535917571582636064768.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch 10.13 | loss 80.37 | ppl 80409505985443062096834258934956032.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch 10.11 | loss 80.16 | ppl 64795373075997802989492572849700864.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch 10.13 | loss 79.55 | ppl 35505560598732445536361724831072256.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch 10.11 | loss 79.47 | ppl 32614023423976494318290744278777856.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch 10.18 | loss 80.43 | ppl 85253552066700602825145065131737088.00\n",
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch 10.12 | loss 79.41 | ppl 30689490118399939407323860050116608.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch 10.12 | loss 79.83 | ppl 46813542360329893908812533531148288.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch 10.12 | loss 80.04 | ppl 57615110561156068553540129418903552.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch 10.17 | loss 79.62 | ppl 37701777082234064235590551234150400.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch 10.13 | loss 79.63 | ppl 38381341066561180739852095708987392.00\n",
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch 10.14 | loss 79.49 | ppl 33334395104663739201335436846825472.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch 10.14 | loss 80.42 | ppl 84078789425920643634106822752731136.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch 10.15 | loss 79.33 | ppl 28314430011752497838475524083548160.00\n",
      "[11.19287586 11.19287586 11.19287586 ... 11.19287586 11.19287586\n",
      " 11.19287586]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is nan\n",
      "Model 6:   emsize: 1024   d_hid: 200   nlayers: 4   nhead: 1   r:nan\n",
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch 10.43 | loss 94.13 | ppl 76189324779867701815727715558454603022336.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch 10.38 | loss 86.14 | ppl 25602434663642726265830933936514007040.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch 10.32 | loss 82.98 | ppl 1094636932322265330906590504265187328.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch 10.42 | loss 82.42 | ppl 624897853957321864784069579056873472.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch 10.41 | loss 81.76 | ppl 320928479388533599301438692128718848.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch 10.37 | loss 82.26 | ppl 531110802621531430005278945161773056.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch 10.38 | loss 81.32 | ppl 208095479268453302930903878447661056.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch 10.43 | loss 81.34 | ppl 211967979070561211063173905928683520.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch 10.40 | loss 81.28 | ppl 199485638427985364728088359702888448.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch 10.42 | loss 81.19 | ppl 181435953544485243906873663598624768.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch 10.39 | loss 80.83 | ppl 127539012779910649161123605315059712.00\n",
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch 10.41 | loss 81.04 | ppl 157041482705030518813241198812069888.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch 10.42 | loss 80.81 | ppl 125008508523686498225435899204730880.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch 10.41 | loss 80.92 | ppl 138570700446891698820093821826629632.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch 10.34 | loss 81.04 | ppl 156793778282619733968773732741677056.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch 10.40 | loss 80.53 | ppl 93789055467383252677356283043512320.00\n",
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch 10.37 | loss 80.62 | ppl 102795934781323420799904596742897664.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch 10.39 | loss 81.04 | ppl 156855304622728070044924991596658688.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch 10.44 | loss 80.58 | ppl 99301385784514547992008214643212288.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch 10.42 | loss 80.96 | ppl 145343754410136011771971153810161664.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch 10.35 | loss 80.55 | ppl 95853153993520231319502232058068992.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch 10.41 | loss 80.79 | ppl 121746377534208697271265325674921984.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch 10.44 | loss 80.33 | ppl 76895312353365443080898953512747008.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch 10.39 | loss 80.99 | ppl 148410824715946784273463869618257920.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch 10.42 | loss 80.79 | ppl 121928919898348616069376593778180096.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch 10.41 | loss 81.13 | ppl 171864384333420132050946970867466240.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch 10.44 | loss 80.79 | ppl 122530652744897807482463378642829312.00\n",
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch 10.41 | loss 80.03 | ppl 57019983912360899590150569496412160.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch 10.41 | loss 80.29 | ppl 74134361446298706368493436790636544.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch 10.40 | loss 80.81 | ppl 125147693395491113211571621985255424.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch 10.43 | loss 80.34 | ppl 78014058532765085317893638710099968.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch 10.40 | loss 80.36 | ppl 79792769627466321423427191898636288.00\n",
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch 10.41 | loss 80.54 | ppl 94657265550041268377420344781176832.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch 10.41 | loss 80.68 | ppl 109272909593283325275948091095121920.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch 10.39 | loss 80.57 | ppl 97972703694334065533215385824788480.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch 10.46 | loss 80.45 | ppl 86919353236980979413114189084884992.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch 10.41 | loss 79.75 | ppl 43175940747938539754588114679496704.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch 10.51 | loss 80.41 | ppl 83243377019280875740013768413806592.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch 10.35 | loss 80.20 | ppl 67847381183609264711530093519831040.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch 10.41 | loss 80.18 | ppl 66256863373105988436380892117598208.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch 10.40 | loss 80.43 | ppl 85591929027164150471800197516099584.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch 10.40 | loss 80.32 | ppl 76527813856432236558741337010077696.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch 10.39 | loss 80.13 | ppl 63197075850192153515097730771320832.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch 10.39 | loss 80.20 | ppl 67548248201209133889170088927428608.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch 10.39 | loss 80.17 | ppl 65534203338069971787716557283524608.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch 10.37 | loss 80.60 | ppl 101059928747022798339167307610193920.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch 10.42 | loss 80.12 | ppl 62178628083514123752330543830663168.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch 10.42 | loss 80.28 | ppl 73016507865961692313724255492636672.00\n",
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch 10.61 | loss 79.99 | ppl 54866127086056770990219923474612224.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch 10.62 | loss 79.90 | ppl 49945899322581909030151481743900672.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch 10.41 | loss 80.28 | ppl 73370659314820187960852544684556288.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch 10.41 | loss 80.20 | ppl 67972108493900932654998824933130240.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch 10.40 | loss 80.28 | ppl 73208319125478496512685071937830912.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch 10.36 | loss 80.34 | ppl 77829856911593430538945455131197440.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch 10.41 | loss 80.01 | ppl 56176187318665223971857891520937984.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch 10.39 | loss 80.21 | ppl 68054726531076261774909853059252224.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch 10.38 | loss 79.94 | ppl 51954786437536292646963870135484416.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch 10.36 | loss 80.16 | ppl 64836173974085646870944293085773824.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch 10.39 | loss 80.08 | ppl 59916078290003645484179632911548416.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch 10.40 | loss 80.26 | ppl 71619523002744357184952083249364992.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch 10.41 | loss 80.14 | ppl 63914511348073873202949528009834496.00\n",
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch 10.42 | loss 80.10 | ppl 61395755096279625117298302889492480.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch 10.37 | loss 80.31 | ppl 75498297326424717666040407480336384.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch 10.42 | loss 79.69 | ppl 40836129235694242999883116278972416.00\n",
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch 10.42 | loss 80.07 | ppl 59589834373104937102162373753438208.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch 10.41 | loss 80.15 | ppl 64490209924172792684409668086267904.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch 10.37 | loss 80.18 | ppl 66341467737311038383506497789231104.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch 10.36 | loss 79.96 | ppl 53384831587791762471571559010009088.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch 10.40 | loss 80.28 | ppl 73005880963863067305701900734693376.00\n",
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch 10.42 | loss 79.93 | ppl 51522558117906039006118072876531712.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch 10.36 | loss 80.23 | ppl 69566808546563727037890261197783040.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch 10.39 | loss 80.25 | ppl 71169612440554009140574802497503232.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch 10.37 | loss 79.97 | ppl 53644713845609316457720287667945472.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch 10.42 | loss 80.08 | ppl 59953037300256943737713750335029248.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch 10.44 | loss 80.02 | ppl 56361412116788569309726279886438400.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch 10.45 | loss 80.47 | ppl 88778881020101866774825506646786048.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch 10.43 | loss 79.85 | ppl 47661596799277619803620256165920768.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch 10.40 | loss 79.94 | ppl 52261541795988525034961609372467200.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch 10.42 | loss 80.40 | ppl 82393496758691608662702931415400448.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch 10.38 | loss 79.74 | ppl 42913243116909315866137854329487360.00\n",
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch 10.43 | loss 79.94 | ppl 52364464579239055667888140146704384.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch 10.41 | loss 80.04 | ppl 57954789041188217568463408538320896.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch 10.42 | loss 79.82 | ppl 46251462308802965550365619498516480.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch 10.36 | loss 80.09 | ppl 60811083347165217999724236335218688.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch 10.38 | loss 79.91 | ppl 50686110655989654790992819050774528.00\n",
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch 10.36 | loss 80.20 | ppl 67778823923565244852967011449307136.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch 10.43 | loss 79.91 | ppl 50661856738270271532456801747337216.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch 10.39 | loss 79.97 | ppl 53922519177066671516207058536890368.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch 10.42 | loss 79.60 | ppl 37251763668512765684017939873792000.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch 10.40 | loss 80.14 | ppl 63875509005771385923044442208468992.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch 10.40 | loss 79.44 | ppl 31781101027600721808423968885964800.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch 10.40 | loss 80.50 | ppl 91500622754524496505283820317048832.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch 10.40 | loss 79.93 | ppl 51454828646805673933737648680075264.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch 10.37 | loss 79.75 | ppl 42999341889563802546241689326977024.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch 10.41 | loss 80.41 | ppl 83281102553028970911778469503303680.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch 10.39 | loss 79.16 | ppl 24014151925513780248389365216051200.00\n",
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch 10.35 | loss 79.73 | ppl 42310066201888585827177464747524096.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch 10.42 | loss 80.37 | ppl 80469836687361365555750297457917952.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch 10.46 | loss 80.04 | ppl 57466929631557379738476133792874496.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch 10.38 | loss 79.94 | ppl 52171629054646657509718579233161216.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch 10.38 | loss 79.95 | ppl 52633624811821398727078022656032768.00\n",
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch 10.42 | loss 79.80 | ppl 45369204964280425572186470509182976.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch 10.40 | loss 79.82 | ppl 46117329593845456710268339494060032.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch 10.40 | loss 79.60 | ppl 37106094738443082067340913853595648.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch 10.41 | loss 79.85 | ppl 47816053007957342708497460297203712.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch 10.38 | loss 79.94 | ppl 52052278094138025933697339740913664.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch 10.40 | loss 80.02 | ppl 56313813881416143911489983890849792.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch 10.39 | loss 80.31 | ppl 75177300115769798934773548716130304.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch 10.36 | loss 79.79 | ppl 45043982842017509447347679564660736.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch 10.39 | loss 79.68 | ppl 40170887462070400666150323882033152.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch 10.39 | loss 80.01 | ppl 56053452040063834601066365915758592.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch 10.41 | loss 80.03 | ppl 57362170381104292729347979234770944.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch 10.43 | loss 79.65 | ppl 39144567112611478977487985491050496.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch 10.44 | loss 79.86 | ppl 48134707671292505025469489566711808.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch 10.47 | loss 79.78 | ppl 44577111263787465301432376791924736.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch 10.42 | loss 79.84 | ppl 47312929696509852450712096956481536.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch 10.44 | loss 79.99 | ppl 55100162144621314191811776016809984.00\n",
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch 10.40 | loss 79.68 | ppl 40294885932915022984979982160232448.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch 10.42 | loss 79.86 | ppl 48165891355445270385429628512632832.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch 10.38 | loss 79.78 | ppl 44351172257323931293176658502090752.00\n",
      "[10.97680855 10.97680855 10.97680855 ... 10.97680855 10.97680855\n",
      " 10.97680855]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is nan\n",
      "Model 7:   emsize: 1024   d_hid: 200   nlayers: 4   nhead: 4   r:nan\n",
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch  3.37 | loss 92.27 | ppl 11838205750890477349505020183929275023360.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch  3.30 | loss 79.33 | ppl 28317110312231377253811201666711552.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch  3.32 | loss 75.74 | ppl 783007842486477353100345992019968.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch  3.33 | loss 74.83 | ppl 316061021115379413602170988134400.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch  3.34 | loss 73.81 | ppl 113614531051964107732016233971712.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch  3.32 | loss 73.62 | ppl 93743820470809275441917741498368.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch  3.33 | loss 72.29 | ppl 24849268644970627254666569711616.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch  3.36 | loss 72.89 | ppl 45177762533938565005117929553920.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch  3.34 | loss 72.31 | ppl 25295276297402370013230822064128.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch  3.30 | loss 72.05 | ppl 19486085921548987295084956352512.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch  3.28 | loss 71.22 | ppl 8529022641173491563663657533440.00\n",
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch  3.32 | loss 71.88 | ppl 16518738883805439660237913063424.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch  3.33 | loss 71.61 | ppl 12529326176456001685015962320896.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch  3.32 | loss 71.64 | ppl 12968214649451940472707730112512.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch  3.32 | loss 71.43 | ppl 10538700733634834804867871014912.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch  3.29 | loss 70.60 | ppl 4579466563733938133524417609728.00\n",
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch  3.37 | loss 70.52 | ppl 4232201610747482442455438065664.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch  3.31 | loss 70.71 | ppl 5141077122904537928042394157056.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch  3.30 | loss 70.60 | ppl 4565651445333126497703932461056.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch  3.35 | loss 70.03 | ppl 2598067150499568299716154228736.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch  3.30 | loss 70.24 | ppl 3204803780038608446418317213696.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch  3.31 | loss 70.21 | ppl 3109613996445511786584260739072.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch  3.31 | loss 69.64 | ppl 1759241367685978675764965408768.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch  3.32 | loss 70.55 | ppl 4339611907567041104782737539072.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch  3.28 | loss 69.80 | ppl 2066360295781785954224911155200.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch  3.33 | loss 70.11 | ppl 2799256613246040054115409592320.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch  3.41 | loss 69.88 | ppl 2232461482288309389811058212864.00\n",
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch  3.31 | loss 69.71 | ppl 1885257824753170373334580330496.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch  3.29 | loss 69.63 | ppl 1742227371725964731623543930880.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch  3.32 | loss 69.45 | ppl 1445903908511575127692448628736.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch  3.31 | loss 69.51 | ppl 1546184847084333372770439135232.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch  3.32 | loss 70.19 | ppl 3030074044094072055892354793472.00\n",
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch  3.30 | loss 69.53 | ppl 1573109061237108433960870871040.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch  3.36 | loss 70.08 | ppl 2717886727385965593381608357888.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch  3.31 | loss 69.27 | ppl 1210929103512410641062233636864.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch  3.31 | loss 68.98 | ppl 904274965373863305689448841216.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch  3.34 | loss 69.21 | ppl 1137566693605888947340181504000.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch  3.32 | loss 69.02 | ppl 944650414913807885236881588224.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch  3.34 | loss 69.15 | ppl 1071633762640302807165192634368.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch  3.37 | loss 69.40 | ppl 1374173552036045274149793300480.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch  3.31 | loss 68.96 | ppl 888139135292357859636492107776.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch  3.34 | loss 68.66 | ppl 656094257126572032587336777728.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch  3.29 | loss 68.48 | ppl 549119395868226694535299727360.00\n",
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch  3.33 | loss 69.49 | ppl 1515413853917529179952292823040.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch  3.32 | loss 69.19 | ppl 1123294008129256878030047936512.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch  3.33 | loss 68.72 | ppl 701142440071272561782919528448.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch  3.34 | loss 68.68 | ppl 672284087083753858167925112832.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch  3.34 | loss 68.55 | ppl 590138226555739640776605302784.00\n",
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch  3.34 | loss 68.15 | ppl 396352379482167575304449556480.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch  3.35 | loss 68.68 | ppl 674426941127981636026617561088.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch  3.30 | loss 68.48 | ppl 548455377112149652638468669440.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch  3.37 | loss 68.29 | ppl 452794262217880951420620898304.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch  3.32 | loss 68.58 | ppl 608822053725595398284855214080.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch  3.32 | loss 68.37 | ppl 494363188129120479734711451648.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch  3.31 | loss 68.06 | ppl 359845032201475572245288976384.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch  3.35 | loss 68.33 | ppl 473499076975548717109998518272.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch  3.30 | loss 67.99 | ppl 338489438119964614330484260864.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch  3.33 | loss 68.83 | ppl 780958087260333768264990588928.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch  3.30 | loss 68.48 | ppl 548880610588262570682402996224.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch  3.33 | loss 68.63 | ppl 636838747417145641096863285248.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch  3.37 | loss 68.63 | ppl 638441950720573555071770427392.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch  3.30 | loss 67.89 | ppl 304030594173129697622614867968.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch  3.33 | loss 68.25 | ppl 436629547909903757550708326400.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch  3.31 | loss 68.12 | ppl 383658060799483702659147366400.00\n",
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch  3.28 | loss 68.19 | ppl 412642527301242955217879695360.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch  3.31 | loss 68.20 | ppl 414769932695740034642975129600.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch  3.35 | loss 68.15 | ppl 396871433575874362535053361152.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch  3.33 | loss 68.03 | ppl 350726500373468419005364568064.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch  3.34 | loss 67.69 | ppl 248969442922795178756071751680.00\n",
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch  3.35 | loss 68.02 | ppl 346587871304526454134742712320.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch  3.37 | loss 67.75 | ppl 266349191188918292536872665088.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch  3.31 | loss 67.74 | ppl 263298216525185957576036057088.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch  3.30 | loss 67.83 | ppl 286820169591010927417736822784.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch  3.32 | loss 67.77 | ppl 271212080646131259213256589312.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch  3.34 | loss 68.04 | ppl 353953789560386763739638005760.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch  3.35 | loss 67.25 | ppl 161509390097506179648650739712.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch  3.32 | loss 67.92 | ppl 313275525637664446367095848960.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch  3.36 | loss 67.60 | ppl 227591831069936077667435544576.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch  3.36 | loss 67.52 | ppl 210256813688104181144011931648.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch  3.33 | loss 67.58 | ppl 224115395302474271831107829760.00\n",
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch  3.35 | loss 67.83 | ppl 285981067749374303641396051968.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch  3.33 | loss 67.73 | ppl 259509193514033802308480925696.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch  3.33 | loss 67.38 | ppl 183201979534657392990935318528.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch  3.36 | loss 68.05 | ppl 357007142740290637755639136256.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch  3.32 | loss 67.19 | ppl 152188798504291340716311838720.00\n",
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch  3.32 | loss 67.36 | ppl 179867099855498253694389977088.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch  3.32 | loss 67.88 | ppl 302970381726418054976389513216.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch  3.36 | loss 67.23 | ppl 157761576488783846882276278272.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch  3.30 | loss 67.56 | ppl 219364505357961050300354658304.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch  3.30 | loss 67.40 | ppl 186659397383454856313806258176.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch  3.34 | loss 67.64 | ppl 236717239286889665291047927808.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch  3.34 | loss 66.92 | ppl 115674502884935522360397660160.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch  3.32 | loss 67.14 | ppl 143676824185742739641190055936.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch  3.32 | loss 67.44 | ppl 194619447494586791863024353280.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch  3.29 | loss 67.02 | ppl 128073620242440473968864395264.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch  3.36 | loss 67.37 | ppl 182064945826352760945157079040.00\n",
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch  3.35 | loss 67.26 | ppl 162897475568566785138399117312.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch  3.32 | loss 67.39 | ppl 185240913686002077097210675200.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch  3.35 | loss 66.95 | ppl 118903995555204010181977440256.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch  3.33 | loss 67.60 | ppl 228956789149318270084272095232.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch  3.32 | loss 67.18 | ppl 149219019508615351666349178880.00\n",
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch  3.29 | loss 67.32 | ppl 173048737436940154640267214848.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch  3.33 | loss 67.04 | ppl 130446405599363492693657255936.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch  3.30 | loss 66.76 | ppl 98959799016260437697630306304.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch  3.32 | loss 66.96 | ppl 120436107773155955350144286720.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch  3.32 | loss 66.86 | ppl 108954179263147515807963021312.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch  3.30 | loss 66.69 | ppl 91825621725861186742612131840.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch  3.34 | loss 67.32 | ppl 172340424140227134372016291840.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch  3.32 | loss 66.73 | ppl 95697792719086245342817026048.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch  3.31 | loss 67.47 | ppl 199855637592496238412108398592.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch  3.31 | loss 66.84 | ppl 106269008836181023835643969536.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch  3.28 | loss 67.22 | ppl 156256780623378521355997675520.00\n",
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch  3.34 | loss 67.32 | ppl 171997581817341825458611683328.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch  3.29 | loss 67.05 | ppl 131215078340476849804550340608.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch  3.34 | loss 67.11 | ppl 140103806975400441996984188928.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch  3.31 | loss 66.15 | ppl 53333032318662898187979718656.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch  3.30 | loss 66.49 | ppl 75317890631803018619767488512.00\n",
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch  3.31 | loss 66.77 | ppl 99983794508154019350084321280.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch  3.33 | loss 66.90 | ppl 113836565773854992263572094976.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch  3.32 | loss 67.12 | ppl 141010966414035780295219740672.00\n",
      "[10.81129646 10.87336159 10.80174446 ... 10.9279232  10.86354065\n",
      " 10.92480087]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is 0.33\n",
      "Model 8:   emsize: 1024   d_hid: 350   nlayers: 1   nhead: 1   r:0.327\n",
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch  3.39 | loss 94.35 | ppl 94699366344100131808851643535487999672320.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch  3.42 | loss 85.43 | ppl 12626320920103650122634707902373298176.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch  3.33 | loss 82.60 | ppl 744196495471360724848139392755171328.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch  3.35 | loss 82.15 | ppl 473637924589334365858280725830172672.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch  3.37 | loss 81.81 | ppl 338153957943098636442738926920138752.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch  3.34 | loss 81.82 | ppl 340878070163134358192278800730947584.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch  3.37 | loss 82.03 | ppl 423973366789023856081023222912385024.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch  3.40 | loss 81.18 | ppl 180061196895513146522768054750806016.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch  3.38 | loss 81.11 | ppl 168808697332227544132038003905265664.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch  3.33 | loss 80.66 | ppl 107721758720470916472329640995717120.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch  3.39 | loss 81.47 | ppl 240625562288505415919231435498586112.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch  3.36 | loss 80.42 | ppl 84295386268945342234617920608010240.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch  3.38 | loss 80.84 | ppl 128023819260217021674252742728613888.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch  3.36 | loss 80.49 | ppl 90328323012813682742577570863120384.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch  3.40 | loss 80.99 | ppl 149389948241998409990610029318766592.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch  3.41 | loss 80.49 | ppl 90612726134177325448444048079585280.00\n",
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch  3.38 | loss 80.70 | ppl 112103321423756638980246827163451392.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch  3.38 | loss 80.95 | ppl 143687377480498665817388969225093120.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch  3.41 | loss 81.07 | ppl 161822731091587212326520515148644352.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch  3.39 | loss 80.42 | ppl 83908215266969848348758236635070464.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch  3.43 | loss 80.64 | ppl 104651574214297863160062341118164992.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch  3.38 | loss 80.79 | ppl 122529080409776755332120533234876416.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch  3.40 | loss 80.87 | ppl 131689858624157072546574038290399232.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch  3.37 | loss 80.24 | ppl 70091549700276833455801127067975680.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch  3.40 | loss 80.63 | ppl 104392105371408045811227320951767040.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch  3.37 | loss 80.44 | ppl 86256609149824903500850195396034560.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch  3.39 | loss 80.27 | ppl 72286431838773946694969118152458240.00\n",
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch  3.38 | loss 80.61 | ppl 102429839316402161604612458217996288.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch  3.36 | loss 80.28 | ppl 72945091427525598678525140375961600.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch  3.34 | loss 80.14 | ppl 63758953515174783620252306119852032.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch  3.38 | loss 80.30 | ppl 74930269988877306620101934800961536.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch  3.33 | loss 80.62 | ppl 102979120012834451897393223282196480.00\n",
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch  3.38 | loss 80.13 | ppl 63161392468216669370955543212982272.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch  3.38 | loss 80.51 | ppl 92280118889683107098888886286286848.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch  3.39 | loss 80.51 | ppl 92342124294206642863555660644089856.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch  3.41 | loss 80.48 | ppl 89168172671194719572829620687339520.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch  3.35 | loss 80.50 | ppl 90916433128168432217577363225968640.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch  3.36 | loss 80.30 | ppl 74644001652865095187684571989671936.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch  3.41 | loss 79.85 | ppl 47543530339295608062943564537004032.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch  3.31 | loss 80.35 | ppl 78604475807967463159076055238574080.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch  3.36 | loss 80.02 | ppl 56687703005599871494339390972362752.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch  3.35 | loss 80.62 | ppl 103189703149465505384563719631011840.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch  3.37 | loss 80.28 | ppl 73215301957725091388518200193843200.00\n",
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch  3.42 | loss 80.57 | ppl 97924004519766523406196887913496576.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch  3.40 | loss 80.44 | ppl 85807537115711017279776509764042752.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch  3.33 | loss 79.92 | ppl 51214722472369152524814939068563456.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch  3.36 | loss 80.66 | ppl 107561973256028217082014151701692416.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch  3.38 | loss 80.04 | ppl 57447011826021319793102269787930624.00\n",
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch  3.35 | loss 80.39 | ppl 81722640693096356926228167485554688.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch  3.43 | loss 80.16 | ppl 65285523194321740403837146534772736.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch  3.40 | loss 80.27 | ppl 72894497981173799027763618858926080.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch  3.38 | loss 80.00 | ppl 55200414002845960211349954761850880.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch  3.41 | loss 79.85 | ppl 47470229538192257998774881839218688.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch  3.36 | loss 80.34 | ppl 77690495273074494509209745719558144.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch  3.40 | loss 80.25 | ppl 70944582715045640512733493371863040.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch  3.37 | loss 80.20 | ppl 67849089139938705434721123462807552.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch  3.36 | loss 80.18 | ppl 66607511097682322318640967336329216.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch  3.35 | loss 79.95 | ppl 52865571772897454127143224736743424.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch  3.37 | loss 79.95 | ppl 52687795064806651095659658847715328.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch  3.39 | loss 80.11 | ppl 61729853009948095412081404007153664.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch  3.34 | loss 80.01 | ppl 55896945640659051404285944426135552.00\n",
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch  3.36 | loss 80.00 | ppl 55614337878026850481009117859151872.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch  3.39 | loss 80.29 | ppl 73888778913380022461491267711795200.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch  3.35 | loss 79.72 | ppl 41981312504608703766330354228527104.00\n",
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch  3.39 | loss 80.16 | ppl 64834677964102156743311809310621696.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch  3.37 | loss 80.10 | ppl 61374279566544020109590658426077184.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch  3.39 | loss 79.75 | ppl 42998125857865620695321509562941440.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch  3.33 | loss 80.25 | ppl 70958172968792191641711275044503552.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch  3.35 | loss 79.76 | ppl 43696997117518792562088449786511360.00\n",
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch  3.35 | loss 79.98 | ppl 54451981808847318368323160604934144.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch  3.38 | loss 80.02 | ppl 56675603860608308231303521523204096.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch  3.35 | loss 79.85 | ppl 47556522377571549007849028283531264.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch  3.40 | loss 79.60 | ppl 37052144772990954147642886351486976.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch  3.38 | loss 80.44 | ppl 86205970192867109690173824096534528.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch  3.37 | loss 80.14 | ppl 63564158980166688848114965301690368.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch  3.36 | loss 80.27 | ppl 72662717201732131017654368938754048.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch  3.42 | loss 79.92 | ppl 50952369548258900426860599388930048.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch  3.36 | loss 80.27 | ppl 72352002574730446067257765106024448.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch  3.35 | loss 79.84 | ppl 47299530060453640586491054792900608.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch  3.38 | loss 80.02 | ppl 56267572388611654722357187378675712.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch  3.34 | loss 79.96 | ppl 52973068788664906142727577375080448.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch  3.35 | loss 80.25 | ppl 71117560646930975683491877191942144.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch  3.40 | loss 80.12 | ppl 62387745021181043312363163311144960.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch  3.37 | loss 79.73 | ppl 42381110559869698273416833782513664.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch  3.41 | loss 80.20 | ppl 67980596591790169304525188643160064.00\n",
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch  3.34 | loss 79.79 | ppl 44732238577132798649035352405180416.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch  3.36 | loss 79.46 | ppl 32388879466039633064583842079178752.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch  3.40 | loss 80.12 | ppl 62557429900686041425036472974573568.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch  3.38 | loss 80.22 | ppl 68773960533958437058461835108286464.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch  3.38 | loss 79.46 | ppl 32238619691846339700312480480755712.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch  3.37 | loss 79.87 | ppl 48537547200391169866023898559021056.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch  3.36 | loss 79.92 | ppl 51158466937399427627109479348174848.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch  3.35 | loss 79.85 | ppl 47542930372253300096288152648417280.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch  3.38 | loss 79.66 | ppl 39437429365138583376098875255291904.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch  3.38 | loss 79.29 | ppl 27162250283952435256254862765064192.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch  3.41 | loss 80.11 | ppl 61590100139187770684028201174302720.00\n",
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch  3.37 | loss 79.76 | ppl 43599496165465559767551853507641344.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch  3.37 | loss 80.00 | ppl 55527443956089962297380443745419264.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch  3.40 | loss 79.90 | ppl 50057461882987115340929881518112768.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch  3.36 | loss 79.89 | ppl 49772036565981511018071361843101696.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch  3.39 | loss 80.05 | ppl 58151618121563200075633977720832000.00\n",
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch  3.37 | loss 79.74 | ppl 42795649598567426927886883742547968.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch  3.39 | loss 79.71 | ppl 41276193276361611222983075812081664.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch  3.34 | loss 79.90 | ppl 50375334276379573462254693105270784.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch  3.36 | loss 79.45 | ppl 31818234805859221170666575618899968.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch  3.36 | loss 80.29 | ppl 74027529746436964405459838574788608.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch  3.38 | loss 80.11 | ppl 61972408255690084529495288829706240.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch  3.33 | loss 79.87 | ppl 48799692520877631024441056160645120.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch  3.35 | loss 79.88 | ppl 48948970101033009160813818352238592.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch  3.36 | loss 79.97 | ppl 53820681283252917700001419747131392.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch  3.38 | loss 80.16 | ppl 64743329398906901013315462776225792.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch  3.37 | loss 79.51 | ppl 33934573336547608557045995454595072.00\n",
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch  3.39 | loss 79.83 | ppl 46757067704549992644964287602753536.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch  3.43 | loss 79.59 | ppl 36848285055215596100209869655638016.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch  3.36 | loss 79.50 | ppl 33533069049930068107612008415756288.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch  3.35 | loss 79.84 | ppl 47255136091761594347575363420291072.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch  3.40 | loss 79.83 | ppl 46605875136881611938189583486812160.00\n",
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch  3.37 | loss 79.87 | ppl 48858705595210439952577947829796864.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch  3.36 | loss 80.38 | ppl 80691282434732721225529487483994112.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch  3.34 | loss 79.56 | ppl 35545633097489365222897602156036096.00\n",
      "[11.598876 11.598876 11.598876 ... 11.598876 11.598876 11.598876]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is nan\n",
      "Model 9:   emsize: 1024   d_hid: 350   nlayers: 1   nhead: 4   r:nan\n",
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch 10.28 | loss 96.71 | ppl 997217495119543067013941028607000859115520.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch 10.27 | loss 87.17 | ppl 71798700588901147071410755752419131392.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch 10.26 | loss 82.32 | ppl 564250560218602265558530299386658816.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch 10.27 | loss 82.85 | ppl 961518655346170206204448143180824576.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch 10.29 | loss 81.82 | ppl 340898689603235008691994257031954432.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch 10.27 | loss 81.46 | ppl 239722222786453556290287860902264832.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch 10.33 | loss 81.76 | ppl 323049690272510451093213529063292928.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch 10.27 | loss 81.25 | ppl 193534633182082720874767750029901824.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch 10.29 | loss 80.88 | ppl 133859760592186355348232440714887168.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch 10.34 | loss 80.88 | ppl 133550652311410224824953275076837376.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch 10.28 | loss 80.71 | ppl 112850655975272724452128698052640768.00\n",
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch 10.23 | loss 80.96 | ppl 144255534283234528029865925234655232.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch 10.25 | loss 80.90 | ppl 136400582090042384266766501934006272.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch 10.25 | loss 80.81 | ppl 124134600720228700055199872970653696.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch 10.26 | loss 80.83 | ppl 126765514840979437242325428745535488.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch 10.27 | loss 80.31 | ppl 75910636455021056176928124439625728.00\n",
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch 10.24 | loss 80.34 | ppl 77928395573420377185905185141030912.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch 10.29 | loss 80.96 | ppl 144483137827607926933194171654078464.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch 10.30 | loss 80.63 | ppl 103819744198548510059695005040115712.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch 10.31 | loss 80.56 | ppl 97312087602162680399637073467801600.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch 10.25 | loss 80.79 | ppl 122524344549300135000520828433465344.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch 10.26 | loss 80.34 | ppl 78113049242661707413407799451844608.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch 10.27 | loss 80.76 | ppl 118918639420162380264423320526520320.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch 10.25 | loss 80.48 | ppl 89325322472449058947725863069679616.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch 10.30 | loss 80.81 | ppl 124826151145923170257742128346038272.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch 10.29 | loss 80.42 | ppl 84247142527306852462692428431753216.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch 10.25 | loss 80.46 | ppl 88020691018137071419088133613944832.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch 10.25 | loss 80.55 | ppl 95757706474690616019309666192326656.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch 10.29 | loss 80.51 | ppl 92711134307560816445052045393657856.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch 10.22 | loss 80.68 | ppl 109727694172761072411604390944702464.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch 10.31 | loss 79.88 | ppl 48993812208664618810378894188740608.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch 10.26 | loss 81.15 | ppl 174994567625781256496566165882011648.00\n",
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch 10.26 | loss 80.83 | ppl 126614968899953330094746314246979584.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch 10.21 | loss 80.20 | ppl 67354185212024141223176844909477888.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch 10.29 | loss 80.14 | ppl 63488240329103184837458174087790592.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch 10.32 | loss 80.34 | ppl 77846127398816537306812841451323392.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch 10.32 | loss 80.21 | ppl 68225726427475379846482405779046400.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch 10.28 | loss 80.24 | ppl 70497238754498195193994766232584192.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch 10.21 | loss 79.78 | ppl 44527299811401686167425667647406080.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch 10.28 | loss 81.01 | ppl 152500130452819264854268587471273984.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch 10.35 | loss 80.00 | ppl 55324486250564610210139409645305856.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch 10.27 | loss 80.38 | ppl 81228980359364544665037081965232128.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch 10.27 | loss 80.37 | ppl 80093694927964280057200539420590080.00\n",
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch 10.32 | loss 80.47 | ppl 88581244795796676269470811141701632.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch 10.30 | loss 79.98 | ppl 54514299741677565794992894542610432.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch 10.27 | loss 80.16 | ppl 64913663192480718155565756670017536.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch 10.32 | loss 80.18 | ppl 66219077733006961864618856585101312.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch 10.34 | loss 80.65 | ppl 105855816827666299459034173025550336.00\n",
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch 10.36 | loss 80.16 | ppl 64703292246095513927489704427520000.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch 10.32 | loss 80.29 | ppl 74348129945983084832874629232066560.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch 10.32 | loss 80.23 | ppl 69987649639995764345343839297339392.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch 10.29 | loss 80.40 | ppl 82564129493426433608893340802088960.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch 10.31 | loss 80.26 | ppl 71906115169564281460345550336425984.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch 10.32 | loss 80.33 | ppl 77285899431425058185355728003268608.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch 10.33 | loss 80.31 | ppl 75589076595059199116389324112789504.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch 10.31 | loss 80.28 | ppl 73276212217613998711927795055853568.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch 10.30 | loss 80.00 | ppl 55530193728184289897985632341327872.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch 10.32 | loss 80.35 | ppl 78489934841917756838152571176615936.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch 10.28 | loss 80.32 | ppl 76219332299818124890595437274202112.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch 10.31 | loss 79.81 | ppl 45854196384729124428802101686042624.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch 10.26 | loss 80.04 | ppl 57565124229961418064899315173687296.00\n",
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch 10.34 | loss 80.42 | ppl 84629459486524089136060562113822720.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch 10.31 | loss 80.48 | ppl 89236169144658441254653085085597696.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch 10.31 | loss 79.74 | ppl 42908312594157710264906757686951936.00\n",
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch 10.32 | loss 80.19 | ppl 66718000526461355323951003566342144.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch 10.37 | loss 80.29 | ppl 74210013653072177504990137249955840.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch 10.32 | loss 80.61 | ppl 102251135273413213972478580270039040.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch 10.27 | loss 79.38 | ppl 29854764539788525197387448121819136.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch 10.35 | loss 79.91 | ppl 50505190730942130929189366452453376.00\n",
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch 10.32 | loss 80.18 | ppl 66072961763676289790471174723993600.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch 10.34 | loss 80.07 | ppl 59428855681741317391979533995868160.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch 10.33 | loss 80.25 | ppl 70957870371851744737825441527627776.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch 10.37 | loss 79.98 | ppl 54265980728763754718268736504070144.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch 10.31 | loss 80.10 | ppl 61048993479965674680261190975750144.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch 10.29 | loss 79.90 | ppl 50326127074211424987855342085865472.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch 10.25 | loss 79.79 | ppl 44924969590123490424390208599359488.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch 10.33 | loss 79.89 | ppl 49538874830249460719742670027620352.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch 10.29 | loss 80.27 | ppl 72538077152737544882884826831519744.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch 10.33 | loss 80.03 | ppl 57232645997516177820594699166023680.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch 10.31 | loss 79.93 | ppl 51820134748553736349380307983532032.00\n",
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch 10.37 | loss 79.80 | ppl 45143566481300618817076420195385344.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch 10.32 | loss 79.94 | ppl 52440099611383520746978173534601216.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch 10.30 | loss 79.74 | ppl 42555008729827249020160613126504448.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch 10.28 | loss 80.02 | ppl 56668739636652688458911545145950208.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch 10.33 | loss 79.63 | ppl 38429035477245726160717827719299072.00\n",
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch 10.34 | loss 80.15 | ppl 64586465888895493223629270324084736.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch 10.33 | loss 79.78 | ppl 44469525960781240685276154401128448.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch 10.27 | loss 79.83 | ppl 46957470632687323004233526538141696.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch 10.35 | loss 80.24 | ppl 70119281325315900591225848209604608.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch 10.30 | loss 79.79 | ppl 44989104785942865777987781927108608.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch 10.31 | loss 80.11 | ppl 61805153516350135040405265337810944.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch 10.33 | loss 79.59 | ppl 36924092855871828300276930743107584.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch 10.31 | loss 79.52 | ppl 34133277086368618368127606284353536.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch 10.31 | loss 80.14 | ppl 63906743450942618745824291602825216.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch 10.27 | loss 80.06 | ppl 58794017819618907436999515752628224.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch 10.30 | loss 79.54 | ppl 34964790357872829687627400140029952.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch 10.32 | loss 80.26 | ppl 71755069938279661906497584073015296.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch 10.33 | loss 79.38 | ppl 29805477690602425230616774792183808.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch 10.34 | loss 80.14 | ppl 63838461558709646131415553134297088.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch 10.31 | loss 80.08 | ppl 59891348242125874351931683940335616.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch 10.30 | loss 79.53 | ppl 34544079526952359970884246283747328.00\n",
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch 10.33 | loss 80.33 | ppl 77029882570437084921260782695481344.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch 10.34 | loss 80.14 | ppl 63907683004775299909691434822795264.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch 10.28 | loss 79.99 | ppl 54776691421527186752943110693584896.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch 10.30 | loss 79.48 | ppl 33002846448097368056722186037100544.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch 10.29 | loss 80.08 | ppl 60120930181645505705700257758707712.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch 10.33 | loss 79.77 | ppl 43982354312785689533792718791114752.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch 10.27 | loss 80.00 | ppl 55178497740174426329478983673970688.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch 10.26 | loss 79.80 | ppl 45170921888848371662144032781369344.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch 10.31 | loss 79.71 | ppl 41278215425414775301747220611596288.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch 10.34 | loss 80.02 | ppl 56436555537384140239936852609466368.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch 10.33 | loss 79.60 | ppl 37156185438264633193263303726465024.00\n",
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch 10.26 | loss 79.46 | ppl 32302550905510313168440399918792704.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch 10.26 | loss 80.20 | ppl 67917880644455606014712768278560768.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch 10.34 | loss 80.04 | ppl 57760458773065796030679672034426880.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch 10.34 | loss 79.72 | ppl 42066144495215243237840328754987008.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch 10.30 | loss 79.74 | ppl 42819235119827228577546361878085632.00\n",
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch 10.29 | loss 79.79 | ppl 44759299299708958456988429289783296.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch 10.29 | loss 79.68 | ppl 40164871764971373500223583292489728.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch 10.28 | loss 79.81 | ppl 45873065322254670215262985022078976.00\n",
      "[11.93882561 11.93882561 11.93882561 ... 11.93882561 11.93882561\n",
      " 11.93882561]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is nan\n",
      "Model 10:   emsize: 1024   d_hid: 350   nlayers: 4   nhead: 1   r:nan\n",
      "| epoch   1 |  5000/13056 batches | lr 2.00 | ms/batch 10.60 | loss 96.01 | ppl 495501083865647135952841076125979100839936.00\n",
      "| epoch   1 | 10000/13056 batches | lr 2.00 | ms/batch 10.56 | loss 86.81 | ppl 50352944583809410095176396970609606656.00\n",
      "| epoch   2 |  5000/13056 batches | lr 1.96 | ms/batch 10.53 | loss 83.01 | ppl 1128916029678455525146301461109407744.00\n",
      "| epoch   2 | 10000/13056 batches | lr 1.96 | ms/batch 10.56 | loss 82.35 | ppl 578817373441175064085521733848662016.00\n",
      "| epoch   3 |  5000/13056 batches | lr 1.92 | ms/batch 10.55 | loss 81.88 | ppl 362283181224606059126330715359150080.00\n",
      "| epoch   3 | 10000/13056 batches | lr 1.92 | ms/batch 10.56 | loss 81.61 | ppl 278150627007207542530220633895206912.00\n",
      "| epoch   4 |  5000/13056 batches | lr 1.88 | ms/batch 10.57 | loss 81.36 | ppl 216071446463423091795470164997177344.00\n",
      "| epoch   4 | 10000/13056 batches | lr 1.88 | ms/batch 10.54 | loss 81.28 | ppl 198677926702377410148361206203678720.00\n",
      "| epoch   5 |  5000/13056 batches | lr 1.84 | ms/batch 10.53 | loss 81.06 | ppl 159488460520803720769641852908339200.00\n",
      "| epoch   5 | 10000/13056 batches | lr 1.84 | ms/batch 10.55 | loss 81.17 | ppl 178027338339863072937518037764931584.00\n",
      "| epoch   6 |  5000/13056 batches | lr 1.81 | ms/batch 10.52 | loss 80.78 | ppl 120417104462198219847565601608302592.00\n",
      "| epoch   6 | 10000/13056 batches | lr 1.81 | ms/batch 10.58 | loss 80.64 | ppl 105537460537685820975340160211222528.00\n",
      "| epoch   7 |  5000/13056 batches | lr 1.77 | ms/batch 10.57 | loss 80.79 | ppl 122395627784553008631561501206904832.00\n",
      "| epoch   7 | 10000/13056 batches | lr 1.77 | ms/batch 10.53 | loss 81.25 | ppl 194014950545696122481414709918564352.00\n",
      "| epoch   8 |  5000/13056 batches | lr 1.74 | ms/batch 10.56 | loss 80.96 | ppl 144887309907062138415186081749139456.00\n",
      "| epoch   8 | 10000/13056 batches | lr 1.74 | ms/batch 10.58 | loss 80.51 | ppl 92316959242790676639171890575835136.00\n",
      "| epoch   9 |  5000/13056 batches | lr 1.70 | ms/batch 10.53 | loss 80.30 | ppl 74612454071998965443750351821864960.00\n",
      "| epoch   9 | 10000/13056 batches | lr 1.70 | ms/batch 10.58 | loss 81.20 | ppl 183646553584324625338071283912409088.00\n",
      "| epoch  10 |  5000/13056 batches | lr 1.67 | ms/batch 10.54 | loss 80.26 | ppl 71710528420393009955048986339442688.00\n",
      "| epoch  10 | 10000/13056 batches | lr 1.67 | ms/batch 10.55 | loss 80.69 | ppl 110884993331718970281276657792712704.00\n",
      "| epoch  11 |  5000/13056 batches | lr 1.63 | ms/batch 10.57 | loss 80.58 | ppl 98859388450980366200358304785367040.00\n",
      "| epoch  11 | 10000/13056 batches | lr 1.63 | ms/batch 10.56 | loss 80.56 | ppl 96615970800014739115149325953400832.00\n",
      "| epoch  12 |  5000/13056 batches | lr 1.60 | ms/batch 10.55 | loss 81.17 | ppl 178925094814842626152007010449620992.00\n",
      "| epoch  12 | 10000/13056 batches | lr 1.60 | ms/batch 10.52 | loss 80.11 | ppl 61713910764805341751101688389828608.00\n",
      "| epoch  13 |  5000/13056 batches | lr 1.57 | ms/batch 10.55 | loss 80.66 | ppl 106768706405383810305580380922576896.00\n",
      "| epoch  13 | 10000/13056 batches | lr 1.57 | ms/batch 10.52 | loss 80.26 | ppl 72071144632009019547663469163577344.00\n",
      "| epoch  14 |  5000/13056 batches | lr 1.54 | ms/batch 10.62 | loss 80.28 | ppl 72989296362010011709476722456920064.00\n",
      "| epoch  14 | 10000/13056 batches | lr 1.54 | ms/batch 10.68 | loss 80.72 | ppl 114199483182668374652043185675567104.00\n",
      "| epoch  15 |  5000/13056 batches | lr 1.51 | ms/batch 10.55 | loss 80.47 | ppl 88243763462506752444919576988221440.00\n",
      "| epoch  15 | 10000/13056 batches | lr 1.51 | ms/batch 10.56 | loss 80.62 | ppl 102979482010156501394906170659962880.00\n",
      "| epoch  16 |  5000/13056 batches | lr 1.48 | ms/batch 10.61 | loss 80.45 | ppl 86828832477850613300849578499637248.00\n",
      "| epoch  16 | 10000/13056 batches | lr 1.48 | ms/batch 10.60 | loss 80.52 | ppl 92820913497877289742376895420301312.00\n",
      "| epoch  17 |  5000/13056 batches | lr 1.45 | ms/batch 10.56 | loss 79.91 | ppl 50467832786221897928039354398146560.00\n",
      "| epoch  17 | 10000/13056 batches | lr 1.45 | ms/batch 10.56 | loss 80.53 | ppl 93881051819963014307745280972292096.00\n",
      "| epoch  18 |  5000/13056 batches | lr 1.42 | ms/batch 10.55 | loss 80.33 | ppl 76892980279310708787573821932568576.00\n",
      "| epoch  18 | 10000/13056 batches | lr 1.42 | ms/batch 10.58 | loss 80.52 | ppl 93116565145921211770189495113089024.00\n",
      "| epoch  19 |  5000/13056 batches | lr 1.39 | ms/batch 10.56 | loss 80.33 | ppl 76733513060594873233764743624261632.00\n",
      "| epoch  19 | 10000/13056 batches | lr 1.39 | ms/batch 10.59 | loss 80.57 | ppl 97516841327219090966028716687228928.00\n",
      "| epoch  20 |  5000/13056 batches | lr 1.36 | ms/batch 10.58 | loss 80.57 | ppl 98142106887037975739740255816777728.00\n",
      "| epoch  20 | 10000/13056 batches | lr 1.36 | ms/batch 10.56 | loss 80.26 | ppl 72049778154485557732994425399803904.00\n",
      "| epoch  21 |  5000/13056 batches | lr 1.34 | ms/batch 10.64 | loss 80.45 | ppl 87021027570043537288596269924614144.00\n",
      "| epoch  21 | 10000/13056 batches | lr 1.34 | ms/batch 10.55 | loss 80.20 | ppl 67748945377025094744161419530338304.00\n",
      "| epoch  22 |  5000/13056 batches | lr 1.31 | ms/batch 10.60 | loss 80.30 | ppl 74716897644666805646496398317715456.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  22 | 10000/13056 batches | lr 1.31 | ms/batch 10.58 | loss 79.98 | ppl 54505453593685791474948360040873984.00\n",
      "| epoch  23 |  5000/13056 batches | lr 1.28 | ms/batch 10.61 | loss 80.38 | ppl 81257696282865294017726782510202880.00\n",
      "| epoch  23 | 10000/13056 batches | lr 1.28 | ms/batch 10.58 | loss 79.96 | ppl 53101113715826852116131063510597632.00\n",
      "| epoch  24 |  5000/13056 batches | lr 1.26 | ms/batch 10.53 | loss 80.38 | ppl 80844869350051890012074785399373824.00\n",
      "| epoch  24 | 10000/13056 batches | lr 1.26 | ms/batch 10.57 | loss 80.40 | ppl 82716639804684512851749114918993920.00\n",
      "| epoch  25 |  5000/13056 batches | lr 1.23 | ms/batch 10.57 | loss 80.34 | ppl 77936048951442519951945608179220480.00\n",
      "| epoch  25 | 10000/13056 batches | lr 1.23 | ms/batch 10.58 | loss 79.83 | ppl 46930245563146563858787173209735168.00\n",
      "| epoch  26 |  5000/13056 batches | lr 1.21 | ms/batch 10.59 | loss 80.60 | ppl 101157702990041110768339315467485184.00\n",
      "| epoch  26 | 10000/13056 batches | lr 1.21 | ms/batch 10.56 | loss 80.28 | ppl 73026274070299556769220424695283712.00\n",
      "| epoch  27 |  5000/13056 batches | lr 1.18 | ms/batch 10.55 | loss 80.15 | ppl 64584772457823342750473987851026432.00\n",
      "| epoch  27 | 10000/13056 batches | lr 1.18 | ms/batch 10.57 | loss 79.91 | ppl 50562281042578725216109468835643392.00\n",
      "| epoch  28 |  5000/13056 batches | lr 1.16 | ms/batch 10.61 | loss 80.24 | ppl 70479329184848656782514823358316544.00\n",
      "| epoch  28 | 10000/13056 batches | lr 1.16 | ms/batch 10.54 | loss 79.76 | ppl 43760337666801143457868569158615040.00\n",
      "| epoch  29 |  5000/13056 batches | lr 1.14 | ms/batch 10.55 | loss 80.22 | ppl 68960756351341091962797033781198848.00\n",
      "| epoch  29 | 10000/13056 batches | lr 1.14 | ms/batch 10.55 | loss 80.11 | ppl 61586735593692563450442565923373056.00\n",
      "| epoch  30 |  5000/13056 batches | lr 1.11 | ms/batch 10.60 | loss 80.42 | ppl 84622445492187934389222724465590272.00\n",
      "| epoch  30 | 10000/13056 batches | lr 1.11 | ms/batch 10.53 | loss 79.95 | ppl 52697333069333272959419162485063680.00\n",
      "| epoch  31 |  5000/13056 batches | lr 1.09 | ms/batch 10.56 | loss 79.85 | ppl 47588390136663999951268401683365888.00\n",
      "| epoch  31 | 10000/13056 batches | lr 1.09 | ms/batch 10.56 | loss 80.11 | ppl 62072426669352597111774508377702400.00\n",
      "| epoch  32 |  5000/13056 batches | lr 1.07 | ms/batch 10.57 | loss 80.29 | ppl 73752753269746310070369886532534272.00\n",
      "| epoch  32 | 10000/13056 batches | lr 1.07 | ms/batch 10.57 | loss 79.92 | ppl 50901134707434346851253103575957504.00\n",
      "| epoch  33 |  5000/13056 batches | lr 1.05 | ms/batch 10.53 | loss 80.46 | ppl 88022670216570976687435593063333888.00\n",
      "| epoch  33 | 10000/13056 batches | lr 1.05 | ms/batch 10.59 | loss 79.79 | ppl 44707489869579707349060516682137600.00\n",
      "| epoch  34 |  5000/13056 batches | lr 1.03 | ms/batch 10.60 | loss 80.16 | ppl 65204298860731274929384538148700160.00\n",
      "| epoch  34 | 10000/13056 batches | lr 1.03 | ms/batch 10.57 | loss 79.83 | ppl 46522787853585429097163076243816448.00\n",
      "| epoch  35 |  5000/13056 batches | lr 1.01 | ms/batch 10.57 | loss 79.69 | ppl 40438217129157859942414624061652992.00\n",
      "| epoch  35 | 10000/13056 batches | lr 1.01 | ms/batch 10.55 | loss 80.18 | ppl 66189434746454331901254906733920256.00\n",
      "| epoch  36 |  5000/13056 batches | lr 0.99 | ms/batch 10.57 | loss 79.98 | ppl 54341399797281147162513068896813056.00\n",
      "| epoch  36 | 10000/13056 batches | lr 0.99 | ms/batch 10.52 | loss 80.40 | ppl 82696851601272868386326843834236928.00\n",
      "| epoch  37 |  5000/13056 batches | lr 0.97 | ms/batch 10.58 | loss 80.06 | ppl 59101572088145977901983041444642816.00\n",
      "| epoch  37 | 10000/13056 batches | lr 0.97 | ms/batch 10.58 | loss 79.82 | ppl 46297081008109456073807446858006528.00\n",
      "| epoch  38 |  5000/13056 batches | lr 0.95 | ms/batch 10.56 | loss 79.74 | ppl 42788777777359007784142901228863488.00\n",
      "| epoch  38 | 10000/13056 batches | lr 0.95 | ms/batch 10.53 | loss 80.61 | ppl 101766675989323278696571145369419776.00\n",
      "| epoch  39 |  5000/13056 batches | lr 0.93 | ms/batch 10.55 | loss 79.60 | ppl 37026119338357080774358144999489536.00\n",
      "| epoch  39 | 10000/13056 batches | lr 0.93 | ms/batch 10.58 | loss 79.91 | ppl 50525867037394200609731035082522624.00\n",
      "| epoch  40 |  5000/13056 batches | lr 0.91 | ms/batch 10.60 | loss 79.70 | ppl 41081997170885263837724144272670720.00\n",
      "| epoch  40 | 10000/13056 batches | lr 0.91 | ms/batch 10.56 | loss 80.07 | ppl 59152210650569541054014429281124352.00\n",
      "| epoch  41 |  5000/13056 batches | lr 0.89 | ms/batch 10.57 | loss 79.87 | ppl 48742668940034274441811402533371904.00\n",
      "| epoch  41 | 10000/13056 batches | lr 0.89 | ms/batch 10.56 | loss 79.73 | ppl 42086391082414157456711132320366592.00\n",
      "| epoch  42 |  5000/13056 batches | lr 0.87 | ms/batch 10.59 | loss 80.11 | ppl 61959861569526160732969897771925504.00\n",
      "| epoch  42 | 10000/13056 batches | lr 0.87 | ms/batch 10.59 | loss 79.82 | ppl 46164060402907064073471291987853312.00\n",
      "| epoch  43 |  5000/13056 batches | lr 0.86 | ms/batch 10.53 | loss 80.04 | ppl 57828109190000927777137325009534976.00\n",
      "| epoch  43 | 10000/13056 batches | lr 0.86 | ms/batch 10.55 | loss 79.96 | ppl 52986498314087623987882986890592256.00\n",
      "| epoch  44 |  5000/13056 batches | lr 0.84 | ms/batch 10.57 | loss 80.02 | ppl 56579652171524655455242322052120576.00\n",
      "| epoch  44 | 10000/13056 batches | lr 0.84 | ms/batch 10.56 | loss 79.88 | ppl 49294570218243952739979228371484672.00\n",
      "| epoch  45 |  5000/13056 batches | lr 0.82 | ms/batch 10.53 | loss 79.98 | ppl 54074560631225465804116827175911424.00\n",
      "| epoch  45 | 10000/13056 batches | lr 0.82 | ms/batch 10.56 | loss 80.08 | ppl 59889193947878625711859042546089984.00\n",
      "| epoch  46 |  5000/13056 batches | lr 0.81 | ms/batch 10.59 | loss 80.00 | ppl 55234692828214442758801478006079488.00\n",
      "| epoch  46 | 10000/13056 batches | lr 0.81 | ms/batch 10.57 | loss 79.79 | ppl 44935518163907069434464002860122112.00\n",
      "| epoch  47 |  5000/13056 batches | lr 0.79 | ms/batch 10.57 | loss 79.90 | ppl 50269985719698815578092101164859392.00\n",
      "| epoch  47 | 10000/13056 batches | lr 0.79 | ms/batch 10.56 | loss 80.25 | ppl 71125406300682784345535223187898368.00\n",
      "| epoch  48 |  5000/13056 batches | lr 0.77 | ms/batch 10.57 | loss 79.84 | ppl 47390497372534394254853060589256704.00\n",
      "| epoch  48 | 10000/13056 batches | lr 0.77 | ms/batch 10.54 | loss 79.98 | ppl 54147166428561056793376461376454656.00\n",
      "| epoch  49 |  5000/13056 batches | lr 0.76 | ms/batch 10.64 | loss 79.95 | ppl 52932698781708804801674461419929600.00\n",
      "| epoch  49 | 10000/13056 batches | lr 0.76 | ms/batch 10.55 | loss 79.83 | ppl 46518718796104234109711373706461184.00\n",
      "| epoch  50 |  5000/13056 batches | lr 0.74 | ms/batch 10.58 | loss 80.13 | ppl 62967957900034567758769072958865408.00\n",
      "| epoch  50 | 10000/13056 batches | lr 0.74 | ms/batch 10.53 | loss 79.59 | ppl 36631541778021778434733826778333184.00\n",
      "| epoch  51 |  5000/13056 batches | lr 0.73 | ms/batch 10.53 | loss 80.07 | ppl 59262772496111805595573734915178496.00\n",
      "| epoch  51 | 10000/13056 batches | lr 0.73 | ms/batch 10.56 | loss 79.95 | ppl 52506030560568097949460080860594176.00\n",
      "| epoch  52 |  5000/13056 batches | lr 0.71 | ms/batch 10.58 | loss 80.08 | ppl 59888042981768523507737878813736960.00\n",
      "| epoch  52 | 10000/13056 batches | lr 0.71 | ms/batch 10.59 | loss 79.76 | ppl 43627632789856134723991226527252480.00\n",
      "| epoch  53 |  5000/13056 batches | lr 0.70 | ms/batch 10.59 | loss 80.18 | ppl 66503919441927120795391596836159488.00\n",
      "| epoch  53 | 10000/13056 batches | lr 0.70 | ms/batch 10.51 | loss 79.65 | ppl 39048129467593973796047743461883904.00\n",
      "| epoch  54 |  5000/13056 batches | lr 0.69 | ms/batch 10.55 | loss 79.74 | ppl 42888613853352737800790699516362752.00\n",
      "| epoch  54 | 10000/13056 batches | lr 0.69 | ms/batch 10.55 | loss 79.61 | ppl 37626345293387032178501431133732864.00\n",
      "| epoch  55 |  5000/13056 batches | lr 0.67 | ms/batch 10.55 | loss 79.81 | ppl 45808476245785413965713865269313536.00\n",
      "| epoch  55 | 10000/13056 batches | lr 0.67 | ms/batch 10.56 | loss 80.15 | ppl 64371444548871534863571544818319360.00\n",
      "| epoch  56 |  5000/13056 batches | lr 0.66 | ms/batch 10.57 | loss 79.50 | ppl 33655237057256746743659827126337536.00\n",
      "| epoch  56 | 10000/13056 batches | lr 0.66 | ms/batch 10.59 | loss 79.73 | ppl 42359744159016614096375499641061376.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  57 |  5000/13056 batches | lr 0.65 | ms/batch 10.57 | loss 80.06 | ppl 58970771902195911450951947764367360.00\n",
      "| epoch  57 | 10000/13056 batches | lr 0.65 | ms/batch 10.55 | loss 79.32 | ppl 28202046960870902177468399547842560.00\n",
      "| epoch  58 |  5000/13056 batches | lr 0.63 | ms/batch 10.57 | loss 79.47 | ppl 32530013807707068546653451209146368.00\n",
      "| epoch  58 | 10000/13056 batches | lr 0.63 | ms/batch 10.56 | loss 80.11 | ppl 62023859793326923030812009644949504.00\n",
      "| epoch  59 |  5000/13056 batches | lr 0.62 | ms/batch 10.58 | loss 80.23 | ppl 69846242195667239186498428866658304.00\n",
      "| epoch  59 | 10000/13056 batches | lr 0.62 | ms/batch 10.58 | loss 79.89 | ppl 49832391213090747348964370483773440.00\n",
      "| epoch  60 |  5000/13056 batches | lr 0.61 | ms/batch 10.61 | loss 79.68 | ppl 40111420548506549633957397794914304.00\n",
      "| epoch  60 | 10000/13056 batches | lr 0.61 | ms/batch 10.58 | loss 79.61 | ppl 37378216302692801882482288684433408.00\n",
      "[11.61507225 11.61507225 11.61507225 ... 11.61507225 11.61507225\n",
      " 11.61507225]\n",
      "[ 0.546314    0.546314    0.546314   ... 34.99910736 34.99910736\n",
      " 34.99910736]\n",
      "Pearson r of the model is nan\n",
      "Model 11:   emsize: 1024   d_hid: 350   nlayers: 4   nhead: 4   r:nan\n"
     ]
    }
   ],
   "source": [
    "ntokens = 1024  # For ff estimation\n",
    "dropout = 0.2  # dropout probability\n",
    "epochs  = 60 \n",
    "emsize = 1024\n",
    "\n",
    "data_arr = []\n",
    "models_tested = 0\n",
    "\n",
    "for d_hid in range(50, 500, 150):\n",
    "    for nlayers in range(1, 7, 3):\n",
    "        for nhead in range(1, 7, 3):\n",
    "            r = 0.0\n",
    "            fail = False\n",
    "            try:\n",
    "                model = TransformerModel(ntokens, 1, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
    "\n",
    "                criterion = nn.MSELoss()\n",
    "                lr = 2.  # learning rate\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "                for epoch in range(1, epochs + 1):\n",
    "                    train(model)\n",
    "                    scheduler.step()\n",
    "\n",
    "                r = test()\n",
    "\n",
    "                data_arr.append((emsize, d_hid, nlayers, nhead, r))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                fail = True\n",
    "\n",
    "            if fail:\n",
    "                print(\"Model %d:   emsize: %d   d_hid: %d   nlayers: %d   nhead: %d   FAILED\" % (models_tested, emsize, d_hid, nlayers, nhead))\n",
    "            else:\n",
    "                print(\"Model %d:   emsize: %d   d_hid: %d   nlayers: %d   nhead: %d   r:%.3f\" % (models_tested, emsize, d_hid, nlayers, nhead, r))\n",
    "\n",
    "            models_tested += 1\n",
    "                \n",
    "data_save = np.array(data_arr)\n",
    "np.save(\"results.npy\", data_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
