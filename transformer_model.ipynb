{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset, TensorDataset, DataLoader\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, out_size: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Linear(ntoken, d_model) # Embedding layer converted into linear layer\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, out_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "#         src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "    \n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.decoder(output)\n",
    "#         print(output.size())\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change working directory to labels\n",
    "work_dir = \"C:/file_lists_with_labels_ff_estimator\"\n",
    "os.chdir(work_dir)\n",
    "\n",
    "# Reads labels into dict of (filename: ff_value)\n",
    "training_data_labels = {}\n",
    "with open(\"training.txt\") as f:\n",
    "    for line in f:\n",
    "        key, val = line.split()\n",
    "        training_data_labels[key] = float(val)\n",
    "        \n",
    "test_data_labels = {}\n",
    "with open(\"test.txt\") as f:\n",
    "    for line in f:\n",
    "        key, val = line.split()\n",
    "        test_data_labels[key] = float(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Change directory to data\n",
    "os.chdir(\"C:/rf_without_tgc\")\n",
    "\n",
    "# Iterate thru data to create lists of tensors\n",
    "training_data = []\n",
    "training_labels = []\n",
    "for file in tqdm(os.listdir()):\n",
    "    if file not in training_data_labels:\n",
    "        continue\n",
    "        \n",
    "    file_data = pd.read_csv(file, header=None).T\n",
    "    \n",
    "    x_tensor = file_data.to_numpy().astype(np.float32)\n",
    "    y_tensor = float(training_data_labels[file])\n",
    "    \n",
    "    for x in x_tensor:\n",
    "        training_data.append(x)\n",
    "        training_labels.append(y_tensor)\n",
    "    \n",
    "test_data = []\n",
    "test_labels = []\n",
    "for file in tqdm(os.listdir()):\n",
    "    if file not in test_data_labels:\n",
    "        continue\n",
    "        \n",
    "    file_data = pd.read_csv(file, header=None).T\n",
    "    \n",
    "    x_tensor = file_data.to_numpy().astype(np.float32)\n",
    "    y_tensor = float(test_data_labels[file])\n",
    "\n",
    "    for x in x_tensor:\n",
    "        test_data.append(x)\n",
    "        test_labels.append(y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create class for DataLoader compatability\n",
    "# class Data():\n",
    "#     def __init__(self, x, y):\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.x)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         X = self.x[idx]\n",
    "#         y =  self.y[idx]\n",
    "\n",
    "#         return X, y\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "training_labels = np.array(training_labels)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(training_data.shape)\n",
    "print(test_data.shape)\n",
    "    \n",
    "# training_data = np.swapaxes(training_data, 1, 2)\n",
    "# test_data = np.swapaxes(test_data, 1, 2)\n",
    "\n",
    "# Create data tensors\n",
    "training_data = torch.Tensor(training_data)\n",
    "training_labels = torch.Tensor(training_labels)\n",
    "\n",
    "test_data = torch.Tensor(test_data)\n",
    "test_labels = torch.Tensor(test_labels)\n",
    "\n",
    "# training_data = (training_data - torch.mean(training_data)) / torch.std(training_data)\n",
    "# test_data = (test_data - torch.mean(test_data)) / torch.std(test_data)\n",
    "\n",
    "train_dataset = TensorDataset(training_data, training_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# # Load tensors into class for torch DataLoaders\n",
    "# train_data = Data(training_data, training_labels)\n",
    "# test_data = Data(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader Parameters\n",
    "train_loader_params = {\n",
    "    \"batch_size\":  32, \n",
    "    \"shuffle\":     True,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_loader = DataLoader(train_dataset, **train_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose device for torch computing\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 1024  # For ff estimation\n",
    "emsize = 1024#200  # embedding dimension\n",
    "d_hid = 100  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 4  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.1  # dropout probability\n",
    "model = TransformerModel(ntokens, 1, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "lr = 3  # learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 5000\n",
    "    start_time = time.time()\n",
    "#     src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = 261120 // 20 # Total training signals // batch_size\n",
    "    for batch, (signal, target) in enumerate(train_loader):\n",
    "        signal, target = signal.to(device), target.to(device)\n",
    "#         signal = torch.unsqueeze(signal, dim=1)\n",
    "        batch_size = signal.size(0)\n",
    "#         print(signal.size())\n",
    "#         print(target.size())\n",
    "        output = model(signal)\n",
    "        output = torch.squeeze(output)\n",
    "        loss = criterion(output, target)\n",
    "#         print(output.size(), \"\\n\")\n",
    "#         print(output.size(), target.size())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "#             ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 50\n",
    "best_model = None\n",
    "\n",
    "begin_time = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "#     val_loss = evaluate(model, val_data)\n",
    "#     val_ppl = math.exp(val_loss)\n",
    "#     elapsed = time.time() - epoch_start_time\n",
    "#     print('-' * 89)\n",
    "#     print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "#           f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "#     print('-' * 89)\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(time.time() - begin_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# DataLoader Parameters\n",
    "loader_params = {\n",
    "    \"batch_size\":  1, \n",
    "    \"shuffle\":     False,\n",
    "    \"num_workers\": 0\n",
    "}\n",
    "\n",
    "# Create test DataLoader\n",
    "test_loader = DataLoader(test_dataset, **loader_params)\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialise arrays and dict\n",
    "    predictions = np.array([])\n",
    "    labels = np.array([])\n",
    "    averaged_dict = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (signal, label) in tqdm(enumerate(test_loader)):\n",
    "            # Send input to device\n",
    "            signal = torch.Tensor(signal).to(device)\n",
    "            signal = torch.unsqueeze(signal, 0)\n",
    "             \n",
    "            # Get output of net, append to lists\n",
    "            output = model(signal).cpu().detach().numpy()\n",
    "            output = output[0][0]\n",
    "            predictions = np.append(predictions, output)            \n",
    "            labels = np.append(labels, label)\n",
    "#             print(output, \"\\n\")\n",
    "#             print(output, label)\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] not in averaged_dict:\n",
    "                averaged_dict[labels[i]] = [predictions[i]]\n",
    "            else:\n",
    "                averaged_dict[labels[i]].append(predictions[i])\n",
    "            \n",
    "    for i in averaged_dict:\n",
    "        averaged_dict[i] = np.mean(averaged_dict[i])\n",
    "    \n",
    "    averaged_predictions = []\n",
    "    ordered_labels = []\n",
    "        \n",
    "    for i in averaged_dict:\n",
    "        ordered_labels.append(i)\n",
    "        averaged_predictions.append(averaged_dict[i])\n",
    "    \n",
    "    print(predictions)\n",
    "    print(labels)\n",
    "    \n",
    "#     r = scipy.stats.pearsonr(predictions, labels)\n",
    "    \n",
    "    r = stats.pearsonr(averaged_predictions, ordered_labels)\n",
    "    \n",
    "    print('Pearson r of the model is %.2f' % r[0])\n",
    "    return r[0]\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 1024  # For ff estimation\n",
    "dropout = 0.2  # dropout probability\n",
    "epochs  = 60 \n",
    "emsize = 1024\n",
    "\n",
    "data_arr = []\n",
    "models_tested = 0\n",
    "\n",
    "for d_hid in range(50, 500, 150):\n",
    "    for nlayers in range(1, 7, 3):\n",
    "        for nhead in range(1, 7, 3):\n",
    "            r = 0.0\n",
    "            fail = False\n",
    "            try:\n",
    "                model = TransformerModel(ntokens, 1, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
    "\n",
    "                criterion = nn.MSELoss()\n",
    "                lr = 2.  # learning rate\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "                scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.98)\n",
    "\n",
    "                for epoch in range(1, epochs + 1):\n",
    "                    train(model)\n",
    "                    scheduler.step()\n",
    "\n",
    "                r = test()\n",
    "\n",
    "                data_arr.append((emsize, d_hid, nlayers, nhead, r))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                fail = True\n",
    "\n",
    "            if fail:\n",
    "                print(\"Model %d:   emsize: %d   d_hid: %d   nlayers: %d   nhead: %d   FAILED\" % (models_tested, emsize, d_hid, nlayers, nhead))\n",
    "            else:\n",
    "                print(\"Model %d:   emsize: %d   d_hid: %d   nlayers: %d   nhead: %d   r:%.3f\" % (models_tested, emsize, d_hid, nlayers, nhead, r))\n",
    "\n",
    "            models_tested += 1\n",
    "                \n",
    "data_save = np.array(data_arr)\n",
    "np.save(\"results.npy\", data_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
